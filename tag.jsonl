{"id": 1, "data": " So, we need to touch on some of the things that network function virtualization is driving into containers, and in particular into Kubernetes. So, let's take a look, first of all, at container networking from a topological view, is that if we look at some of the key areas or challenges that we face in containers when we're considering the deployment of this in a bare metal environment, some of those challenges are in the areas listed above. Down that first column is that we've got-- we've got multiple network interfaces are required for your typical VNF in a coms type workload. This is a significant deviation from what we might find in an OTT or an IT environment where we talked about the cul-de-sac. That is, the traffic ingresses on a particular port and egresses on the similar port. We talk about that as being a north to south type of traffic. And that's great. Containers are very well suited for that environment. But in a coms environment, that traffic is more of an East to West in many cases. And then as it flows in on one port and it's got a destination that is heading in a different direction, that's the west side, So, it needs to flow out another port. Kubernetes, and containers in general, are evolving rapidly to address this issue, but it's one of the shortcomings that needs to be addressed. Similarly, we've got-- we've got challenges inside the acceleration aspects of things. Again, coms service provider workloads typically have a high flow rate of I/O associated with them and lesser on the compute side of things. So, we need accelerators for data plane acceleration for that east to west traffic as we spoke of. And then there's actually orchestration of the management of those platforms themselves. We identified in our earlier discussions about how the orchestration in containers have descriptor capabilities in their feature set that allow us to identify capacity or capabilities of the host platform that we're targeting. And we need a way to pull those resources from those platforms so that they can be accessed in that container orchestration environment.[O1] And capabilities that we might find, for example, in the Intel enhanced platform awareness, also known as EPA, are critical elements to allow the orchestration environment to pull those host machines to identify what resources are available in those machines, and also, at a higher level, to identify what resources are being consumed at any time as we begin to look at optimizing the workloads that can be deployed there. And to that consumption level, that drives the issue of telemetry and functionality. For example, like Collect D is a resource that allows the orchestration and the upper layer of elements of the management system to identify the functionality thresholds that are being consumed by those machines. So those are some of the challenges that we see in some of the technologies that we bring to bear in addressing the evolution of the development that's taking place inside the container environment to meet the needs of those coms service provider or those NFE workloads. So, in the networking area, one of the interesting areas is the the multi network interface. This is an area that's being driven by Intel and others into that environment that extends beyond the traditional single interface that originally would have found-- a zero interface, if you will-- in a Kubernetes environment to allow for multiple interfaces. And that is whether those interfaces are bonded to give us greater capacity or if they're bonded for resiliency or redundancy in that environment, or whether we've actually got interfaces that are plumbed to different networks. That is, an east network and a west network. And in addition, to the out of band network interface oftentimes that we find for the OAM and P functionality that's associated with these VNF functions in that comms environment, and that is the management or the MS interfaces typically would reside on an outer band network interface as opposed to being superimposed on the East interface or the West interface as we spoke about that. And the reasons for that is that if you look at the functionality inside the data center, you may have multiple layers of switches that are pointed in one direction or the other and there's an isolation that takes place. Some of that's due to NAT'ing and others are simply that, as the traffic traverses these networks, it's a hop along the way and we're going to perform some computational calculations on that.[O2] But, nevertheless, our responsibility is to move that traffic through a network as opposed to terminating it to an endpoint in that network. So, those are some of the use cases. Multus is an effort, a project that's underway, to address the functionality that takes place. The element there is that-- we spoke earlier about the pods inside the containers-- and those pods have that one to one association with that network element. And there really is that one to one and we need to expand that so that the pods themselves aren't owning a single interface but rather can have access to multiple interfaces. And in the future that those multiple interfaces can actually be shared across multiple containers. One of the efforts that's in there is known as the CNI, and that's the Container Network Interface plug-in module. So inside containers we have this concept of plug-ins, and those plug-ins are an industry solution to that East to West traffic. So, we can complement some of the limitations that we find in the existing packages-- for example, in the container environment-- that limit us to both packet acceleration and to the single interface through these efforts for the Multus CNI. And Intel's actively engaged in some of those areas to accelerate that. The acceleration point, there are lots of different technologies for acceleration. One of them is DPDK accelerated Open vSwitch. So, we talked about container applications needing to interface to each other, and one way of doing that is by running traffic through an accelerated vSwitch where the traffic doesn't need to leave that host but can actually move from container to container as long as it's inside a common pod. Or if traffic's moving North to South through the platform itself, off of that interface and being sorted through DPDK OVS application and other acceleration applications, for example, like FDjo. And these are the efforts that are underway. And there's a link to a reference down here for a GitHub to some user space CNI plug-ins in your container environment. And the intent here, again, is to accommodate a container design that would enable better throughput in those East to West traffic loads that we'd find in those coms spaces. We've mentioned DPDK a little bit. One of the other packet accelerations is a little closer to bare metal is the SR-IOV. That's the single route I/O virtualization functionality. And even in the SR-IOV there's an effort underway to integrate this in more tightly into the Kubernetes design so that we have access to the acceleration that an SR-IOV based acceleration model can give us. And, again, in that space, you may also find that SR-IOV can rely on DPDK at the application layer for the pulling mechanism. And there's some hot links in here to some of the work that's going on. Again, GitHub is a great source for that, and Intel and Redhead are very actively involved in that packet acceleration CNI plug-in NOTICES AND DISCLAIMERS [O1]Please merge these two subtitles. [O2]Please merge these two subtitles.", "label": [[557, 560, "VNFs"], [3864, 3868, "VNFs"], [97, 107, "Containers"], [877, 887, "Containers"], [1195, 1205, "Containers"], [1811, 1821, "Containers"], [4873, 4883, "Containers"], [5224, 5234, "Containers"], [5361, 5371, "Containers"], [304, 314, "Containers"], [132, 142, "Kubernetes"], [1179, 1189, "Kubernetes"], [3394, 3405, "Kubernetes"], [184, 193, "Containers"], [6332, 6336, "DPDK"], [6768, 6773, "DPDK"], [7025, 7036, "Kubernetes"], [7207, 7213, "DPDK"], [2066, 2075, "Containers"], [3004, 3013, "Containers"], [5307, 5316, "Containers"], [5586, 5596, "Containers"], [5892, 5896, "DPDK"], [6157, 6167, "Containers"], [6171, 6181, "Containers"], [6866, 6872, "SR-IOV"], [6948, 6954, "SR-IOV"], [7094, 7100, "SR-IOV"], [7189, 7195, "SR-IOV"]]}
{"id": 2, "data": " Hi, so let's wrap this up a little bit, talk about some of the other challenges that are facing us if we choose to head down the container path for our interesting workloads here. So, resource management inside Kubernetes, one of the things that we could possibly run into is that while the clusters are deployed on a wide array of heterogeneous environments, this implies that there are different hardware resources below. So, we can have different versions of CPUs, whether those are in the same generation of family or even different generations of family. The amount of memory that's available on those machines may, in fact, be different, as well as some of the core resources, whether this is hardware acceleration or whether there's a particular instantiation of a net  card.[O1] There are a number of things that can be different as we look at these large scale computer environments in that environment. So, one of our goals, then, is to introduce a way of managing those resources so that not only can we identify what those resources are, but then we can target specific workloads if they're dependent on features or capabilities in that resource to those workloads. And now we've mentioned Intel's EPA, or enhanced platform awareness before. This allows us to identify, for example, the CPU instances themselves, but also, some of the features that may exist on the card, whether those are large pages, whether those are any particular NUMA alignment for physical resources or accelerators that are involved in those resources as well as some of the other deeper capabilities in their instructions and architectures. So AVX 512, for example, if you've got a feature that choosing that type of capability for pattern matching or some type of security capabilities that are in there. So, there are lots of features that are available to us that in that resource management we need to be able to access, and this is an ongoing effort for the Kubernetes environment under the container work to continue to keep up with those various capabilities that show up in the deployment of those standard high volume servers from generation to generation. One of the other areas that's a challenge, we spoke about networking, and this is going to get into some of the aspects associated with that is the device plugins. So, if you have different network cards, even though you're going to have drivers in there, you may want to access particular acceleration capabilities that those intelligent cards are able to access. In order to do that, then the vendor of those devices may, in fact, have to write custom Kubernetes code in order to allow that device to be integrated into that system so that we can pull those resources and identify them from a management and orchestration layer.[O2] Some of the use cases that we may find for that do exist in the com space, whether those are rand capabilities that might need FPGAs, or whether those are things that, for example, you might find in an APC where we could take advantage of some of the packet acceleration. So, these NFE limitations can be addressed by the device plugins, but again, it places a burden, then, on the provider of that core technology to ensure that their product not only has drivers for the host operating system, but it has been integrated in so that they have Kubernetes awareness that's in there. So, what are we doing? The industry is moving forward in this area. Intel is helping a number of vendors in this space to provide integrated solutions that allow for the easy deployment of those workloads and those plug-ins into this environment. And this is an ongoing area of development research. So, before you assume that you're going to be able to move forward a particular accelerator card, if it's in a Kubernetes or a container-based environment, it's worth your due diligence to dig down into it. We talked about some of the packet acceleration before, and SR-IOV is a key element normally in that packet acceleration.[O4] It allows us to get near bare metal capabilities. Intel has been working very closely with Red Hat in the area of supporting the SR-IOV capabilities inside a Kubernetes environment. And here you see a little example of some of that work. So, the areas that are ongoing are to provide for that plug-in so that we've got the ability to advertise to multiple consumers above.[O5] Because SR-IOV implies single route. That is, it's a single card. But that the resources of that card could be multiplex across multiple pods then, and we want to be able to allocate certain resources for that on a pod by pod basis as those get consumed by a platform without over committing the resources there. So, this is an area of ongoing research, and there's a reference down here on the hotlink that will allow you to dig into that a little bit further. As you look at the future and we begin to look forward to it, SR-IOV's not the only solution, but there is certainly a good model for the container network interfaces in that we expect to be extended into additional pluggable modules allow for us to multiplex more than one port and multiplex resources of that port into these environments as we go forward. So, we can address pod scheduling, the SLAs for the VNFs that are actually in there so that if we allocate 4 gig of resources to a particular VNF, we can guarantee that that amount of resources is going to be allocated to that VNF and not consumed by other VNFs, even though there's a common resource on in the porting that's below that. So, this is all an active area of work that that's going on among the community to support these types of networking loads that are more unique in that com service application than you would find, for example, in a traditional IoT over the top environment. So, there's more to come in this area. I do encourage you to think wisely about this NOTICES AND DISCLAIMERS [O1]Please merge these two subtitles. [O2]Please merge these two subtitles. [O3]Please delete. [O4]Please merge these two subtitles. [O5]Please merge these two subtitles.", "label": [[5254, 5258, "VNFs"], [5458, 5463, "VNFs"], [211, 222, "Kubernetes"], [1952, 1962, "Kubernetes"], [3334, 3344, "Kubernetes"], [3782, 3793, "Kubernetes"], [2609, 2619, "Kubernetes"], [4163, 4173, "Kubernetes"], [4906, 4912, "SR-IOV"], [5344, 5347, "VNFs"], [5767, 5770, "IoT"], [130, 140, "Containers"], [1985, 1994, "Containers"], [3799, 3808, "Containers"], [3939, 3945, "SR-IOV"], [4134, 4140, "SR-IOV"], [4390, 4396, "SR-IOV"], [4981, 4991, "Containers"], [5429, 5432, "Containers"]]}
{"id": 3, "data": " Data Plan Development Kit started as Intel project. And it was open sourced very early with, at that time, huge help from 6WIND. Initial open source community got created. And already, in 2014, there was the release that included support for other CPUs and NICs. And this was done fully in open source. At that time, the versioning was 1.7. Later over time this thing, moved to Ubuntu like versioning of year dot month. And this was also when the first DPDK Summit was held. Over time, more and more vendors joined the community and contributed to the code base. And in 2017, it was all transferred as proper open source community under Linux Foundation, which also further ensures this openness. And in 2018, in particular, focus was to do a lot of contributions that are abstracting what is then all DPDK user space application from the underlying platform. In this way, making it more cloud friendly for those environments. The statistics of the community in terms of vendors joined and contributions and commits look very healthy. And there is growth to be observed over the years. And today, DPDK is the primary way NOTICES AND DISCLAIMERS", "label": [[453, 458, "DPDK"], [803, 807, "DPDK"], [1098, 1102, "DPDK"]]}
{"id": 4, "data": " Welcome to our session on Data Plan Development Kit or shorter DPDK where I'm going to go and cover the DPDK fundamentals. Where this thing fits, in general, is on their journey to run multiple workload categories-- very broad categories on volume ingredients in volume server on Intel architecture. So, this is, in particular, focusing on packet processing part of now very old 4 to 1 vision where, initially, we were running and then more and more migrating over the years. Application processing control processing already then in comes vertical. And with DPDK, this allows us to run packet processing type of workloads also on Intel architecture-based servers. So, the fundamentals of DPDK APIs in libraries are, for example, that this supports both run to competition and pipeline models. It does not include scheduler. So, for example, when it needs to access devices, it is pulling them in endless loop. So, it supports various operating system 32-bit, 64-bit different types, Intel Atom and Xeon processors. It recognizes required number of cores, and instructions include that type of memory and channels to that.[O1] So optimal packet allocation across all those different channels then happen. It supports huge pages so that it's easy to access memory in one go instead of going there multiple times. And it uses bulk concepts so that a lot of packets are processed simultaneously, and it's licensed as BSD primarily and then partly of the GPL because some parts are in Linux kernel. So, it is useful for building very optimized workloads in communications vertical for packet processing. It can also be used for similar type of workloads in Cloud, enterprise, and government, and the big categories of those modules are, for example, core libraries. They are taking care of memory management, software rings, timers, and similar. Packets classification is big category, then accelerate the software libraries, statistics, quality of service, and package framework is also there. So, on the bottom part of this diagram, you can see all the different type of devices that are supported. And this is, for example, starting with Ethernet devices, then there are the crypto devices, event-driven devices with their pull mode drivers, security compression, and broadband devices were added recently. So, this type of environment is including a lot of sample applications. They are easy way to start when called BSD license needs to be adopted. And readily they will be also useful on their own. For example, a lot of performance characterization we are doing with layer 2 and layer 3 forwarding applications from there. But then these type of code samples are then integrated by all the different type of vendors into their solutions. And today, we have a great adoption of those, and there is really a lot of products available during excellent packet processing using this type of approach.", "label": [[64, 68, "DPDK"], [105, 109, "DPDK"], [560, 564, "DPDK"], [690, 694, "DPDK"]]}
{"id": 5, "data": " For data plan development kit, we have a lot of both open source community projects that are adopting it, and also vendors in their commercial products. So, there is, for example, list of virtual switches and virtual routers. Open virtual switch is something that Intel teams patched with DPDK many years ago. It's now included in all the usual distributions of Linux. So, they're listed, for example, here in the middle. On Red Hat, Ubuntu, and so on, it's all included. Then Tungsten Fabric, or what was previously OpenContrail, is also in the meantime patched with DPDK with further investments that we are putting into that to have the vRouter there running really well and optimized. VMware vCloud environment, in particular vSphere 6.7 with NSX-T 2.2 is supporting and including the DPDK for packet processing. And then we did quite a lot of work inside FD.io, including DPDK, and then there are the other communities and switches that are also supporting it. In terms of adoption in the operating system, beyond what I mentioned-- Red Hat and Ubuntu-- there is also adoption in Free BSD, in Wind, and Windows, Fedora, CentOS. A lot of package generators are benefiting from blasting packets by using DPDK libraries. So, a TRex package generator are, for example, some of those that we used a lot. There are the other ones. And it is also helping in storage environments, in storage workloads. So, there is particular version of it. And this is also something that we contributed into Ceph. And then there are inside operating systems, TCP/IP stacks, that are being optimized with some of the examples listed here. Inside DPDK, there is also support for accelerators in sense of having generic API, where the application in user space is then hitting the API. This protects the software investments for the software vendor that is doing that. And then through this device framework, there are different drivers that can be plugged in. And this can be, for example, the implementation on Intel Xeon processor or it can be QuickAssist Technology type of chipset or adapter, accelerating, compression, and encryption algorithms. It can be some sort of SmartNIC, or it can be other devices. So, the idea is to have this type of device API abstracting NOTICES AND DISCLAIMERS", "label": [[290, 294, "DPDK"], [568, 573, "DPDK"], [790, 794, "DPDK"], [878, 882, "DPDK"], [1208, 1212, "DPDK"], [1629, 1633, "DPDK"]]}
{"id": 6, "data": " Welcome to our session on Fast Data I/O. Fast Data I/O project, or abbreviated FD.io and read as Fido, is Linux foundation project consisting of scalable network stack, open benchmarking effort, and integration into NFV and cloud environments. Scalable network stack is based on vector packet processing, VPP, which is scalable, fast, and feature-rich network stack. Open benchmarking environment is an effort consisting of methodologies, tools, and publishing results of measurements in well-defined labs. And NFV and cloud integrations mean integration into various NFV communities, into appropriate cloud networking communities, and is coming as part of the open source Linux distributions like Ubuntu, CentOS, and openSUSE. FD.io has five targeted use cases for adoption. It started historically as a physical appliance, where the VPP stack control plane parts are in production in routers since 2002. The second one is for building virtualized network functions, where the network stack can be used for faster programming of these types of elements. The third one is to use it in NFV infrastructure and with it build virtual switch or virtual routers, and already exists integration with OpenStack that KVM. The fourth one is to build containerized or Cloud Native virtual functions. And the fifth one is to use it in containerized platforms with a vRouter for containers and there is already integration in Kubernetes and Docker environments. So, with these type of use cases, we can build various appliances, virtualized network functions, or containerized network functions like Load Balancer, Carrier Grade NAT, firewalls, elements running on universal CPE, and IPSEC Gateways and SDWAN and other different VNFs. Out of the box functionality for building these use cases is very rich compared to some other environments. And here is a comparison of how, for example, a software development kit for building discrete appliances or for NFV infrastructure, containerized platforms, VNFs and CNFs, how does it compare to DPDK, which we introduced in another session, which is a software development kit for packet processing. It is integrated in FD.io. Obvious DPDK is, for example, part of NFV environments for virtualized or containerized versions.[O3] And with FD.io, as mentioned before, we can build all of that because of a huge versatility NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please delete. [O3]Please merge these two subtitles.", "label": [[80, 85, "FD.io"], [2152, 2157, "FD.io"], [2270, 2275, "FD.io"], [1717, 1721, "VNFs"], [1989, 1993, "VNFs"], [1944, 1947, "NFV"], [1241, 1254, "Containers"], [1414, 1424, "Kubernetes"], [1551, 1564, "Containers"], [1964, 1977, "Containers"], [512, 515, "NFV"], [569, 572, "NFV"], [1085, 1089, "NFV"], [1324, 1337, "Containers"], [1367, 1377, "Containers"], [2027, 2031, "DPDK"], [2167, 2171, "DPDK"], [2197, 2200, "NFV"]]}
{"id": 7, "data": " VPP, as foundation for FD.IO, is doing, as the name says, Vector Packet Processing. This means that it takes whole vector of packets and processes it through the graph node. And that way it goes from node to node and completes the whole graph, which is much more efficient way of doing that than taking individual packets and trying to do it, because in this case, when whole vector of packets is being processed, the first packet warms up the instruction set. And everything else goes much faster then. This also gets it to the predictable latencies and predictable run rates. So, the graph itself can be reorganized. It can be extended with plugins for the nodes that are done in software or they can have also some hardware acceleration.[O2] And the way how the packets are ingested is that there is a DPDK element as input. And then the whole set vector of packets is being processed in this rough node. And then this is how eventually it completes the whole graph. To configure this and to manage it, there is honeycomb management agent that exposes interfaces like NETCONF/YANG, REST, and BGP for further SDN integration. So, the universal network stack is covering Layers 2 to 4. It has control plane, traffic management, overlays, and other functionality, support for Linux and FreeBSD, kernel interfaces. It works in containerized and virtualized environments. And it can be used to build appliances, VNFs, CNFs, and infrastructure virtualized and containerized. So, it has extensible modular design, where modules are pluggable. And the whole graph can be reorganized easily through the system of plugins, extended. As mentioned, it is very fast, and scalable, and deterministic. So fast is because of the method that it uses to process packets, scalable because it's very parallelized and modular in design, and deterministic because of what was mentioned, that the instruction caches, everything gets pre-warmed. If the run rate loses a little bit, it will catch up in next rounds. So, it is also developer-friendly and all nicely packaged and documented. High-level view on the VPP is presented in this diagram-- So, devices are coming, of course, at the bottom of the stack. So, support for DPDK, QuickAssist Technology, and other type of interfaces is there. The three layers of IPSEC are-- so, for Layer 2, there is usual functionality of discovery, bridge, Access Control Lists, VLANs, MPLS. For IP, and in particular for IPSEC, there is version 4, version 6, and all the other related functionality that fits there. For Layer 4, it can be made stateless and stateful. For traffic management, there is appropriate functionality. For control plane, I already mentioned BGP. The other functionality is supported there to build real elements out of the whole stack. And then overlays are also supported so that we can get to nice virtualized environments. For controlling all that, supported is OpenDayLight, and Contiv, and Neutral ML2, and a few other environments there. So, re-combining different functionalities included in network stack, already mentioned appliances-- and virtualized, containerized elements can be built. Here are some examples of what needs to be included to build environment for universal CPE or for Load Balancer in Cloud environments or Broadband Network Gateway, BNG, or Intrusion Prevention System.[O5] So, the right choice of functionalities allows for fast integration of these types of elements based on very mature code in VPP. For performance in scaling, performance, one based on the algorithms that are being used for Vector Packet Processing is fast. But then also, because it uses the instruction sets for vector instructions like AVX, it does have full packet processing integration of DPDK. And because of highly parallelized and modular design, it comes to linear scaling. These types of numbers are then being measured in each release NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles. [O3]Please delete. [O4]Please delete. [O5]Please merge these two subtitles.", "label": [[24, 29, "FD.io"], [1411, 1415, "VNFs"], [806, 810, "DPDK"], [1327, 1340, "Containers"], [1345, 1357, "Virtualization"], [1458, 1471, "Containers"], [1442, 1454, "Virtualization"], [2206, 2210, "DPDK"], [2845, 2856, "Virtualization"], [3094, 3105, "Virtualization"], [3107, 3120, "Containers"], [3742, 3746, "DPDK"]]}
{"id": 8, "data": " As part of FD.io Continuous System Integration and Testing effort, there is open benchmarking, and as any performance characterization that we like to call benchmarking. It needs to follow the properly defined testing methodology. It needs to be repeatable, not be tuned for particular vendor product marketing outcomes, and also needs to have realistic use cases in realistic traffic scenarios.[O2] So, for FD.io CSIT, it is based on standard industry benchmarks and tools, like appropriate RFCs and open-source tools are included. It is easy to replicate in different labs and it works on different platforms from different vendors and it does provide realistic outcomes in terms of some numbers that are representative of deployed environments. And this is really taken seriously in the FD.io community, and every release is publishing appropriate reports with all the tools. So, if somebody likes to revalidate that, they are free to do that, contribute it back into next releases for FD.io. These type of results have breadth of test use cases, depth of all the detailed information in there, and then also are predictable because it can be then, I said, rerun and compared with what you are getting. This way and based on all the performance optimizations, it comes to what is, in marketing terms, used as terabyte elements on Intel architecture-based server NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles.", "label": [[12, 17, "FD.io"], [791, 796, "FD.io"], [989, 995, "FD.io"], [409, 414, "FD.io"]]}
{"id": 9, "data": " [O1] For previously mentioned five target use cases, there is a decent amount of vendor adoption from top tier of vendors, like Cisco and ZTE, for example. And this is coming in different implementations of those use cases. So physical appliances, for quite some time, Cisco is using the stack to build these types of products. In virtualized, there is-- for infrastructure-- faster building of it. Cisco has their version of OpenStack. They also have a virtual topology system that is based on Red Hat OpenStack and properly integrated over there. Alibaba and Yahoo have cloud load balancers based on it. And then in containerized environments for the functions, ZTE based quite a lot of their work on it. And then also, there is a Cisco Container Platform. And the vendors that are building these types of products, also here you can see a quote of happy engineers, how much work this type of environment saved them because they did not have to go and rework this type of functionality inside the network stack. A summary, FD.io is a very mature and robust network stack for integration of elements and environments for NFV and cloud. It includes inside this CSIT open benchmarking effort, which is very comprehensive. It addresses five key use cases, which are building appliances, virtualized network functions, virtualized infrastructure, containerized network functions, and containerized infrastructure platforms. And it can be used out of the box NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[1025, 1031, "FD.io"], [332, 343, "Virtualization"], [1123, 1126, "NFV"], [619, 632, "Containers"], [740, 749, "Containers"], [1285, 1297, "Virtualization"], [1317, 1328, "Virtualization"], [1345, 1359, "Containers"], [1381, 1395, "Containers"]]}
{"id": 10, "data": " Hi. In this session, we'll cover introduction and basics[O1] of cryptography, give a little history and how we're using it. So even now, when you're accessing our website here, this is being secured. So, if you click next to the HTPS, which is Hypertext Transfer Protocol, secure version of it, the way how HTTP protocol is running over transport layer security, you can go there and click and check with your own browser that this is a secured web. You can click on your certificate and see that proper certificate authority that your operating system and browser are trusting issued to our website such security and that the whole thing is properly configured. [O1]Please merge these two subtitles.", "label": [[65, 77, "Cryptography"]]}
{"id": 11, "data": " History of cryptography started many, many years ago. So even in ancient Egypt, anthropologists discover that hieroglyphs were written in a way that some were substituted with the goal of somehow concealing the message, obscuring the meaning of it. In many centuries later, there was also, for example, in the middle of this slides, there is cipher that is using cylinders and leather rope, leather band. So those letters written on the leather would be wrapped around the cylinder. And once unwrapped, to really read it again, you would need to have the cylinder of the same characteristic. And in Roman Empire, there was a Caesar cipher, which is shift cipher. At that time, there was already an alphabet. And when they were shifting it by number of letters, they would be using decoder wheel to get the meaning of the message back again. This, of course, over time developed further and got more complicated so that it's much harder to get the meaning of the message without having all the right tools in place. So in the Middle Ages, there was a Vigenere cipher, which is much more complicated version of a shift cipher being used with substitution of letters. And then there was very complicated construction of Enigma machine that could change the letters in many trillions of ways. And still, even this type of machine-- there was another team that, once they got hold of it, they could disassemble it, reverse engineer, and figure out how it works and pretty much intercept the messages by doing that. NOTICES AND DISCLAIMERS", "label": [[12, 24, "Cryptography"]]}
{"id": 15, "data": " To help exchanged encrypted messages in very distributed environments, like Internet, for that, we have public key cryptography, which is form of asymmetric cryptography in the sense that the encryption key is not the same on both sides of the communication channel. We have public key and private key. So the sending party has private key, which needs to be really well-protected to encrypt the message. And then there is the public key that is, as the name says, publicly shared so that the receiving party can use that key to decrypt the message. And this way, we can, without sharing the same key, we can share the encrypted messaging between two different parties. Of course, it goes the other way around. So if the receiving party needs to send something back and needs to have similar type of mechanisms, we can implement it both ways. So the other related concept to that are digital signatures so that the messages, by having the private key, we can do the digest of it. And this way, we ensure the integrity of the message. And the receiving party, by using the public key, can go and verify that this message is really originating from the sender and that it was not tampered with in the transit. And to help with establishing trust between multiple parties, there are certain amount of very trusted certificate authorities that have the role of giving these type of certificates to include the identification that is really verified by certificate authority and the public key and the proper domain name qualifier for the, for example, the website. And this is what I was mentioning at the beginning of this whole section. This is how you could verify that over there, the HTTPS with a small logo, view certificate, that the right certificate authority that you also trust is involved in ensuring that the website and the message, and no-tampering, and the right algorithms are supported by delivering this type of web content. NOTICES AND DISCLAIMERS", "label": [[116, 128, "Cryptography"], [158, 170, "Cryptography"]]}
{"id": 17, "data": " than handling RTL upgrades, and so on. Why Hyperscan? There are other regular expression engines to be used. So this one is most optimized, and not just for raw matching speeds. The teams focused also on different modes of operation being streaming and non-streaming. The non-streaming is block on small writes so when very small text needs to be found, like searching for user agent is example here, for performance under high match rates, when there is a lot of matches happening and callbacks are going to be triggered, scaling to multiple cores and threats so that its design is paralyzed and takes advantage, software takes advantage then, of available processing resources. That pattern database is compiled in short time. And that the byte code size and the stream state and overall memory utilization is then highly optimized. Long list of Intel architecture instruction set is being used. Some are here, so streaming extensions and vector extensions, for example, which, in performance-required environments, means that Hyperscan can then replace other type of engines. There are different modes of operation that are supported. So there is block mode, where the data is taken as a block. And then it's being scanned for patterns by using the database of compiled patterns. There is vectoring mode, where data is not just taken in blocks, but there are vectors of those blocks. And then there is a streaming mode, which is streaming data and applying vectors on the stream. And all three of those are optimized with Hyperscan. The benefits is that it is very feature-rich. So many different types of regular expressions are supported. It does cross-compilation to multiple architectures. It does support different modes of operation, like block and vector and streaming. And it is robust with extensive features, used in many different RegEx, regular expression, constructs. It supports different environments in terms of hardware platforms, so Atom, Core, and Xeon processors. And compared to hardware acceleration, like FPGA being look-aside in, for example, some adapter, latencies are much better because this is all then in CPU caches and in memory. It is much more flexible, easier to program being C and C++ libraries.", "label": [[44, 53, "Hyperscan"], [1030, 1039, "Hyperscan"], [1526, 1535, "Hyperscan"]]}
{"id": 18, "data": " Welcome to our session on Hyperscan, industry's fastest regular expression and literal matching algorithm, based on Intel platforms, available in open source, and licensed with BSD. This is really well-tuned and optimized on a variety of Intel architecture processors, from Atom over Core to Xeon, supporting or integrated in various operating systems-- so various Linux distributions, FreeBSD, Windows, and OS X. It is itself based in C and C++ and has language bindings for Go language, for Java, for Python, and integrated in variety of open source communities and commercial products, like intrusion protection systems, intrusion detection systems, like Snort and Suricata, in deep packet inspection engine within SDWAN or for network and web security. This is improving throughput, allowing optimization of memory usage or reduction of CPU cycles for doing this type of regular expressions and an example of Snort, which is IDS/IPS product and involves large amount of literal and regular expression work. When it was integrated with Hyperscan, the throughput went up three times. Regular expressions are patterns used to match characters in text. And here are a few examples. We can look for exact matches or searching for some characters, replacing them with different strings. We can find also the strings in different streams of words, how many times they would repeat, all the different types of combinations are possible, creating a lot of different patterns to look for. And Hyperscan engine can use anywhere between one to many thousand of those. And it's based on libpcre Perl-compatible regular expression, which is used as semantic basis for fuzzing and for automating testing. Hyperscan, as a regular expression matching library, is running well-optimized on variety of Intel processors. It has BSD licensing for open source. It is having those match rule sets, all the different patterns that are then compiled, with input also being what kind of mode do we use for Hyperscan into byte code, which is database of those patterns, which is supporting multiple platforms then. It's byte code. And then the engine itself is going to be using the pattern database, is going to be looking at those in available text as per mode of operation. And when needed, it will do the callback if the match is found. And this makes the whole environment very flexible and powerful. NOTICES AND DISCLAIMERS", "label": [[27, 36, "Hyperscan"], [1040, 1049, "Hyperscan"], [1488, 1497, "Hyperscan"], [1695, 1704, "Hyperscan"], [1985, 1994, "Hyperscan"]]}
{"id": 19, "data": " In summary, Hyperscan is available in open source with BSD license on http://hyperscan.io. It provides substantial benefits to various projects or products that are using regular expressions and literal matching and need high performance of that, like IDS/IPS. An example of Snort improvement in throughput was three times. Suricata was six times. So it improves throughput. It reduces the memory footprint compared to some other engines doing similar, or it reduces required CPU cycles for it. And it is mature and solid and available in a big number of commercial products. NOTICES AND DISCLAIMERS", "label": [[13, 22, "Hyperscan"]]}
{"id": 20, "data": " With Hyperscan, we have different types of operations that will have appropriate performance impact. So first operation that we need to do is to compile initial input set of patterns into the bytecode. Compiler is implemented in C++. It does dynamic memory allocation. The compilation time will depend on complexity of the patterns. And anything unsupported will be given immediately to the developers. So the result is the bytecode database, which is then working on different target environments. The next operation is then the runtime. It can be in different modes. So here, example is the block mode, which is very popular. So it has the runtime components of scratch space, which is the working memory where the engine is doing the comparisons. It uses the compiled bytecode from the previous step as read-only data. And then it works on the blocks of input data. If something is found in those regular expressions and literal matching, then the matches are returned with a callback. And it is only predictable memory allocations that are being done. So it's not dynamically going to, for example, use too much memory. So the runtime is in C only. And if we want to do the other mode of operation, this is streaming. It is similar to the block mode. But also, the stream state needs to be maintained. So there are additional operations added, like open, write, and close. And there are also shortcuts like, for example, to reset the stream. An appropriate diagram is then presented here. For performance tuning, this is as in other similar engines of this type. And number of these tips are pretty intuitive. So the block-based matching on predefined blocks is going faster than for anything streaming. The scratch usage is to allocate the space once for your patterns database and then use it at compile time for multiple scans. If we are paralyzing the functionality with multiple concurrent operations, then each of them will have to use their own scratch space. The pattern syntax exact match with literals is, of course, faster than any wild cards. And especially if we are searching for longer literals, this ones are easier to identify within the text. And if we are searching for something particular, it's better to put it at the beginning of the pattern than to have it after a couple of wild cards at the end. And in terms of flags, it is faster if there is a single match and if we are not asking where exactly in the text there is start of the match, which, in number of cases, is acceptable. And this can be used that way. And of course, more details on tuning are available on the web.", "label": [[6, 15, "Hyperscan"]]}
{"id": 21, "data": " Welcome to the section about hardware ingredients, configurations, and how does an appropriate stake look like. Here we will start with the ingredients. And at the beginning, just the little introduction about the difference between normal IT and cloud environments, compared to what we're looking at network function virtualization computing is that in cloud computing, there is little data requests coming in. There is significant amount of compute, and then there is a real-- a comparatively small amount of data coming out. For example, SQL requests into relational database is one of the workloads that we are having over there. On the other side, we have for network virtualization, and we are virtualizing network appliances. Then we have high data rate of data pull in traffic. And this is a number of cases going through the element. So, the amount of data high going in, high going out. It can also go bi-directional and corresponding compute per the amount of data is much lower for NFV. So, this is then the resulting in very different hardware configurations that will be explained throughout this presentation. So, looking at where we are today with NFV is a lot of control plane workloads are virtualized now in production for many years and number of data pulling workloads is in production much lower. This is what for some time we are doing, and now we have good success putting it in. And the reason is that control plane was both technically easy to do compared to data plane, and we'll explain why in the second. And also, it was fitting on the IT configurations of servers so that the usual IT departments, when they are purchasing servers from the vendors, technically, this workload looking like other IT workloads. It was fitting nicely onto these type of hardware configurations, so it already existed over there. While data plane on the other side is characterized by this high data plane traffic rate-- examples are gateways and routers as network elements-- we measure performance there in packets per second. The hardware configuration that is required there is throughout this presentation called data plan server. It consists of normal IT volume server. In this configuration, it needs to have appropriate number of network ports in configuration that we will call balanced I/O so that both sockets are being fed with traffic. And because of the software that we use, typically, we'll include data plan development kit, and pull mode driver is spinning a core. They will need appropriate amount of cores in the CPUs, So, we will use high count CPUs, like 20 or 22 core, on previous generations like E5 or on the current one, on Xenon scalable, also this goes to, for example, 61, 52 or 61, 38 are those 20 plus core CPUs there. And current network adapters that you are using there is multiple ports per adapter, even number of adapters feeding data plan traffic into the server on 710 series controller. Here we summarized what are the major requirements on NFV infrastructure. So, the server already described components that we are using to build it. Beyond the usual CPUs and network adapters, we can put additional accelerators where needed and where they are beneficial. It can be fixed function already introduced in other presentations was QuickAssist technology, or it can be programmable like FPGA. And there we need to optimize server for high amount of traffic coming from the outside of the server getting in and then also between the virtual machines. If they're chained, then also we have east-west traffic. So, on top of it, we don't just have simple basic workload placement. We need to have a proper layer that does the orchestration, which needs to be both platform aware so that it knows how to recognize all the platform configurations and appropriately place the workload and also needs to be service server, meaning that if service consists of multiple elements, it needs to know how to instantiate them, configure them correctly until the whole service is available. And some of the differences to IT environments are, for example, we need much more deterministic performance. We have much stricter key performance indicators on these high throughputs and also, on low latency and control jitter[O3]. And then we have typically compared to most IT environments in comms vertical, we have much higher availability requirements. And then the resiliency both on complete failure or on the platform impairments. And then there are additional being regulated vertical. We have regulatory environments. Geolocation needs to be well determined, and this makes it generally, both technically, and how it gets implemented in processing organizations-- very different type of environment for data plan workloads. So, we could go and spend quite a lot of time optimizing all of these layers. So, what after many years of practice came up as a conclusion was optimizing where needed and abstract where possible because putting proper abstraction between those layers makes onboarding easier, Llifecycle management over years of production becomes easier. And then still, a number of cases, we will need to go and do very detailed optimization of those stakes. NOTICES AND DISCLAIMERS", "label": [[674, 688, "Virtualization"], [701, 713, "Virtualization"], [319, 334, "Virtualization"], [994, 999, "NFV"], [1165, 1168, "NFV"], [1209, 1220, "Virtualization"], [2992, 2995, "NFV"]]}
{"id": 22, "data": " So, those mentioned several configurations are being built out of volume ingredients and we always use processors and network adapters for these types of workloads. So, now we are using normally Intel Xeon scalable 61, 52, or 38 for data plane or then we are using some for control plane and other usual IT workloads somewhere in the middle of the range of the available core counts and appropriate chipsets that are being delivered in those servers. For Ethernet controller now, current one being used is 710 series, and this allows connectivity of 10, 25, and 40 gigabits on Ethernet ports. It allows multiple ports per adapter, and it is now very stable in production for quite some time adopted by all the usual server manufacturers. In some configurations where we're looking for either more input/output operations per server or we are looking for reduced annual failure rates, we are using solid state drives, instead of normal hard-disk drives, and they come in different categories. They are the SATA ones. They are the PCIe ones. And they are the obtained PCIe ones. So, and then depending on the workloads requirements, sometimes it makes sense to put acceleration cards that can accelerate both the software platform and the application itself.[O2] And here example is fixed function accelerator like QuickAssist technology. And to make this whole software platform also optimized in software, we have different libraries for this hardware acceleration and other algorithm primitives like data plan development kits or QuickAssist patches are available in all the right communities and products. And then also, for storage, we have a storage acceleration library.[O3] So, let's now go in each of these ingredients and introduce which versions of different hardware products that we're using. Over time, we transitioned the Intel Xeon E5 and E7 series into Intel Xeon scalable processors normally for data plane and higher end of control plane, we're using the gold product line. Rarely, it's being used with platinum product line that, like previously, E7 compared to E5 is priced higher. And this is for normal dual socket servers that consist the volume of IT in cloud hardware, and this was by the original definition of  NFE to run the volume hardware. This is how we blended that and deployed most of it. And then we have system on the chip type of products that are used, for example, for universal CPE on-premise environment that are also virtualized, running their workloads. For the Intel Xeon scalable processors in communications workloads, for data plane, primary value comes from higher I/O bandwidth, 50% compared to CPUs and Also, higher memory bandwidth, also 50% so just by running the same workloads on the current platform compared to what was with Xeon E5. It's simply packets are simply passing more easily through it, So, we get much faster data rates on it. And we also have the crypto and compression acceleration in some of the version of the chipset. Or if not available in the server chipset, it can be added on the adapter on the PCIe card. And then we have also other platform ingredients that are improving overall the workload situation on those platforms. So, regarding Ethernet adapters, we have here sample of those that are being produced by us as either single port, dual port, and quad port on different connectivity and protocols. So, you can see here, for example, support for 10 gig, 25 gig, and 40 gig Ethernet ports. Those are Roman numbers. So, for example, X means 10. XXXV means 25 gig Ethernet port, so that's easy to read. And then we have also these type of Ethernet controllers available on products that are coming in blades. In most cases, this is also still produced by Intel but for according to the spec of appropriate server OEM blades. For QuickAssist technology, this is accelerating encryption and compression workloads. Here is a list of all the relevant encryption workflows that many are mostly here listed used in the comms workloads. And the way how the technology is offered is either as a separate chipset where this can be added in, for example the storage devices, we have designed bins for that.[O4] Or now, this is included in some of the server chipsets. It also is available as PC Express plug-in card so that normal rack-mounted servers can also have it, and this is certified with most of the usual IT server OEMs-- this type of PC adapters. And then you also have system on the chip version. So, integrate that together with everything else that is coming on the same piece of silicon. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[2459, 2470, "Virtualization"], [2505, 2535, "Intel Xeon Scalable Processors"], [3809, 3831, "Intel QuickAssist Technology"], [1868, 1900, "Intel Xeon Scalable Processors"], [1314, 1338, "Intel QuickAssist Technology"]]}
{"id": 23, "data": " [O1] The concept of platform awareness required for correct placement of data plane workloads we call Enhanced Platform Awareness or shorter EPA.[O2] And here is an example of physical server layout that because the volume is with dual socket servers, in this case, we have it here. And the way how the different buses and ingredients are connected is that we have two NUMA nodes and BCA buses connected to the appropriate NUMA nodes and memory equally so as per the definition of NUMA. And then this being the physical layout of the server, now, regardless of which workload placement we use.[O3] And originally, all the patches were done for OpenStack. In the meantime, all OpenStack distributions that are relevant are including it. So, it started initially with Wind River and then all the other relevant distributions like RedHat and so on included it. Integrated OpenStack has the required pages to understand how to correctly place data plan workloads on it. And now, recently, we also did similar patches for Kubernetes environments for bare metal where you have here example of good placement where the workload is pinned to the appropriate cores on the server in the green here. And memory is accessed using huge pages on the same NUMA node. And then also the traffic is-- data plane traffic is fed in and out over the adapters that are connected to the PCIe buses on the same NUMA node. If we don't connect the adapters onto the same NUMA node, we need to pass the interconnect. And even way below the bandwidth limitations or interconnect, the way how the environment is really highly tuned, we will already have packet drops, and this is simply not the way to place the workloads for a data plane-- high throughput. Equally so we need to have the memory configure it with huge pages and assets. On the right, NUMA node not passing the interconnect-- again, going to the other side of it. So, this requires that the server has what we call balance I/O, meaning that adapters can be physically connected to both sockets, and this is something that more and more servers are having. But it's still worth checking because there are some of those that initially were used only for IT workloads. It worked perfectly well for control plane, but it cannot really do cost effectively the data plane ones.[O4] So, lists of features that need to be taken care of in placing the workloads include a number of cases for connectivity configuring Single Route I/O Virtualization or SR-IOV, configuring huge pages, understanding the NUMA nodes, and also building virtual CPUs two cores. And it can also include further features of hardware and software. Some of them are listed here. So, CPU models to be chosen with the right instructions, taking care of the caches, and the buses in the server, virtual switch, real-time features, trust execution technology, and similar ones. Long time ago, we measured what is the impact and came to the conclusion that for data plan traffic, this simply needs to be done and also came very early to further conclusions that workload placement-- every workload placement needs to be aware of this type of physical layout and that simply descriptors of those functions need to announce the requirement for that. And then the lowest layer of the orchestration stack needs to be aware of those descriptor requirements and hit appropriate APIs on the Virtual Infrastructure Manager layer. So, this is, for example, OpenStack API is exposed, but they need to be appropriately consumed. Or now when we have similar in Kubernetes environments, also to be consumed over there. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[1017, 1028, "Kubernetes"], [2463, 2477, "Virtualization"], [3547, 3557, "Kubernetes"], [2481, 2487, "SR-IOV"]]}
{"id": 24, "data": " For systems that require high number of input/output operations per server or lower annual failure rates, we are using SSDs, and SSDs generally fill the void that existed between system memory and hard disk drives, traditional ones, where CPU's caches were, of course, the fastest from the access time than it was memory. And then there was the big gap to get to the hard disk drives. And they were first initially based on 3D NAND technology filled with SSDs at that time SATA and faster PCIe1s. And recently, we introduced Also, the update SSDs, which have much faster response time. And now, most recently, we introduced also obtained memory.[O2] Both of those obtained SSDs in memory are based on 3D XPoint technology where one is coming in the form factor of SSDs, and other one is coming on the DIMM slots. And also memory can be made persistent. So appropriate positioning for different type of workloads is presented on the slide here where on the top, we have obtained SSDs, which are much faster in IOPS in access times. So lower latency has higher quality of service and are also longer for endurance. And then the 3D NAND-based SSDs, PCIe-based one in the middle-- they have higher capacity. And then we also have the where needed where we still want to have SSDs but don't need these type of best features in category.[O3] We can also made it on the SATA-based ones. And an example of obtained SSD drive on PCIe interface, in this case here, is P4800X, as in marketing terms, the most expensive data center SSD. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles.", "label": [[119, 124, "Intel SSD"], [130, 134, "Intel SSD"], [456, 460, "Intel SSD"], [543, 547, "Intel SSD"], [674, 678, "Intel SSD"], [765, 769, "Intel SSD"], [979, 983, "Intel SSD"], [1141, 1145, "Intel SSD"], [1272, 1276, "Intel SSD"], [1408, 1411, "Intel SSD"], [1521, 1524, "Intel SSD"]]}
{"id": 25, "data": " Welcome to our session on introduction on single route I/O virtualization-- shorter, SR-IOV. For beginning, let's just say that when servers are physical and connected to physical network switch [O3] which, in this case, for example, ethernet cables-- we have 1:1 relationship between application and the hardware. When we virtualize those environments, then there is one physical server connected to physical switch, and then there are multiple virtual machines running. They think they are talking to some adapter. But this adapter, in case of virtual machines, is also virtualized. And here in example is where inside the host operating system there is virtual switch exposing different ports to different virtual machines. To connect virtual machines to network we have different methods. We can directly assign network interface card to the virtual machine. Or we can also expose the same card to multiple virtual machines. Or if you're using virtual switch, then we are exposing virtual interfaces to the VMs. Direct assignment is one of the methods where the whole network device is given to one virtual machine. So, this means that the virtual machine will own fully this network device. The data is sent from the VM directly into memory using direct memory access. This is one of the functionalities provided by Intel virtualization technology for direct I/O, or VT-d.[O4] And this allows for near-native performance due to the connectivity being direct from one VM owning full network device. And in this case, then, of course, device cannot be shared with other virtual machines or be used for work by the operating system because it is fully assigned.[O5] This is not the most flexible setup. And this is why PCI-SIG, which is a peripheral component interconnect special interest group-- at that time led by Intel and a number of other companies-- defined the standard called single route I/O virtualization and sharing. Normally, we abbreviate it only to SR-IOV, where the objective is to expose such PCI device as looking to the virtual machines then supported by the server BIOS and hypervisor with this type of virtualization technology for direct I/O, or VT-d, which allows it to be kind of partitioned and exposed as multiple devices to all the different virtual machines. In this way, we will be sharing the device. To define that, there is a physical function which needs to be addressed. And over there, we can then define multiple virtual functions. And then, those virtual functions are exposed to the virtual machines so that it talks directly to virtual function, and by this way avoids the overheads of using NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please delete. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles. [O5]Please merge these two subtitles.", "label": [[60, 74, "Virtualization"], [86, 92, "SR-IOV"], [324, 334, "Virtualization"], [572, 584, "Virtualization"], [1328, 1342, "Virtualization"], [1906, 1920, "Virtualization"], [1969, 1975, "SR-IOV"], [2128, 2143, "Virtualization"]]}
{"id": 26, "data": " [O1] A summary of mentioned hardware ingredients and related software ones, we can see that most of the data plan configurations and control plane configurations that we have today fits very nicely on Intel Xeon Gold type of CPUs. Where I was mentioning those 20 cores for data plan, we can have the middle count, of course, like 12, 14, on Xeon scalable for control plane and other controllers and managements and IT workloads. And then, we will put appropriate number of ethernet controllers for data plan on balanced I/O, meaning feeding both sockets. And if the workloads also require higher amount of I/O per seconds, then we will start putting more and more SSDs into the nodes. And we also have option of using the quick assist accelerator to accelerate some of the encryption algorithms. So, this is summarized here on the left side in this diagram. So, we have the server configuration down there. The hypervisor is then going to allow running multiple virtual machines on top of it. Some of the hardware ingredients if used, for example, Quick Assist here or few of the other ones or the ethernet controllers also can be exposed through SR-IOV directly through the virtual machine, or hypervisor can also have virtual switch that then needs to be optimized with Data Plan Development Kit feeding packets over the virtual switch or virtual router, for example, for control into the VMs. And the applications there to be efficient to data plan also then need to use Data Plan Development Kit or other mechanisms that increase packet processing. To have this whole stack properly configured, we need to use the concepts like enhanced platform awareness so that we recognize the physical layout of the server replace the workloads accordingly. And then the whole orchestration stack is going to configure all the elements and put them as needed into the service chain. The important concept here is that, as there are many layers of the stack, the higher we go, the more abstracted it should be, and expose minimum that is required for performance reasons. And because here we talk about high amount of data plan throughput, in a number of cases, it turns out that there is quite a lot that needs to be exposed and configured, and to get to fully automated environments, it is quite some effort. And we are handling this in both ways. One is getting smarter and faster in handling it the current way. The other one is investing in decoupling those layers so that the on-boarding and the utilization of these type of environment becomes easier and we start moving towards more of the cloud environments.", "label": [[665, 669, "Intel SSD"], [1148, 1154, "SR-IOV"], [2494, 2506, "Utilities"], [1633, 1660, "Enhanced Platform Awareness"]]}
{"id": 27, "data": " In more details in SR-IOV architecture we have hardware virtual ethernet bridge, which is where all the physical and virtual functions are being connected. Physical functions have physical function driver exposing them as a PCI device to the OS. And then, virtual functions have equally so virtual function driver exposing them into the higher levels of the software stack, so to the operating system there. And the way how this thing is really stitched together into the robust tech is that there is Intel virtualization technology for direct I/O-- or shorter, VT-d-- which is providing the functionality of mapping I/O devices to the virtual machines. It is also doing the direct memory access and interrupts remapping[O2]. And it improves reliability and security because it does the isolation of the data parts for different functions underneath. The benefits of using SR-IOV is that a single physical device can be exposed as multiple virtual devices to the OS, and then to the virtual machines. And this is all based on PCI-SIG specification. And we get to near-native performance between the virtual machines and the external ethernet ports because this is completely bypassing the hypervisor. It does protect the data with isolation of Intel VT-d. And it does the automatic receive and transmit load balancing with round-robin function scheduled with also bandwidth limiting transmit function where needed. And each virtual function, that gets dedicated resources, its own receive and transmit buffers, and its own descriptors. This way, we can, between virtual machines for the traffic, implement different connectivity scenarios on the same physical host. So, the traffic, that is then called east-west. We can do with virtual switch and with virtual functions. Inside the adapter then also switched, there is between different hosts, So, traffic exiting physical server. This is North-South, also supported through virtual switch and through virtual functions. And then, you can combine the two into traffic that runs inside the server. NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles.", "label": [[19, 26, "SR-IOV"], [508, 522, "Virtualization"], [874, 880, "SR-IOV"]]}
{"id": 28, "data": " In more details on how SR-IOV gets implemented here on example of Intel Ethernet Controller from 710 family is the adapter itself has number of supporting functionalities. So, there is virtual Ethernet port aggregator. There is virtual Ethernet bridge. And there are then physical functions and virtual functions. Number of supported virtual functions per device is 128. And then, appropriately, either over virtual switch or connected with SR-IOV, we will have appropriate number of VMs connected. Logical equivalent is if we are doing SR-IOV in containers. And there, we have virtual station interfaces connected to --- to those virtual functions. If we look at how network traffic would be flowing when using virtual function direct assignment to the virtual machines, we will have examples starting w-h-- on the same host. So, for East-West, we can get-- using 40-gig adapters because of the PCI limitations, we can get on dual port adapters to two times that much. But let's say that we get to 44 gigabit per seconds. And the numbers will, of course, vary per packet size and-- but this way, we can then-- inside the adapter, until we reach the bandwidth limits, we can get this thing through the virtual Ethernet bridge. When we do more of that, it is eating into the same aggregate bandwidth. So, the numbers get smaller when we are starting to chain more functions there. And eventually, as we continue chaining those functions, the numbers get further smaller because the aggregates needs to stay the same. And when we have, like, six virtual functions utilized and, between them, we are only sending-- in this case, instead of 40-something, we are sending 6-point-something gigabit per second. When we start also combining this thing with traffic leaving the server, then from-- in this example here, we have one physical and four virtual functions used. The transmit is used for the-- only for the default traffic of netperf. And the VMs are talking between themselves, for example, with 11 gig. But then the external traffic going out is then also 11 gig. And the rate of this external traffic, by design of the adapter, can be limited by using low-level tools, like given here in the example. And here, it's important to note that, while the adapter itself-- and firmware has a lot of functionality-- we also need to look at how well-supported this firmware is and can we get these type of adapters from what is the normal adoption channel, where, normally, communication service providers will be  buying from the OEMs and the telco equipment manufacturers. What is available in this type of firmware there? What is supported in drivers that are being used for different environments? And then also very important is to have the whole stack fully managed and automated. What is supported in the desired software-defined networking solution?[O2] So beyond just pure functionality of the adapter, it's important to look at the whole stack because only then we can fully automate it. [O2]Please merge these two subtitles.", "label": [[24, 30, "SR-IOV"], [442, 448, "SR-IOV"], [538, 544, "SR-IOV"], [1563, 1571, "Utilities"]]}
{"id": 29, "data": " Looking at how ethernet controller 710 series implements different network virtualization and NFV models, we need to look at, in particular, control parts and data parts and the concept of network virtualization edge, which on the underlay of the network sits and uses the underlay to connect physically to the real network while exposes its parts to the virtual machines. And so, on the underlaying, the layer 3 is being used. And to the virtual machines it talks real internet frames. And this is how pretty much the VMs think they are having the real network. And this NVE can be implemented in virtual switch, in physical switch, and also in other parts of adapters here. So, we have here Model 1 is for kernel data paths. It is not being optimized with DPDK, so, it would work for a very low rate ofpathsonly. The second one is using more data paths where virtual switch example here given is obvious. It can be also other virtual switches or equivalent virtual routers like VMWare, Contrail or similar optimized with DPDK, where then NVE is providing this type of functionality inside the virtual switch, and then also in the real switch. And all this needs to be properly configured with some goods as the end controller is supporting that example here is with OpenDaylight. But then, also OpenStack has appropriate interfaces for it. We have Model 3, which includes the bypass and the trusted VNFs. So, this is where we're exposing those virtual functions with SR-IOV and bypassing the hypervisor virtual switch for data traffic. And then, inside the VM, directly mapping it with appropriate drivers to the virtual functions of the adapter. And then, we have the fourth model, which is also using the bypass. And then, the difference is that NVE concept on the third one is sitting inside the VNF. NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[76, 90, "Virtualization"], [95, 98, "NFV"], [198, 212, "Virtualization"], [759, 763, "DPDK"], [1024, 1028, "DPDK"], [1470, 1476, "SR-IOV"], [1402, 1405, "VNFs"]]}
{"id": 30, "data": " [O1] In summary, SR-IOV is used to directly connect virtual machines to the adapter, and this way by passing the hypervisor. The drawback of doing that is that in VMF, we have the driver as hardware definition of the adapter. So, it brings challenges in lifecycle management over multiple years of production. And this is performance trade-off to using the virtual switches, that for data plan would require a certain amount of CPU course to be used, but would provide proper nicer obstruction of what they owe to the VMs. As we are getting more and more optimized virtual switches and virtual routers with BTK, we expect that for what's in NFE we are doing with 10 gig and 25 gig ports, we will be using more and more of those virtual switch solutions, compared to what, up to now, most of the deployments for performance reasons we're using SR-IOV. So already mentioned, primary benefit is that we are avoiding the virtual switch. This also has drawbacks because virtual switch is providing quite a lot of functionality when it comes to network virtualization with overlays. The data protection is coming by isolation with Intel VTD. Transmit load balancing is having round robin scheduling for our recent takes with bandwidth for transmit rates can be limited per virtual function. And then also, per virtual function, we are getting dedicated resources, transmit and receive buffers and cues, NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[18, 24, "SR-IOV"], [844, 850, "SR-IOV"], [1048, 1062, "Virtualization"]]}
{"id": 35, "data": " First of all, you can go and download the deep learning frameworks that have been optimized by Intel to run on the Intel CPUs. There's a number of these frameworks. Common frameworks that have been optimized, such as TensorFlow, MXNet, Caffe. And these are available to download at ai.intel.com/framework-optimizations/ . I'd like to draw your attention also to the big DL library for Apache Spark. So this enables deep learning applications to be built upon Apache Spark or your big data Hadoop platforms and infrastructure, again making use of the data and the infrastructure that you have already. One of the other questions we often get asked is, do I need acceleration? And the answer really is, most enterprises are using CPUs for their AI and deep learning applications today. Some may reach a performance threshold where they need to use accelerators. But this really tends to be further down the line, either after the proof of concept has been completed or the initial deployment has been completed, when the artificial intelligence applications are moving to a real scale process. In which case, the review of the performance requirements then need to take place. And at which point acceleration may be justified at that point. Other resources that are available are the Intel AI Academy. Great place to start for developers, students, instructors, and start-ups in artificial intelligence. Here you can learn all about artificial intelligence. You can download tools, resources, begin development with artificial intelligence, find a range of course materials, be able to teach others and spread the knowledge and share what you've learned around artificial intelligence within the artificial intelligence community. So to get started, go to the software.intel.com/ai for the AI Academy. We also have an AI Builders program, which has a solutions library. There are 30 plus public white papers published here that can provide insights and inspiration and information around solutions that have already been built around artificial intelligence. To access this, go to builders.intel.com/ai and Solutions Library. We also have our AI Builders ecosystem, where we have over 120 AI partners covering horizontal solutions, vertical and industry focused solutions, as well as cross-vertical solutions, and system integrator solutions. For more information on the AI Builders, please go to builders.intel.com/ai. So in summary, to find out more, please go to ai.intel.com to learn more about Intel's solutions for artificial intelligence, and builders.intel.com/ai for the builders program and for our partner information. If you want to explore and start using some of the software in the frameworks, please go to software.intel.com/ai, where resources are available to download. And if you're looking to engage in any proof of concept trials or deployments, please reach out to your local Intel NOTICES AND DISCLAIMERS", "label": [[2520, 2543, "Artificial Intelligence"], [1432, 1455, "Artificial Intelligence"], [1695, 1718, "Artificial Intelligence"], [744, 746, "Artificial Intelligence"], [1289, 1291, "Artificial Intelligence"], [1789, 1791, "Artificial Intelligence"], [1816, 1819, "Artificial Intelligence"], [2142, 2144, "Artificial Intelligence"], [2188, 2190, "Artificial Intelligence"], [2370, 2372, "Artificial Intelligence"], [1020, 1043, "Artificial Intelligence"], [1378, 1401, "Artificial Intelligence"], [1515, 1538, "Artificial Intelligence"], [1660, 1683, "Artificial Intelligence"], [2033, 2056, "Artificial Intelligence"]]}
{"id": 36, "data": " Hello. Today, we're going to talk about wireless technologies and about wireless networks that use these radio access technologies.[O1] We start first, with a little bit of an overview and background on how to set the scene for the later content that we have in this presentation. Original networks made use of physical medium in the form of cables to transmit information from one point to another, but with the invention of radio, it was possible to use the air as such medium. And this came with the benefits of mobility-- users weren't tied to the end of a cable to be the transmitter or the receiver of such communications. The air is a medium that can be shared, unlike a cable where it can be really shared between multiple users. The air as a medium can be reused by many different transmitters and receivers. But there need to be certain rules so that they can do that. In order to do that, typically, there is air or the spectrum has been divided into frequency bands that are then used by each type of communication. And as long as its network is using their own portion of the spectrum, they can do so without interfering with each other. Wireless technologies and radio technologies have been used for many different types of uses, from wireless networks-- which is the main topic for today-- but it also has been used for things like radar-- for example-- as well. The first wireless telephone network was created in the 1950s. And since then, we have gone throughout multiple generations of this type of cellular networks that were originally created then. How the spectrum is used is heavily regulated. We'll see in this slide, the example of how the UK allocates their spectrum between all the many different users they can have. This slide shows the complexity of how that is managed. So, at the national level, there is usually a regulator that, very carefully, uses and manages this spectrum and makes the most use of it because it's a public resource. When we look at the spectrum-- how it is used-- there are many-- mainly, three types of classification of the spectrum. We talk about licensed spectrum, unlicensed spectrum, or shared spectrum. Licensed spectrum is typically leased to a single owner. This single owner-- typically, a service provider-- will be the only user of that spectrum. And with the knowledge that there is not going to be a known interference from other transmitters in that network. And that capability gives the service provider the tools to deliver quality of service to the users by managing completely the spectrum and how it is being used. Examples of licensed spectrum and wireless technologies that use that licensed spectrum are cellular technologies such as LT or 5G and all the previous generations. Unlicensed spectrum, on the other hand, doesn't have a single owner. It's still regulated, but there are multiple users that can use that spectrum as long as they follow the rules that have been set. Such rules will include things like transmit power, certain limitations of transmit power, or certain coexistence rules to ensure that multiple transmitters and multiple receivers can actually make efficient use of that spectrum. Examples of wireless technologies are wireless networks that use unlicensed spectrum are Wi-Fi or different types of generations of Wi-Fi and Bluetooth, as well. In between licensed and unlicensed, we also have shared spectrum. Shared spectrum sits between the two. It typically operates in short leases So, it's very dynamic in the way it is being used. Examples of shared spectrum are CBRS, which is common[ND2]ly OnGo in the US. When we look at wireless networks, we can also classify them in the range that they have and the coverage area that they provide, we have personal area networks. Usually very short range. They're within a person or within a small area, such as a room. Coverage is about 10 meters or so. Examples of this type of technologies are Bluetooth-- for example. Local are networks, on the other hand, provide a little bit bigger coverage. These would give coverage around a building, around our campus. It could be a residential property or it could be a stadium. For example, in the range of hundreds of meters, a kilometer to the most. Examples of local area networks are Wi-Fi and similar type of technologies. Providing bigger coverage and wider coverage are wide area networks or WANs. These provide a metropolitan coverage or even national coverage. Even in the case of satellites, they can provide international coverage, as well-- worldwide. CONSUMER vs. IoT TECHNOLOGIES Finally, we have the services that are being delivered on top of these networks and radio access technologies.[O3] We have on one side, consumer-oriented services. These types of services go from voice and telephony to short messaging, media streaming, mobile broadband-- these types of services that are given to consumers. On the other hand, we have services that are delivered to things and machines. So, this is where IoT or the Internet of Things and machine-to-machine communications come into place. There is also a wide variety within these types of services. For example, you can have a sensor that is just delivered in short bursts of data once or twice a day. Or on the other hand, you could have a streaming video camera, a streaming video at very high definition. We have new types of machines required in different types of services within this domain, as well. You could have a robot that requires very reliable and very low latency type of communication. Or you can have an automotive that requires, again, NOTICES AND DISCLAIMERS [O1]Please unite subtitle 5 to 4 as the word \"technology\" refers to \"radio access\" (sub. 4). [ND2]Common / cming??? OnGo??? [O3]Please move this subtitle to the previous one so that it doesn't stand alone.", "label": [[41, 50, "Wireless"], [73, 81, "Wireless"], [1152, 1160, "Wireless"], [1251, 1259, "Wireless"], [1390, 1398, "Wireless"], [3201, 3209, "Wireless"], [2628, 2636, "Wireless"], [3227, 3235, "Wireless"], [3637, 3645, "Wireless"], [106, 131, "Radio Access Technology"], [4677, 4702, "Radio Access Technology"], [5015, 5018, "IoT"], [5026, 5044, "IoT"], [2722, 2724, "5G"], [4575, 4579, "IoT"], [5709, 5721, "Radio Access Technology"]]}
{"id": 37, "data": " One very important type of wireless networks are cellular. Cellular are being used worldwide, and there are billions of users of cellular networks. Let's look at some of the characteristics that make a typical cellular network. The name cellular comes because of the topology, where to reuse the spectrum to make very efficient use of the spectrum, which is a very scarce resource, transmitters, or base stations, are spread around a geography to make sure that each base station is actually using the local resources around it. Cellular networks are a type of wide area network. So it is providing coverage Usually on a nation or a country. And it provides mobility, so it means that users are able to move between different cells, whether they are walking, or whether they're in a car or on a fast train, or any other type of vehicle. Because cellular networks are using licensed spectrum, they are capable of delivering different types of quality of service and different type of services. They can deliver consumer services, such as basic voice to enhanced mobile broadband, and in some cases, as we will see, also deliver M2M and IoT communications. Because they are public networks used by many, many users, they have very strong security mechanisms to ensure that both the network and the users are protected. The use of SIM cards protects both the network and the user, so the user can trust that the network is talking is not a rogue network. And the network also can trust that the user accessing it is not a rogue user. Looking at the history of cellular networks, we'll start with 2G. It came in the 1990s. There were actually multiple technologies that fell under the 2G umbrella. In different regions of the world, we have different types of technologies being used, such as GSM, CDMA, et cetera. All these networks generally only provided voice service, telephony service. In the 2000s, we saw the operation of 3G. 3G brought data services on top of voice that was given in the previous generation. There were mainly two standards under the 3G umbrella, UMTS and CDMA 2000 used in different regions of the world. With 4G, we saw really mobile broadband. The data services being given by 3G were fairly slow. Latency was high. It wasn't the best user experience. But with 4G all those issues were resolved. And we saw really real mobile broadband being delivered to users. There were two main contenders for the 4G crown, two different technologies. One was WIMAX, the other one was LTE. But LTE became really the global standard. In the next year or so, we will see the first 5G networks come into the market. 5G-NR are specified by 3GBP is the global technology of choice, which is undisputed. 5G is going to bring the different type of services to what we saw before. All the previous generations were mostly centered around delivering consumer services. 5G is going to expand the portfolio of users and the different type of use cases that can be delivered over 5G. It's going to be focused on M2M, on internet of things, connecting millions of devices, providing ultra reliable communications, providing low latency communications, in addition of also delivering the typical consumer services that we know today. In this table, we can see a comparison between the different type of technologies and the different types of generations. We had GSM for example, whose primary services delivered were voice and some M2M, but the downlink and uplink speeds were fairly limited at 14.4 kilobits per second. GPRS provided some advancements in the data services that could be delivered, but didn't amount to much more than being able to do email. In the third generation, we saw various technologies like UMTS, TD-SCDMA and CDMA2000. That provided voice, video, and some data. The data was good for browsing, but really not good enough for other type of services, like video streaming. Part of the evolution of UMTS was the HSPA, high speed packet access. This enabled more advanced mobile broadband, much higher throughput, but latency was still an issue. This was resolved in 4G. Both WIMAX and LTE technologies were very much focused on delivering data services and mobile broadband, high quality mobile broadband. WIMAX wasn't developed as quickly. LTE became the global standard, but we can see that WIMAX delivered 70 megabytes per second downlink, and up to 70 megabytes per second uplink. LTE in its various generations can deliver, can achieve much higher throughput, up to three gigabytes per second downlink and 1.5 gigabytes per second uplink. LTE eventually in further generations of LTE also delivered voice services on what's commonly known as voice over LTE, and also has been enhanced to deliver some IoT services with the operation of for example, narrowband IoT or category M1 type of services. With 5G-NR, all of this will be delivered almost from the start. Right now, initial focus has been enhanced mobile broadband with much higher speeds and much higher capability than LTE. We can see the peak theoretical downlink speeds are 20 gigabytes per second in the downlink and 10 gigabytes per second in the uplink. But quickly after that first release, we will see the operation of new type of services like ultra reliable, low latency communications, and massive IoT that will enable these broad range of uses NOTICES AND DISCLAIMERS", "label": [[1136, 1139, "IoT"], [4831, 4834, "5G"], [5296, 5299, "IoT"], [2592, 2594, "5G"], [2626, 2628, "5G"], [2711, 2713, "5G"], [4729, 4733, "IoT"], [4789, 4792, "IoT"], [2873, 2875, "5G"], [2980, 2983, "5G"], [28, 36, "Wireless"], [3020, 3039, "IoT"], [2497, 2501, "LTE"], [2507, 2511, "LTE"], [4109, 4112, "LTE"], [4265, 4269, "LTE"], [4409, 4412, "LTE"], [4568, 4571, "LTE"], [4609, 4612, "LTE"], [4682, 4685, "LTE"], [5007, 5010, "LTE"]]}
{"id": 38, "data": " We're also going to look at that IoT wireless technologies, mobile type communications, and machine-to-machine. Many of us have seen all different types of reports saying there's going to be an explosion in the number of devices that are going to be connected to the internet. With reports even claiming that there are going to be 50 billion connected devices by 2020. Many of these connected devices will use personal area networks, will use other types of technologies, but we're going to look at this graphic at specifically, the cellular or wide area network market. We see it growing exponentially and we see various types of technologies being used for these types of devices connecting using cellular networks. The use cases are pretty varied from smart parking, where car park owner is providing online information about what parking spaces are free and what parking spaces are in use. And helping users find empty spots as quickly as possible. To consumer focus, such as your smart fitness watch or children and elderly type of trackers. Goods tracking, as well, letting you know when your package is going to be delivered. Or letting a business know that their goods that they need for their factory are arriving in time for their operations. To sensors providing environmental monitoring of air quality, water quality, et cetera. Point-of-sales terminal in retail environment are also some of the IoT devices that use technologies, especially cellular technology, especially for mobile type of applications. Smart meters, as well, are using these type of technologies to help service providers or the utilities gather meter information without explicitly having to send someone to the premise. Smart home is also with operation of Alexa-- these types of devices. Or with automation in the house. And finally, also, another example would be agriculture. Putting sensors in crops or putting sensors in cows to make sure the location is known. To make sure that they're healthy. And that they are tracked easily. But all these different types of use cases, all these different types of services require different types of technologies. On the top end, we see the high-end performance type of services. These could be a video camera that is uploading high definition streaming video. Could be an autonomous car, where the driver is enjoying a movie while the car is taking their user to his or her place of work. Different here, in the middle, there is more cost-sensitive. Still requires in the order of tens megabits per second, but they're much lower cost and much more sensitive to cost. And on the bottom, we see the mass volume IoT. These types of technologies are more used by sensors that typically send short bursts of data every few hours or a day. These types of devices are usually battery operated. So a key characteristic is that battery has long-lasting life. Looking at the technologies that are delivered by 3GPP along the years, we saw that there was some M2M type communications being delivered by 2G. This was very efficient. It used a very narrow spectrum. And it was very economic. With 3G, the same type of services that was delivered to consumers in terms of data and where it could also be given to a device or to a thing. Because it wasn't really purposely done for things, it wasn't that effective. In the first generations of LTE, we saw something similar happening. It was you could deliver two things the same type of services that you could deliver to a consumer. So it wasn't really a great fit, but eventually, with release 13 in 2016, we saw the operation of narrow band IoT, or as is now commonly known, category NB1. This was still used in form of LTE, but was much more purposely done for things. Typically, this was mostly aimed at sensors. Some of the capabilities of narrow band IoT was that it much improved link budget, which allowed for the range so allowed things to be located much farther than the base station. Or even underground connectivity where typically, the signal didn't penetrate. This type of CAT NB1 was able to go deep in the ground. We can see in this table, a comparison between the various wireless technologies being used for IoT. We see a comparison here between two different types-- licensed technologies, which we saw in the previous slide like 2G, 3G, LTE, et cetera-- to some unlicensed technologies that are being used specifically for IoT, like SigFox and LoRa. There are other similar technologies out there, as well, but just to focus on two of the most popular ones. Category one is a type of technology that uses LTE, standard technologies, it uses the same type of capability as you have in LTE devices. And the type of transmission that they send, the type of data rates are limited to 10 megabits per second in the downlink and five megabits per second in the uplink. A device that uses category one typically doesn't operate only on a battery or it has to be recharged often. And in terms of connections per cell is typically, the same type of limits that we have for users, for consumers. With category M1, we saw a new type of technology based still on LTE, but use very focused on things. This is where we start seeing more appropriate data rates and much longer battery lives. And with that, also, the cellular network could provide services to many more devices. And also, the cost of these devices was considerably less. Category NB1 was taking that step further. Lower data rates. Similar type of battery life. And in terms of the number of devices that could be supported on the network, still much larger numbers. GSM also has been evolved. And EC-GSM is also, a new type of technology that is based on GSM and it operates in the GSM spectrum. That kind also delivers similar type of KPIs as category narrow bands one. Comparing this to the unlicensed technologies, such as SigFox and LoRa, we also see similar types of KPIs as we saw with the licensed technologies, such as category narrow band one or EC-GSM. But those types of networks are typically-- because they use unlicensed spectrum, quality of service is not guaranteed. NOTICES AND DISCLAIMERS", "label": [[34, 37, "IoT"], [1409, 1412, "IoT"], [2641, 2645, "IoT"], [3612, 3616, "IoT"], [4196, 4200, "IoT"], [4413, 4417, "IoT"], [38, 46, "Wireless"], [3827, 3830, "IoT"], [4160, 4168, "Wireless"], [3362, 3365, "LTE"], [3692, 3695, "LTE"], [4327, 4331, "LTE"], [4596, 4599, "LTE"], [4675, 4678, "LTE"], [5142, 5145, "LTE"]]}
{"id": 39, "data": " We're also going to spend some time looking at how unlicensed spectrum is used, and the most typical radio access technologies that make use of that unlicensed spectrum. We're going to look at Wi-Fi, the most typical use, but we're also going to look at some of the new technologies that are coming over the last few years that are also using this unlicensed spectrum. So, we're going to see some of the differences between them. First, let's focus on Wi-Fi. Wi-Fi is a part of a family. It's a generation of technologies, started in the late 1990s with the operation of 802.11b. There's been multiple generations 11ad, 11n, 11ac. And now the industry is working towards 11ax. We'll see the first type of devices coming up in the market next year. Each generation has given improvements in similar ways that we saw in the cellular technologies using license spectrum. We've had a similar type of advancements generation by generation in the Wi-Fi family. 11ax is poised to provide much better coverage and much better capability in dense environments. It's going to deliver much faster throughputs compared to 11ac or 11an. And with all of that, it's going to deliver much better efficiencies for the network. An additional benefit that 11ax brings is also the extended battery life, which will help mobile users, particularly. In this table, we can see a comparison between all the different generations of Wi-Fi technologies and how it has advanced over the years. We saw the first generation of 802.11, the first Wi-Fi technology, used 2.4 gigahertz, unlicensed bands, with a bandwidth of maximum bandwidth of 20 megahertz. This could deliver two megabits per second. In the second generation, 11b, using similar type of spectrum characteristics, their capability was to deliver higher throughput up to 11 megabits per second was introduced. With 11a in 2003, we saw that five gigahertz was introduced, so going from 2.4 gigahertz to five gigahertz spectrum, but still using 20 megahertz. This brought big throughput improvement up to 54 megabits per second. If we compare that to 11n, very typical, very widely used today, was introduced in 2009, 11n used both 2.4 and 5 gigahertz. And it could use up to 14 megahertz. In addition to that, a new capability came which was to their capability of using different streams, different antennas to deliver much higher throughput. We can see up to 600 megabits per second. With 11ac, which is the last generation that we've seen in the market, spectrum remains the same, 2.4 and 5 gigahertz. But we see channels of up to 160 megahertz. We also see many more antennas being used, up to eight in the downlink, which brings the maximum big data rate that can be achieved up to 6.8 gigabits per second. All this throughput could also be directed to different users using multi-user MIMO technology. In the case of 11ac, it could only be delivered in the downlink. But this also meant that certain users that support this capability could deliver-- could receive much better throughput. The next generation that's coming next year 802.11ax will improve some of these capabilities. The channel will still remain up to 116 megahertz. The number of antennas will be 8, but the multi-user MIMO capabilities will be greatly increased, supporting up to 8 multi-user streams in the downlink and another 8 in the uplink. Advancements in other areas also bring an increased big data rate of up to 10 gigabits per second. There are some market trends happening over the last few years of how unlicensed spectrum in those bands typically used by Wi-Fi, such as 2.4 gigahertz and 5 gigahertz can be used. And these market trends are about using LTE technology in those bands, particularly 5 gigahertz. We see three different types of variants on these. One is LAA, or licensed assisted access. The other is LTE-U or LTE in unlicensed. And the last one is MuLTEFire. We'll see the difference between these three technologies. But why is there a push to use LTE instead of Wi-Fi in the 5 gigahertz unlicensed band? The expectation is that LTE, using LTEs in these unlicensed bands will bring much better integration with the cellular network. Wi-Fi is our local area network technology. So for those users that are mobile, their capability of moving between local area network to a wide area network when they are out of reach of the local network is very interesting. So that integration with the cellular network is a very key capability that LTE brings in that unlicensed. Also LTE, because of the way it has been designed provides better mechanisms to deliver quality of service to the users. Because it's unlicensed, it's very difficult still to guarantee quality of service, but with LTE, the expectation is that it will be better at managing quality of service. LTE has also been optimized to provide better radio coverage and operate better and more efficiently in denser environments. LTE has also been built with mobility in mind, and as such is a very robust technology. With this some of these are also things that are being improved with 11ax, the latest Wi-Fi technology that will come next year. So, whether this market trends succeed or not will really depend on how well 11ac actually delivers some of these capabilities of providing better radio coverage, working better in denser environments, et cetera. Let's look at the differences between LAA or license assisted access, LTE unlicensed and MuLTEFire. Let's start with LAA. LAA is a 3GPP technology. It consists of pairing licensed spectrum with unlicensed. So we typically have two carriers. One is LTE operating in license spectrum. And that is augmented by using unlicensed spectrum. This unlicensed spectrum is accessed using what is referred to as listen-before-talk. Listen-before-talk is the mechanism that Wi-Fi uses to ensure coexistence and ensure there are multiple Wi-Fi networks that can coexist in the same space. So LTE is adopting these mechanisms and using them to ensure there is good coexistence with other Wi-Fi networks in the vicinity. Then we have LTE-U, LTE unlicensed. LTE-U operates on the same concept as LAA. You still have LTE carrier in license spectrum. And that has also been augmented using LTE in unlicensed. But in this case, this is done without listen before talk. So, there is a lot of controversy in LTE unlicensed, because this interesting mechanism that is used in Wi-Fi networks is not being adopted by LTE-U. And therefore, the coexistence between LTE-U and Wi-Fi may be compromised. Lastly, we see MuLTEFire. MuLTEFire is different to LAA and LTE-U. In this case, we don't need license spectrum at all. In this case, the device is utilizing LTE technology, only in unlicensed spectrum. And there is no need to use LTE in licensed. MuLTEFire has been built in with these-- with the listen before talk mechanism to ensure there is good work systems with other Wi-Fi networks. I also want to mention another Wi-Fi technology that wasn't mentioned earlier. The reason I didn't group it with the other technologies is that this is using very different type of spectrum. Where before we were talking about 2.4 and 5 gigahertz, in the case of 11ad, also known as WiGig, the spectrum uses 60 gigahertz, so much higher spectrum. 60 gigahertz still are unlicensed band, unlicensed pretty much worldwide. And with this, because it's much higher spectrum, it's much wider availability of that spectrum, it comes with very, very high peak data rate and peak throughput. This technology also as Wi-Fi takes advantage of the Wi-Fi alliance and all the certification mechanisms, and the ecosystem enablement that comes with the Wi-Fi alliance. So this is also a very popular technology that is being used today, in many cases to replace cable, NOTICES AND DISCLAIMERS", "label": [[102, 127, "Radio Access Technology"], [3676, 3680, "LTE"], [3847, 3850, "LTE"], [3987, 3990, "LTE"], [4068, 4071, "LTE"], [4079, 4082, "LTE"], [4474, 4477, "LTE"], [4510, 4513, "LTE"], [4718, 4722, "LTE"], [4798, 4801, "LTE"], [4923, 4926, "LTE"], [5423, 5426, "LTE"], [5601, 5604, "LTE"], [5931, 5935, "LTE"], [6078, 6082, "LTE"], [6153, 6156, "LTE"], [6224, 6228, "LTE"], [6340, 6343, "LTE"], [6686, 6689, "LTE"], [6759, 6762, "LTE"]]}
{"id": 40, "data": " Hi. In the next couple of sections, we'll be covering Edge Computing for communications service providers beyond basic definitions. This will include discussion about locations from which hardware and software platforms will be providing appropriate use cases in business models, delivering these types of Edge services. So, Edge Computing is the model of placement of compute, storage, and networking functionalities, deeper into the carrier network to be closer to the end customers or devices communicating. And this way we get too much better latencies and higher bandwidths. Also, based on that, we can go and deliver better services capabilities, optimize the TCO, and comply with various regulatory requirements, including data locality. So this way, the Edge allows placing these types of computes in the network, so that the devices themselves are closer to other devices and to the other networks related to their services, without crossing all the care and network, all the hierarchies going up and down, just to reach the next device, maybe connect it on to the same aggregation point or something like that. So, this is the foundation and the requirement to implement what was previously introduced as telco cloud, so that we get end to end metric transformed in a way that we are allowing this type of compute platforms in different hierarchies inside the network, so this then takes advantage of what was being done for the cloud backends for the core network, Edge in particular here, then Access, and also in some cases on premise for the let's say enterprise ID. And this type of transformation, starting from virtualization, software defining everything, allows the automation, making it cloud ready, for this distributed telco cloud concept, allows us to prepare for incremental services compared to what it is today. But then eventually when there is 5G standards agreed, this allows for incremental use cases, incremental revenues, very differentiating for the Comms vertical. A little bit-- another type of representation on where this Edge exactly is. Saw this was the idea of introducing virtualization and cloud principles into the regional data centers, central offices, aggregation points, placing these types of compute platforms in the distributed locations. Critical is that all of that is, if possible, gets exposed as one platform, so that workload placement then correctly chooses where to put appropriate instances. When we talk about latencies, here is a representation that is based on conservative calculations and goes and covers different network hierarchies. So the vacant clouds where over-the-top providers can provide their services is hundred-plus milliseconds. And then depending on what is the latency requirement for appropriate use cases, we will have to add the appropriate locations in the network, define those platforms, and run it from there. Where exactly these locations will be depends on number of questions to be answered. It starts simply with what is the geographical coverage that needs to be taken into account, what is the population distribution in this geography, and what is the situation the transport network, how well are these locations ready to receive IT equipments regarding power and heating situation there, also security requirements in those locations. So it's a number of things to be considered inside this location type of category of the definitions. And also in some of the regulatory systems in different countries, it will be required that data stays within given boundaries of geography, also dealing with big enterprises, they will have their own security departments, having their own requirements, so all these types of things can be taken into consideration, and in this way one decides NOTICES AND DISCLAIMERS[O1] [O1]", "label": [[74, 106, "Cloud Service Providers"], [1873, 1875, "5G"], [1216, 1227, "Telco Cloud"], [1742, 1753, "Telco Cloud"], [55, 69, "Network Edge"], [325, 340, "Network Edge"], [763, 767, "Network Edge"], [1477, 1481, "Network Edge"], [2060, 2064, "Network Edge"]]}
{"id": 41, "data": " After covering locations in platforms, use case is being run from there. We can categorize in the following. So here is the long list that I'm not going to go through, but rather summarize it. There will be those that are focusing on consumer, customers. There will be those that are extension of the usual comm services, now like distributed NFV running out of edge locations. And then they will have typically very incremental services for enterprise and government customers. All that can also be done on platform foundation, exposing some of the platform capabilities instead of building all these as a separate vertically integrated applications. So for example, a lot of analytics capabilities would be nice to expose auto platform and then consume for different vertically focused use cases. Here is a list of those that we were involved in, working directly with communications service providers, experimenting with that, and trying to see what is the right, the best optimized platform in hardware and software, what ecosystem can actually monetize these types of services. So a couple of examples in more details. So for consumer, there is a lot of focus on gaining virtual reality, augmented reality usages. And here are examples with different latencies as major value provided to gamers, for example also being improved when we get to the 5G connectivity in number of milliseconds for Edge gaming, for virtual reality, augmented reality. E-sports is being focused for a number of communications service providers, sponsoring some of the teams, also for branding reasons. So there is quite a lot of focus on these type of usages within the consumer category of use cases. Targeting different verticals come appropriate use cases, so number of communications service providers already for years sponsoring various sports facilities and venues. Here is-- here are a couple of examples how to make them smarter for revenue maximization, for operational efficiency or simply for sport fan better experience. It can start with basic finding the ways and can go up to instant replay of multiple angle cameras, images and all sorts of different things other related use cases. In this case, needs to be normally built, special connectivity environments, and run from the local data centers because of the massive usage of data that is generated and consumed over there. An example of that, which is going on as a proof of concept of Alibaba is a combination of facial recognition, video analytics, surveillance, and the usage of 5G technologies, where we have, for example, for access control, the facial recognition and controlling the badges. And then from the other side, we have a video analytics for various surveillance reasons. And all this is happening in one virtualized server as a network in a box running on the Edge next to the facility, where this is happening. More comprehensive sometimes and more integration requiring use cases are in industrial control. So these are the use cases where in remote manufacturing sites or construction sites, like examples here, using license spectrum, the communications service providers would provide radio access networks locally. And for example, here, it's appropriate throughput form uplink and down link. And latencies are an example of how this type of heavy machinery can be controlled. And this way, it allows, for example, in mining industry an additional, I mean, in other similar cases, very cost effective remote control of these types of machinery. Even further, more complex use cases with very focused industry alliances between the automotive and communication service providers, we have different use cases for automated driving, for controlling how different cars are driving together in some kind of sequence, smarter way of controlling the traffic, downloading all the different over the air software and high definition maps, helping with finding the simple parking spaces, uploading all the collected sensor data that was accumulated during the drives. And this is rather many different combinations of technologies and use cases in rather complex use cases that's combined the communication service provider, the automotive, sometimes the cities that are controlling all this type of traffic and a set specific standardization efforts are happening in the right communities around that. To implement these types of wide variety of use cases, there is a lot of ecosystem partnerships required. So this will include specific companies that are serving the enterprise and government verticals, so software vendors and system integrators take advantage of value of communication service provider Edge locations, but then integrating it for appropriate verticals, pure play system integrators that know about coms and those verticals can really add incremental value big time here. And then the service providers that are controlling, and software defining all those networking and coms functionalities. We have traditional telecommunication equipment manufacturers that can extend NFE distributed locations and really providing those virtualized network functions there. And then we have other, let's say, hardware vendors that can use those building blocks in hardware and the low level software and combine them in the right platforms that their cost optimized for the different usages on premise, on the Edge. And then this is being standardized because there is a lot of interoperability required. And here is some examples of places where it is being standardized, so ETSI multi-axis Edge computing, but then all the other communities they have because of huge importance and focus of the Edge location and services, they have their own within their domain of expertise, focus on what they can contribute towards the Edge cases. So there is on that page, there is OPNFV Edge, open stack Edge, NOTICES AND DISCLAIMERS[O1] [O1]", "label": [[2535, 2537, "5G"], [1353, 1355, "5G"], [4643, 4673, "Communications Service Providers"], [3113, 3145, "Communications Service Providers"], [3160, 3181, "Radio Access Network"], [3622, 3653, "Communications Service Providers"], [4159, 4189, "Communications Service Providers"], [344, 347, "NFV"], [1756, 1788, "Communications Service Providers"], [872, 904, "Communications Service Providers"], [1494, 1526, "Communications Service Providers"], [2830, 2834, "Network Edge"], [4872, 4889, "Communications Service Providers"], [5385, 5389, "Network Edge"], [5567, 5571, "Network Edge"], [5672, 5676, "Network Edge"], [5800, 5804, "Network Edge"]]}
{"id": 42, "data": " In this section, we will be covering what are the hardware and software platforms that need to be put in place in the appropriate locations to run these type of use cases, and what is the architecture, and how does this infrastructure look like? So, from Intel we prepared and invested into a number of different areas, starting from the foundation. At the bottom of this slide, you can see all the different ingredients that we have, which includes appropriate processors. So, Xeon for powering usual volume dual-socket servers, or we also have SoC version of that to power, for example, the world of CPE. Then we have it also based on Core processors and Atom processors, if it needs to be more power- and cost-optimized platforms with the lower capabilities compared to what a fully featured Xeon can do. We also have Ethernet controllers on appropriate adapters for scalable NICs. Then we have, based on Altera acquisition, we have FPGA, typically coming on PCI-E adapters to accelerate compute, networking, and storage algorithms. 3D XPoint is technology based on which we are building data center SSDs, like Intel Optane SSD for data centers. And now also, the persistent memory for similar usage in the storage hierarchy that's now between the DRAM-- we will have the persistent memory, then comes the faster SSDs, then come the slower SSDs. And then further, we have fixed function accelerators like QuickAssist that accelerate a number of security and compression algorithms. And then we have other assets, also coming from the hardware ingredients and investment in the platform. So, based on that, for different ways of consuming those ingredients, we built special software development kits that are then optimized to understand and take advantage of all the hardware underneath. And those are being put together into the right reference architectures to make it easier for the ecosystem to build the use cases and to extract value out of all that. So, here you have lists of investments happening in the access area, where, for example, we have FlexRAN reference software, or we have the virtual CCAP reference software. For the edge, we have MEC libraries, Network Edge Virtualization SDK for developers that allows for easier building of comms related use cases, but it can also be then targeting appropriate other verticals, and simply make development efforts easier by using those libraries that are optimized for the underlying hardware. So, there is other capabilities in terms of other libraries, like for different encryption ciphers, Visual Clouds, and then we are building this into different best-known configurations, which is recommended and well-tested certified hardware configurations. And here, we have example of combinations of Intel architecture so those underlying ingredients with appropriate host operating system with hypervisor and sometimes other software libraries related to that, which allows for easier adoption and consumption of these types of platforms. An example use case that is being worked and also heavily invested is network slicing, which will allow providing different networking capabilities to different use cases. Depending on some needs, like super reliable networking on low latency and some will be perfectly fine to run use cases on higher latencies, and maybe the networking doesn't have to be super reliable. But then from the same physical network, we can slice it and appropriately, out of same network, expose different capabilities for different customer usages. An example of how these types of ingredients and platforms can be applied to Wireless Edge Computing-- so we have specific reference architecture that is based on subsets of those ingredients in hardware. So for example, Xeon and Core processors, Optane Memory, and the Movidius technology coming in platforms with our server OEMs and ODMs that have the Multi-access Edge Compute libraries on top of it Have the other SDKs like network edge virtualization SDK defined in proper hardware configurations to make it easier to build networking functionalities out of that. Then, all the interfaces are exposed to all the orchestration layers. In this type of reference architectures, then, we are working on the defining, documenting, and working on adoption of the ecosystem. Some terminology related to that-- so, Edge Cloud/MEC covers implementations on-premise and off-premise, various types of connectivity, focuses on solving End User and, in particular, Enterprise / Vertical challenges combined with Communications Service Provider value of the edge locations, and can include other comms workloads as appropriate. And there is a big standardization effort within ETSI called Multi-Access Edge Computing. Initially, it was focusing on mobile use cases. Later, all other connectivity was added, along with compute and storage that is required for that. NOTICES AND DISCLAIMERS", "label": [[2172, 2184, "Network Edge"], [3610, 3618, "Wireless"], [4537, 4568, "Communications Service Providers"], [2059, 2066, "FlexRAN"], [3619, 3623, "Network Edge"], [4345, 4349, "Network Edge"], [4582, 4586, "Network Edge"]]}
{"id": 43, "data": " So in this section, I'm going to start with the basics and explain what an EPC or an Evolved Packet Core is and the role that it plays in the network. And then, we'll talk about the motivations for virtualizing the EPC. So this is a quote from the introduction to a report by Deloitte from a couple of years ago. And it really just reminds us that the telecom sector continues to be a key enabler for many types of industries and for society as a whole. Now, there are many new trends emerging over recent years-- the number of new IoT devices coming onto the network. The explosive growth in video in media streaming, over both fixed and wireless networks. And these have really driven the need to look to techniques, such as virtualization, to change the way that networks are built and to build those networks in a more flexible, scalable, and efficient way. Here's another quote that reminds us that one way to think about the global telecoms network is as the world's biggest machine. It's a complex web of networks that interoperate and work together to make data services available to our smartphones. And at the heart of this network and particularly, at the heart of the wireless network, is the Evolved Packet Core, or EPC, that connects end users to the internet. And we'll come on and describe that in much more detail in the coming slides. So whilst fixed access networks have been with us for quite significantly, over 100 years, the wireless networks have really been evolving very quickly over the past three or four decades. And the major steps in the evolution of these wireless networks have come to be known as generations. So we have 1G, 2G, 3G. Today's 4G and coming, 5G-- which we'll talk about some of the motivations for 5G. So back in the 1980s, the first generation, or 1G, networks-- which were analog networks-- were being deployed. These networks really only supported voice. Supported a limited number of users. That takes us forward into the early 1990s, when the first 2G network started the be deployed. One of the major differences between 1G and 2G networks is that 2G networks are based on digital technology. This allowed for much more efficient use of the available radio spectrum, the frequency spectrum. And allowed for a much larger number of users to be supported on these networks. 2G networks also started to introduce the first data services to wireless networks-- most famously, the SMS text service. So if we move forward approximately a decade into the early 2000s, the first 3G networks were starting to be deployed. And were really focused on improving the bandwidth available for data services and enabling a wider adoption of services, such as video calling and limited internet access. And finally on this picture, in the early 2010s, LTE or 4G networks started to be deployed. LTE stands for long-term evolution-- by the way. And with these networks, the focus was really on achieving a broadband like internet access, a data service to the end users. And of course, the rise of these networks really coincided with the massive growth of smartphone market. So this rather complicated picture just tries to give a high level view of the telecom network end-to-end. And on the left hand side of the picture, we can see the various types of access technologies that are used for users to access services. These could be fixed line access, cable, DSL, broadband access to both consumers in their homes and to enterprises. And really, the focus of our discussion today is the radio access network-- the wireless access which we see in the top left. The red circle shows the extended scope of a 5G network versus the yellow circle, which really shows where the 4G core network-- the EPC we're discussing today-- plays today. One of the reasons that the core network for a 5G network will have increased scope and will reach out more to the edges of the network, closer to the users is some of the capabilities that will be expected from 5G networks that drive low latency, very high bandwidth access to the network. So we've already started to talk a little bit about 5G networks, which are in development, and let's think about when those networks will be coming into production. This forecast from a report by Ericsson, gives a view of when that transition will happen. And I think really, the summary is that we will start to see very early adoption of 5G networks this year, in 2018. While we'll see more mainstream adoption from 2020 onwards. So what's driving the demand for a new network? What's driving the demand for 5G? One of the reasons is to enable ultra-reliable wireless connections to enable a new class of services, which really were not able to rely on the wireless network until this point. Remote medical treatment might be such an example. To enable extreme real-time communications-- i.e. communications with very fast response, very low latency. An example of this may be where you want to utilize a network, such as this, to enable robotic control in an industrial or a manufacturing environment, where the feedback from issuing instruction to response needs to be extremely short. Of course, we continue to see the demand for broadband access high data rates everywhere. So the 5G networks will deliver high bandwidth to users. And also, we see lots of new types of devices, IoT-- internet of things-- devices coming onto the network. This is a challenge to existing networks both because of the sheer number of these devices and that they have very different requirements on the network, very different characteristics. So together, all of these things and more are driving the demand for 5G. And let's come back now, to looking at the LTE network, the 4G network. Here, we have a simplified picture-- a very high-level view of where the packet core sits in the network and what its role is. So to the left of this picture, we show a UE-- a user equipment-- i.e. a smartphone would be a perfect example of a UE. Which connects to a radio access network, which goes by-- in the standards for 4G-- goes by the name of an E-UTRAN an evolved universal terrestrial RAN. The role of the EPC-- the role of the Evolved Packet Core-- is to connect that radio access network to the internet in its simplest form. And we'll come on and talk a little in a bit more detail about the five major elements that go to make up an EPC in the next section. NOTICES AND DISCLAIMERS", "label": [[76, 79, "EPC"], [86, 105, "EPC"], [216, 219, "EPC"], [533, 536, "IoT"], [640, 648, "Wireless"], [1181, 1189, "Wireless"], [1206, 1225, "EPC"], [1230, 1233, "EPC"], [199, 212, "Virtualization"], [728, 742, "Virtualization"], [1449, 1457, "Wireless"], [1589, 1597, "Wireless"], [1691, 1693, "5G"], [1746, 1749, "5G"], [2392, 2400, "Wireless"], [2790, 2793, "LTE"], [2833, 2836, "LTE"], [3554, 3562, "Wireless"], [3645, 3647, "5G"], [3733, 3736, "EPC"], [3822, 3824, "5G"], [3987, 3989, "5G"], [4118, 4120, "5G"], [4406, 4408, "5G"], [4576, 4578, "5G"], [4627, 4635, "Wireless"], [4725, 4733, "Wireless"], [5253, 5255, "5G"], [5349, 5353, "IoT"], [5356, 5374, "IoT"], [5665, 5667, "5G"], [5712, 5715, "LTE"], [6157, 6160, "EPC"], [6179, 6198, "EPC"], [6008, 6028, "Radio Access Network"], [6220, 6240, "Radio Access Network"], [6388, 6391, "EPC"]]}
{"id": 44, "data": " In the last section of Edge Computing for Communication Service Providers, we covered how industry leaders are partnering with the right ecosystem in building this type of use cases and services. So, to provide services in different categories, appropriate ecosystem vendors need to be included from platform, comms, consumer, and enterprise / government. And there will be also required flexibility from communications service provider sites and business models with these type of partners because serving some smaller customers fully integrated service without customizations can provide it, but to deal with big enterprises, there will be a lot of customization and integration, typically from the service providers that are already active in this type of verticals that have to take the value of the CommSP edge and complement it with what particular use cases are-- that, let's say, manufacturing vertical would need. So, this can be done by Communication Service Providers, either by-- for example, acquiring companies that are very active in the space already, and we have cases of that. So, some are acquiring expertise in health care, and then they have the particular Comm SP health business, or they have to create strategic alliances with this type of, let's say, health care providers or someone serving like manufacturing vertical. So, to help in building this type of partnership, and to help with having a pool of vendors, like software vendors and platform vendors, that can be chosen from, from Intel's site. With the Network Builders program, we have very specific program focused on the Edge ecosystem that will allow CommSPs to come faster to these types of services. It already includes input from the communications service providers and those Enterprise verticals. Then we have Application Developers-- very specific for different use cases. Systems Integrators are providing the expertise from the verticals. And from the Comms Service Provider, networking. And then they also have appropriate Hardware Platform Vendors and from modular systems to more integrated ones.", "label": [[43, 74, "Communications Service Providers"], [702, 719, "Communications Service Providers"], [406, 437, "Communications Service Providers"], [948, 979, "Communications Service Providers"], [1608, 1612, "Network Edge"], [1725, 1757, "Communications Service Providers"]]}
{"id": 45, "data": " So let's come on and talk about the role of NFV in the EPC. Network function virtualization, or NFV, is a trend that the telecom industry has been on for a number of years now, where we use virtualization technology to separate the dependencies of the software from the underlying hardware. In the case of EPC, we can deploy the EPC functions-- that we previously described as virtual machines-- on top of a virtualization software layer, sitting on top of standard high-volume compute resources-- standard servers, standard NIC cards, standard storage components. This also offers the ability to support service function chaining to providing connectivity between those virtual machines. And really, this is an underlying technology that enables service providers to deploy their EPC networks in a more flexible, and dynamic, and cost efficient way. So, to summarize this section, 4G networks are now ubiquitous at this point and the technology is mature. Virtualization of the 4G Evolved Packet Core, or EPC, has been gaining traction now for several years. And virtual EPCs are now offered by pretty much, all vendors. And all deployments to date have been on Intel architecture. And Intel has been investing in this space with our vendors and our customers for a number of years to ensure that those offerings are optimized on top of our technology. Most markets are now in late deployment phases and are adding to their EPCs incrementally as required. Data new users keep coming onto the network, data rates keep going up so service providers need to continue to invest in their EPC capacity. And many are choosing virtualized EPC instances as the way to do that. The major vendors in this market continue to be the likes of Huawei, Erickson, and Nokia. Although, one of the promises of network function virtualization was to enable new entrants to come into the market and we've certainly seen that in the case of virtual EPC, with the likes of a firm to many others providing software first, software only offerings to the market. And we expect to see the next major spending curve in 2020 plus, as service providers invest in new core network capacity to support their rollout of 5G networks. NOTICES AND DISCLAIMERS", "label": [[45, 48, "NFV"], [56, 59, "EPC"], [61, 92, "NFV"], [97, 100, "NFV"], [307, 310, "EPC"], [330, 333, "EPC"], [782, 785, "EPC"], [958, 972, "Virtualization"], [191, 205, "Virtualization"], [409, 423, "Virtualization"], [983, 1002, "EPC"], [1007, 1010, "EPC"], [1073, 1077, "EPC"], [1426, 1430, "EPC"], [1585, 1588, "EPC"], [1633, 1636, "EPC"], [1793, 1824, "NFV"], [1929, 1932, "EPC"], [2189, 2191, "5G"]]}
{"id": 46, "data": " So now we'll come on and define some of the major elements of an EPC in a little bit more detail. As you can see from this picture, there are really, five major elements that go to make up the EPC. The first of which-- highlighted here-- is the PDN or the P-GW the PDN GW. The function of this element of the EPC to provide access to external networks-- i.e. to provide access typically to the internet. A user equipment or user smartphone may have simultaneous connectivity with more than one P-GW for accessing multiple services. And the P-GW also performs some policy enforcement, packet filtering for each user, and in conjunction with the PCRF-- which we'll come through in a moment-- supports functions like charging and packet screening. The next major element of the EPC is called the MME or the mobility management entity. And it's really the key control node for the LTE access network. It's responsible for keeping track of the users' devices whilst they're in idle mode. It's involved in the activation and deactivation of connections. And it's also responsible for choosing which S-GW a user is connected to. It also plays a role in handover so when you move from one cell site to another as a move with your smartphone. The next major element we'll take a quick look at is called the HSS or the Home Subscriber Server. This is really a central database that contains user-related and subscription-related information. It's the repository for users information in the network. And the other functions call on this database to determine who a user is and the policy that should be applied. The S-GW or serving gateway routes and forwards user data packets. And it also acts as the mobility anchor-- i.e. as a user moves from one cell site to another, they stay connected to the S-GW. So it acts as an anchor during handovers from one cell site to another. And they can also play a role in if you are handing over from an LTE network to another network say, a 3G network. It also performs a role in replication of some user data in the case of lawful intercept scenarios. So finally, we have the PCRF or the Policy and Charging Rules Function. And this really is the entity that determines the policy rules of the network. It sets quality of service rules for subscribers. So depending on the contract that you have with your service provider, you may be entitled to different quality of service guarantees. And it acts as a controlling bridge between the LTE network and an external public network which we're not showing in this picture, called the IMS or IP multimedia subsystem, where if you're routing voice calls out from the LTE network to another network. So the PCRF plays a role in this function, as well. So now, let's take a pause and look at some of the drivers for virtualizing the EPC. Really, virtual EPC emerged as one of the very first network function virtualization or NFV use cases. And it's remained a very compelling use case for NFV and many operators are in the process or have already deployed virtual EPC or in the process of doing so. And one thing to note is that while virtualizing EPC is an important step for many operators for their 4G networks, it's also become a key technology step-- a key enabler-- paving the way to some new IoT and 5G services. So some of the benefits that we can achieve from virtualizing the EPC include some of the-- if you like-- standard virtualization benefits, where moving from purpose built, fixed function hardware to more agile, software-based network services service providers can dramatically and dynamically, rather, scale their mobile networks. And they're able to achieve cost savings, both in their operational costs and their capital expenditure costs. And this has really become critical for service providers in recent years as they've had to cope with very rapid growth in data that they have to handle for their networks. And the ability to support many more devices, whilst the revenues that they can expect to achieve from their subscriber base has largely stayed static. So, another feature is the ability to dynamically allocate capacity to meet peak usage requirements, but then, to scale back that capacity during quieter periods of the day. That's another very important benefit of virtualization. It enables service providers to more efficiently use their resources. And it enables them to save power, as well-- which is one of the major drivers of their operational costs. Lastly, virtualization offers service providers a path to improve the time it takes them to deploy new services. In the past, in order to deploy a new service, a service provider would typically have to install a custom appliance. That process of procuring, testing, installing an appliance could take weeks, months and so it really limited their ability to react with speed to their desire to introduce a new service. Virtualization offers service providers the ability to do this by deploying software onto existing NFV infrastructure and really offers the opportunity to dramatically drive down the amount of time it takes to create a new service. It enables them to try new services more, to be more innovative, to try things that may not work. It enables that degree of flexibility. And while telecom service providers are facing increasing amounts of competitive pressure from some of the over the top service providers that are riding over the top of their networks, this has become a really key driver for virtualization.", "label": [[66, 69, "EPC"], [194, 197, "EPC"], [310, 313, "EPC"], [878, 881, "LTE"], [776, 779, "EPC"], [2468, 2471, "LTE"], [2644, 2647, "LTE"], [2829, 2832, "EPC"], [2901, 2904, "NFV"], [2883, 2897, "Virtualization"], [2965, 2968, "NFV"], [3040, 3043, "EPC"], [3124, 3127, "EPC"], [3275, 3278, "IoT"], [3283, 3285, "5G"], [3345, 3357, "Virtualization"], [3362, 3365, "EPC"], [3111, 3123, "Virtualization"], [4892, 4906, "Virtualization"], [4991, 4994, "NFV"], [5487, 5501, "Virtualization"], [4280, 4294, "Virtualization"], [3411, 3425, "Virtualization"], [2791, 2803, "Virtualization"], [2808, 2811, "EPC"], [1934, 1937, "LTE"], [4481, 4495, "Virtualization"]]}
{"id": 47, "data": " Let's have a look at the various enablement strategies that Intel is putting in place to help these nascent virtual RAN ecosystem. Let's start with the software part first. In FlexRAN, Intel has been focusing on the development of the layer one-- often referred to as the basement. This part of the radio access network is one of the most challenging to implement in software, and Intel has really squeezed all the performance possible out of our general purpose computing platforms. Our partner, Radisys, has been focusing on delivering layer 2 and layer 3. Together, the assets that both Radisys and Intel bring to the table are a great starting point for those in the ecosystem trying to develop their own virtual RAN solutions. FlexRAN is not a virtual RAN product, but the assets that both Radisys and Intel bring to the table and a great starting point for those in the ecosystem that are trying to develop their own virtual RAN solutions. Intel will license the layer 1, and that ecosystem can directly liaise with Radisys to get access to that layer 2 and layer 3 if they wish to do so. Because of the software architecture that Intel has adopted in the development of FlexRAN, it is possible for the ecosystem to create virtual RAN solutions based on FlexRAN that meet different types of deployment requirements. With FlexRAN, the ecosystem can implement fully virtualized solutions, whether they are completely centralized, partially distributed, or even integrated and fully distributed, as well as partially virtualized ones. It is the choice of the solution vendor utilizing FlexRAN how to use it depending on the type of segment and application that their solution is targeting. These are just about a few example implementations that show the flexibility and scalability of FlexRAN. Typically, the two fundamental factors that will determine the type of virtual RAN solutions for a particular deployment are: was the connectivity to the site? Which is, where the antennas are located? And second, where can I deploy my general purpose computing platforms? Depending on the answers to these two questions, there will be virtual RAN solutions that will fit better one deployment than others. FlexRAN-- because it's a full software implementation-- can be easily adapted, and Intel has different types of hardware platforms that will fit different types of network locations all the way from remote central offices up to the coordinate work if needed.", "label": [[109, 120, "vRAN"], [177, 184, "FlexRAN"], [300, 320, "Radio Access Network"], [710, 721, "vRAN"], [733, 740, "FlexRAN"], [750, 761, "vRAN"], [924, 935, "vRAN"], [1178, 1185, "FlexRAN"], [1230, 1241, "vRAN"], [1261, 1268, "FlexRAN"], [1328, 1335, "FlexRAN"], [1521, 1532, "Virtualization"], [1589, 1596, "FlexRAN"], [1790, 1797, "FlexRAN"], [1870, 1881, "vRAN"], [2135, 2146, "vRAN"], [2206, 2213, "FlexRAN"]]}
{"id": 48, "data": " The majority of service providers have embarked in their network transformation journey. Typically that journey starts with a core network, but as the virtualization of those network functions is maturing, now, there is a trend among service providers to start thinking about how the access functions are going to be virtualized. From Intel, we are looking at network transformation from an end-to-end perspective. We have definitely spent a lot of time trying to enable general purpose computing platforms to be able to implement challenging core network functions. And now, there is a lot of focus in also enabling the radio access network to be virtualized. We have strategies that is looking both at their hardware elements in the platform and also the software enablement that is going to be helpful to the ecosystem as they want to create solutions for virtual RAN, Before we start delving into the specific enabling strategies that Intel is implementing to enable virtual RAN ecosystem, let's have a look at the reason why service providers are interested in virtualizing their radio access networks. I would start by saying that the same benefits that apply to virtualizing the core network will also apply to virtualizing the radio access network. Implementation of virtual network functions software elements using general purpose computing platforms will help grow an ecosystem and will help faster innovation cycles as well as improve energy consumption targets. On the other hand, virtualizing the radio access network brings additional benefits. With a software implementation and with virtualization comes the possibility to centralize some of the radio access functions. With centralization, there is a possibility of coordination between different functions. Whereas before, the radio access was extremely distributed, and the possibilities of coordination were limited, with centralization, and it is possible to co-locate in the same platform multiple radio access functions that control multiple cells, therefore allowing for coordination between different cells. With coordination, there is an improvement of the efficiency in the use of radio resources, which is a very scarce and expensive resource. In addition to this benefit, using general purpose computing platforms to implement radio access also enables the co-location of more wireless computing and its cloud applications that will benefit from being co-located with the radio access network. We've seen the benefits that virtualizing the RAN brings to service providers and why are they demanding virtual RAN solutions. This is creating the momentum in the industry over the last two or three years. And we've seen a lot of different types of industry initiatives happening to address some of the different aspects around virtual RAN. We've seen xRAN, vRAN projects in the Telecom infrastructure project. And we have even seen 3GPP trying to address some of the challenges of virtualizing the RAN and trying to create a common reference architecture around different options on how to virtualize the RAN. Let's now have a look at the different types of solutions that virtual RAN ecosystem is bringing to market. We are seeing different types of approaches. The partial virtualization is the more normal evolution from the existing traditional and distributed solutions. This partially virtualized solutions will generally only virtualize and centralize part of the RAN functionality. The other part will remain distributed and will remain implemented using custom hardware. The most disruptive solutions are fully virtualized. These types of solutions are fully implemented in software, and some of these solutions will be fully centralized, some of these solutions will be only partly centralized. But the common theme is that the whole RAN is implemented in software. And the third category are the integrated type of solutions. Those are still deployed in a very distributed manner, but limited general purpose computing platforms as part of the solution, which may implement the whole of the RAN functionality NOTICES AND DISCLAIMERS", "label": [[152, 166, "Virtualization"], [318, 329, "Virtualization"], [622, 642, "Radio Access Network"], [860, 871, "vRAN"], [971, 983, "vRAN"], [1067, 1079, "Virtualization"], [1495, 1507, "Virtualization"], [1512, 1532, "Radio Access Network"], [1601, 1615, "Virtualization"], [2358, 2367, "Wireless"], [2453, 2473, "Radio Access Network"], [2504, 2516, "Virtualization"], [2580, 2591, "vRAN"], [2805, 2816, "vRAN"], [2835, 2839, "vRAN"], [2959, 2971, "Virtualization"], [3068, 3078, "Virtualization"], [3151, 3162, "vRAN"], [3253, 3267, "Virtualization"], [3369, 3380, "Virtualization"], [1086, 1107, "Radio Access Network"], [1170, 1182, "Virtualization"], [1219, 1231, "Virtualization"], [1236, 1256, "Radio Access Network"], [3598, 3609, "Virtualization"], [3411, 3421, "Virtualization"], [1276, 1301, "VNFs"], [2521, 2524, "vRAN"], [649, 660, "Virtualization"], [3822, 3825, "Radio Access Network"], [4080, 4083, "Radio Access Network"], [3449, 3452, "Radio Access Network"], [1972, 1994, "Radio Access Network"], [3083, 3086, "Radio Access Network"]]}
{"id": 49, "data": " So let's start with an overview of UCPE, or universal CPE, and SD-WAN, which stands for software defined wide area networking. So universal CPE, or universal customer premise equipment, this area has been gaining a lot of attention recently. But what does it mean? So, as enterprise customers continue to adopt cloud services, comm service providers are able to replace multiple fixed function appliances, or CP equipment, with virtual CPEs, VCPs, or universal CPEs. The promise to improve the speed of service provisioning, and to reduce capital expenditure and operational expenditure costs. With a single virtual CPE platform, customers can support multiple functions, typically delivered as VNFs, or virtual network functions. Examples of these functions may include WAN routing, virtual private network, firewall, intrusion protection systems, session border controllers, and perhaps most importantly, software defined WAN, or SD-WAN. Many service providers see universal CPE as a way to gain the benefits of NFE and bring this to their enterprise customers. So, if SD-WAN is such an important use case for universal CPE, let's take a look at it and think about what it does. Software defined wide area networking, or SD-WAN is a specific application of SDN, or software defined networking technology applied to WAN connections. Which are-- and let's remember that WAN connections are what are used to connect enterprise networks, so branch offices, to data centers, typically, and typically over large geographical distances. So a WAN might be used, for example, to connect a branch office to a central corporate network, or to connect data centers separated by a large distance. In the past and today, these WAN connections often used technology that required special proprietary hardware. The SD-WAN movement seeks to move to a more cloud network controlled situation using a software approach. For enterprises, SD-WAN offers increased flexibility, simplifies their IT operations, and it can achieve more rapid and elastic deployments. And ultimately, for an enterprise, they're looking to reduce the cost of their leasing private service provider provided connections, typically NPLS connections. So all of these things can result in cost reduction, business agility, a faster time to enable connectivity to a branch, and reduced operational costs. For a Comm service provider, SD-WAN can address a disruptive threat. Many enterprises were already looking at SD-WAN as a way to reduce their dependency on NPLS service provider provided NPLS connections, and to use the internet for their connectivity. And comm service providers, by proactively offering SD-WAN services, can address that potential competitive threat. It also offers the ability to simplify operations for a comm service provider. Today, to deploy a routing, firewall, and WAN acceleration functions to an enterprise customer, would require three appliances to be shipped, or even worse, sent to an enterprise customer, followed by an engineer visit to configure and install those devices. So it offers comm service providers flexibility, speed, and an ability to meet their customers' requirements in those areas. So why is SD-WAN important in the first place? Well, one of the reasons has been a dramatic shift in traffic patterns from enterprises over recent years. So if we go back a little bit in time, the vast majority of traffic from an enterprise would have been client to server. So was served by their service provider provided NPLS circuits, and the majority of their traffic was going from their enterprise branches, to their corporate network, or to their corporate central data centers. With the emergence of so many cloud provided services, salesforce.com, Outlook 365 would be good examples, there has been a pretty dramatic shift in traffic patterns from enterprise sites, where their traffic is going to the internet, rather than to their corporate data centers. Now this is one of the spurs for the need to pretty dramatically change the way that WAN connectivity was offered to enterprises from service providers. It also offers the ability to centralize control, to use SDN techniques to centralize the control of policy. And it can improve, offer a way to quickly improve route selection and network response time. It offers the ability to provide a level of automation of service provisioning, the configuration of those services, and the overall orchestration of the end to end solution, by abstracting that into rules and policies which can be applied from a control points. And ultimately, it offers the ability NOTICES AND DISCLAIMERS", "label": [[36, 40, "uCPE"], [44, 58, "uCPE"], [64, 70, "SD-WAN"], [131, 144, "uCPE"], [149, 185, "uCPE"], [452, 466, "uCPE"], [933, 939, "SD-WAN"], [1072, 1078, "SD-WAN"], [1113, 1126, "uCPE"], [1224, 1230, "SD-WAN"], [1182, 1219, "SD-WAN"], [1268, 1306, "SDN"], [1260, 1263, "SDN"], [1802, 1808, "SD-WAN"], [1921, 1927, "SD-WAN"], [2388, 2394, "SD-WAN"], [2469, 2475, "SD-WAN"], [2664, 2670, "SD-WAN"], [3201, 3207, "SD-WAN"], [696, 701, "VNFs"], [908, 928, "SD-WAN"], [968, 981, "uCPE"], [89, 126, "SD-WAN"], [4168, 4171, "SDN"]]}
{"id": 50, "data": " Let's now look at the hardware centric aspects of the Intel enablement strategy for virtual RAN. General purpose computing platforms nowadays can be comprised of two different types of compute where one side will have the traditional CPU that runs software, but there are FPGAs as well that can be utilized to implement some functions that are better off loading such as the turbo encoder or decoder and some of the CPRI interface to the radio. Looking specifically at the CPUs-- Intel offers a wide range of solutions that addresses specific network locations. For those network locations where a traditional data center platform can be deployed, we have this here on scalable processor. This offers the best performance and the most scalable platform in the market. For those locations where full traditional data center solution is not viable, we offer two different types of products-- both are system on chip type of solutions. One is the CMD processor, which is a server or a chip essentially and the Intel Atom SOC for where there is less compute required. The latest generation of the Xeon scalable processors bring significant improvements over the last generation-- improvements that increase significantly the efficiency of virtual RAN functions. Amongst those improvements are the Intel AVX 512, which is a comprehensive extension to the existing vector instruction set. Another new capability is the improvement to the cost higher hierarchy. And latest but not greatest is the improvement in performance that comes with every new generation. With this, Xeon scalable processors can deliver more data throughput, more data storage, and most importantly, for virtual RAN functions, more efficient signal processing. Signal processing is a very important part of virtual RAN. Specifically, some aspects like being beamforming, modulation, and decoding. And because AVX 512 and because of the other improvements in the platform, these are greatly improved in the latest generation of CN scalable processors. But the Intel scalable processors are not a good fit for those more remote and network locations, such as central offices or cell sites. These type of network locations have very specific requirements in terms of physical space availability, in terms of power, in terms of cooling, et cetera that make traditional data center solutions not a good fit. For this type of network locations, Intel can offer the CMD system on a chip. This type of solution integrates compute, chipsets, networking-- all in a single SOC, which brings low power and very dense type of solutions that are a really good fit for those type of locations that are very remote in the network. Because the cores and the architecture are reused from the chip scalable processors, all the advancements, such as ABX 512 and the cache improvements and the performance improvements, are also available in the CMD platforms. For certain virtual RAN implementations, particularly for those with heavy centralization, and FPGA can be an ideal companion to the CPU. An FPGA is an ideal compute platform for some of the signal processing functions in the RAN. Since these functions are very compute intensive, offloading these functions from the CPU to the FPGA allows for greater efficiency, and more sets can be concurrently deployed in the platform. In addition to this, because an FPGA is programmable, it can also be used to accelerate other virtual network functions that may be deployed in the same hardware platform. In addition to being an ideal compute platform for some of the basement functions, an FPGA is also a good fit for the digital front end functionality that resides in the radial unit. Traditionally, this function has been implemented with A6, but FPGAs offer greater flexibility because of their programmability.", "label": [[85, 96, "vRAN"], [1236, 1247, "vRAN"], [1671, 1682, "vRAN"], [1774, 1785, "vRAN"], [2919, 2930, "vRAN"], [3133, 3136, "Radio Access Network"]]}
{"id": 51, "data": " So in this section, let's talk about the various deployment options for SD-WAN. There are at least three models that have emerged, and I'll seek to describe those here. Option one could be described as a hybrid model where SD-WAN is deployed on top of new hardware, but deployed behind existing network hardware on a customer premise. Now, this may be done to provide seamless overlay or, more likely, to accommodate some legacy access circuit requirements, like E1 or T1. Perhaps the more common and more flexible option is option 2 where SD-WAN is deployed on new hardware which replaces existing network hardware. And then there is a third option where SD-WAN could be deployed as a software upgrade on existing hardware, either routers or WAN accelerators. Certain vendors are planning to offer SD-WAN capability running on existing products. Option two, I feel, probably offers the most opportunity for innovation and cost-savings as it sees SD-WAN deployed on brand new hardware and offers the ability to not only deploy an SD-WAN solution on that hardware, but to migrate many other functions required for enterprise connectivity, firewalls, WAN acceleration, intrusion protection systems, as virtual VNF software running on the same platform. So here, we look at a high-level view of the end-to-end network and seek to point out the key network locations where universal CPE and SD-WAN can be deployed and disrupt the traditional approaches of providing enterprise connectivity. The three main deployment locations are, of course, at the customer premise itself, in the provider edge-- i.e. the service provider's central office or points of presence location-- or some elements of the solution can be or will be deployed in the service provider's centralized data center. So here I'm trying to explain several options for universal or virtual CPE deployment scenarios for different types of enterprise locations, different types of requirements on the enterprise side. It really demonstrates the flexibility of options that can be enabled by using Intel architecture platforms, because using Intel architecture platforms, from the smallest to the largest case, provides a consistent environment so applications can be seamlessly ported from one location, one deployment scenario, to another. So starting at the left, at a very small site or, perhaps, a SOHO office, power, space, costs will all be very constrained, and so a very small compute platform is required. One of the options that Intel can provide in this scenario is from our Atom family of products. And we see many of our partners offering solutions based on the Atom C3000 family of CPUs to address this small enterprise or SOHO use case. In this instance, the number of users at that site, and therefore, the bandwidth requirements will be low. And on a small platform such as this, you may expect to be hosting one or two VNFs at the customer premise. That could, for example, be SD-WAN, typically, and perhaps, a VPN, a VNF. While you may be hosting a broader set of services in a centralized location, so routing, firewall, carrier-grade network address translation, these things may, if required, be served from a centralized cloud location in the service provider's network. At a very large enterprise branch location, a more powerful CPE platform, virtualized CPE platform, can be deployed. And for example, that would likely to be deployed on an Intel Xeon D-2100 CPU platform or, potentially, a server based on Intel Scalable-- Xeon Scalable CPUs. These platforms offer far more computing power and can host a broad range of connectivity VNFs concurrently, maybe up to 6 or more, and can handle the bandwidth requirements of a large enterprise site. In this scenario, many of those VNFs will be hosted on the customer premise, and perhaps only the control portion, the control plane portion of the SD-WAN solution and the WAN acceleration solution would be hosted centrally in the service provider's network. So there are many Intel technologies, such as QuickAssist, accelerators for security and storage algorithms, DPDK, Intel Run Sure technologies, that can support the high performance and efficient running of VNFs on these platforms. And one of the beauties of the range of CPU offerings that Intel has for this solution is that these technologies can be run in the very smallest scenarios right up to the very largest, again, enabling portability NOTICES AND DISCLAIMERS", "label": [[73, 79, "SD-WAN"], [224, 230, "SD-WAN"], [541, 547, "SD-WAN"], [657, 663, "SD-WAN"], [800, 806, "SD-WAN"], [948, 954, "SD-WAN"], [1031, 1037, "SD-WAN"], [1209, 1212, "VNFs"], [1388, 1394, "SD-WAN"], [2898, 2902, "VNFs"], [2956, 2962, "SD-WAN"], [2997, 3000, "VNFs"], [1370, 1383, "uCPE"], [3621, 3625, "VNFs"], [3765, 3769, "VNFs"], [3881, 3887, "SD-WAN"], [4199, 4203, "VNFs"]]}
{"id": 52, "data": " So, in summary, service assurance capabilities are and will remain essential for realizing the full benefits of SDN and NFV. [O2] The industry must provide SDN and NFV solutions that fit and interoperate with existing network management systems.[O3] Intel Xeon and Intel Scalable processor platforms provide an incredibly rich source of telemetry. And Intel has worked with the industry to provide simple and unified interfaces to access that telemetry to enable NFV platforms, workloads, and services to be better managed. So, I would invite you to engage with us-- engage with Intel on enabling these features to unlock more value and new use models, along with the integration of MANO and Service Assurance frameworks.[O4] I think I would suggest that a great place to start would be the Intel Network Builders program, where many of our ecosystem partners promote and highlight their solutions. Thank you. NOTICES AND DISCLAIMERS [O2]Please delete. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[113, 116, "SDN"], [121, 124, "NFV"], [157, 160, "SDN"], [165, 168, "NFV"], [464, 467, "NFV"]]}
{"id": 53, "data": " Today, we're going to be talking[O2] about software-defined networking, software-defined infrastructure, and network function virtualization.[O3] To begin with, software-defined networking, or SDN, is an architectural approach, where what we're doing is separating out the application layer, which is where your business application logic resides, from the control layer, which is where a lot of your network services are configured, and also, from the infrastructure layer. layer.[O4] And we're doing this in a way that things are centrally managed. It can be programmed through software, obviously. It's not fixed-function logic. And the interfaces that we're defining are all open and vendor neutral. So that's software-defined networking in a nutshell. SDN uses open flow as one of the first instantiations of an SDN protocol. So, again, we're taking this OpenFlow protocol. It's enabling innovation to occur above the infrastructure layer. And this allows us to simplify the provisioning to optimize performance, to have the business applications provide policies down in an open way that is not tied to any one ven'or's implementation. Where the hardware and software are decoupled in this kind of architecture, control plane and the forwarding are also decoupled and the physical and logical configuration is similarly decoupled. We talk about OpenFlow as one of the primary examples of this. There are other examples of protocols that can be used. So, for example, P4 might be one. EBPF, and there's a number of others. But this OpenFlow is one of the first and one of the most widely used. Software-defined infrastructure is taking similar concepts, but broadening it to do a lot more than just networking, but also, the storage and the compute. And what we're trying to do is achieve better efficiencies by having things defined through a software approach rather than fixed function hardware. This allows us to pool resources across a broad range of infrastructures and a broad base of physical hardware. So, if we look at some of the potential use cases of this, CRM, customer relationship management, online sales, et cetera, we're able to assign those the right infrastructure attributes that's needed for the business application. We're doing all this with an intelligent resource orchestration, so that as the needs of the business change, might be changing on a very dynamic basis or changing over longer periods of time, we're able to change things without swapping out the hardware pieces. Now, we're going to be talking about network function virtualization.[O6] So why are we doing this? What is network function virtualization? Well, if we start on the left side of this picture, what we can see is we're coming from a world where everything was based around physical appliances. In a physical appliance, you had the application and the operating system were tied to one hardware instance. and it was inflexible and hard to upgrade individual components as different things changed. And you had physical appliances, such as a load balancer or a firewall, and these all resulted in a very fixed configuration.[O7] With network function virtualization, we're taking those physical network appliances and transforming them into virtualized network functions or VNFs. These VNFs can make use of something of different physical hardware attributes expressed through a hypervisor. The hypervisor provides a layer of virtualization such that the VNF is independent from the needs NOTICES AND DISCLAIMERS [O1]This subtitle is relevant only for En>Es translation. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles. [O5]Please delete. [O6]Please merge these two subtitles. [O7]Please merge these two subtitles.", "label": [[110, 141, "NFV"], [162, 189, "SDN"], [194, 197, "SDN"], [758, 761, "SDN"], [715, 742, "SDN"], [818, 821, "SDN"], [2618, 2649, "NFV"], [2547, 2578, "NFV"], [3141, 3172, "NFV"], [3281, 3285, "VNFs"], [3248, 3277, "VNFs"], [3293, 3297, "VNFs"], [3433, 3447, "Virtualization"], [3462, 3465, "VNFs"]]}
{"id": 54, "data": " So now we're going to look at the history and purpose of SDN. If we look at what they were trying to accomplish originally, we found that networking was really becoming a bottleneck in a lot of workloads. We wanted to be able to adopt some of the virtualization and cloud technologies that were rapidly becoming available. And we saw that networking really needed to become programmable, automated, flexible, virtualized, as well as having abstracted interfaces. So, what that meant then was going from a vendor specific protocol, we wanted to have the control plane and the data plane separated. Now, one of the founding bodies that looked at network function virtualization and made use of software defined networking was ETSI. ETSI is a standard body that defined this ETSI network function virtualization architecture. And they laid out their goals and they're here on the left side of the slide. And the first one was getting better CAPEX by using commercial off the shelf technologies. Having flexibility in how one assigned virtual network functions to the hardware. Having the ability to develop new services very quickly and enable the pace of innovation. Improving the OPEX, the operational expense of the automation. Reducing power, so for example, you might have over time of day you would be able to power down specific hardware. And finally, having standardized open interfaces so that you could have a multi-vendor solution. And the architecture that was used to achieve this is here on the right side. So, starting from the bottom we've got the network function virtualization infrastructure. So, we've got hardware and software components to that. Then we've got the virtual network functions in the middle, the VNFs. On the right we've got the management and orchestration. And then overriding all of this we've got the operational support systems and billing support systems.[O1] So, in many cases, those operating support systems and billing support systems are already existing in the various communications service providers, and this needs to work with those. So, another key element of this entire architecture is the concept of network virtualization. So, what network virtualization does is the primary goal was enabling multi-tenancy. But it took a similar approach that we did to server virtualization and applied it to the network itself. So, it does this through creating overlays. So, by connecting virtual machines through a V-switch and having potentially multiple tenants, we put an overlay on the physical network so that these multi-tenants can have whatever policy makes sense for the business application. It may be that strict isolation is the desired intent, in which case I can be programmatically configured. All of this is creating the network transformation journey that we're on. We're going from these purpose-built silos of hardware where each one had its own management console and was very hard to configure, and going to a multi-purpose kind of environment where you've got common infrastructure, orchestration. You've got unified management control. And you're really able to transform the business and achieve the workload efficiencies using Intel architecture and enabling all of the platform features that are necessary through orchestration and software defined networking and network function virtualization. So really at the end of the day, what we have with SDN and NFV is open innovation. This allows us to have competitive competition in the supply chain. We have software defined networks. This has the right network abstractions to enable faster innovation. And finally, network function virtualization, which has all the attributes we talked about like reducing CAPEX, OPEX, and power consumption. NOTICES AND DISCLAIMERS", "label": [[58, 61, "SDN"], [248, 262, "Virtualization"], [410, 421, "Virtualization"], [778, 809, "NFV"], [1032, 1057, "VNFs"], [1562, 1593, "NFV"], [1685, 1710, "VNFs"], [1730, 1734, "VNFs"], [2154, 2176, "NFV"], [2187, 2209, "NFV"], [3417, 3420, "SDN"], [3425, 3428, "NFV"], [3525, 3550, "SDN"], [3634, 3665, "NFV"], [645, 676, "NFV"], [2316, 2330, "Virtualization"], [3333, 3364, "NFV"], [2015, 2047, "Cloud Service Providers"]]}
{"id": 55, "data": " All right. So now we're going to look at the big picture and where SDN or SDI fits for the communications service providers. [O1] And what's really happening with this Software-Defined Infrastructure is it is transforming today's networking infrastructure. [O2] The cloud already transformed the data center, and now it's transforming the network. So, if we look at what's happening to service delivery, we've got a bunch of different workloads here in various Virtual Network Functions, the VNFs. So, one example the, Evolved Packet Core-- so that's used for call setup and tear down and supporting a lot of our smartphone infrastructure. There's a variety of these different workloads. These can be placed using SDI anywhere and any time because we've got the software-defined infrastructure and the layers of virtualization to support that. Part of that would be the orchestration software which worries about the placement and matching of workloads to the appropriate hardware. We've got the attributes of the infrastructure-- so, for example, power, performance, security, etc. We're able to do this on high-volume servers such as Intel architecture. And by having the knowledge of what the workload needs and what the infrastructure provides, we can meet very demanding SLAs. Finally, by doing all of this, we're pooling resources across a range of hardware combinations and be able to get much better efficiency by having resource pools that can adapt to changing needs of the business. Part of this transformation is having the tools and the automation to manage it effectively. So, part of it would be this unified management plane. This is where the communications really does meet the cloud. Being able to automate operations in a very efficient way-- this allows us to enforce policy do the workload placement, manage the lifecycle of these various business applications. And finally, really modernize and virtualize the overall system architecture-- so, part of this would be our 4 to 1 workload consolidation-- I'll show that in the next slide-- and having platforms that are optimized for NFV. So, what do we mean by the 4 to 1 workload consolidation? Well, this is a journey that Intel has been on for quite some time where we've moved from having a Xeon processor, for example, really only being used for the application and control plane, but moving it so that it can also work across packet processing in an effective way and finally, being able to do some of the signal processing in a very efficient way. So, this is what we mean by the 4 to 1 workload consolidation application control packet and signal processing. Network function virtualization-- this is all about virtualizing these applications. So, whether it be on the control plane, the data plane, or some next generation services, all of this, again, working on Intel architecture. And finally, looking at NFV/SDN for communications, in the big picture, what we're able to do is have the orchestration and the controller manage a number of compute nodes along with its networking and storage attributes in a very efficient pooled manner.", "label": [[68, 71, "SDN"], [462, 487, "VNFs"], [493, 497, "VNFs"], [813, 827, "Virtualization"], [1919, 1929, "Virtualization"], [2105, 2108, "NFV"], [2639, 2670, "NFV"], [2691, 2703, "Virtualization"], [2889, 2892, "NFV"], [2893, 2896, "SDN"], [520, 539, "EPC"]]}
{"id": 56, "data": " All right. So now, we're going to be looking at who uses SDN and why.[O2] So, the first one we're going to look at is the Communication Service Provider. What they're trying to achieve is to reduce the overall cost of network operation and also allow them to develop new incremental revenue sources through business innovation. Next, the Cloud Service Providers. So, they're really trying to achieve massive levels of scale, but also maintaining service level agreements and able to adapt to changes in customer demands while overall reducing their cost of delivery. Next, we've got the Enterprise, which is really all about delivering business value as fast as possible and with lowest total cost of ownership. Finally, we've got Technical Computing where really, we want to shift the boundaries and operate at maximum scale to get the most efficient use and most efficient performance for their applications. So, all of this requires flexible efficient infrastructure NOTICES AND DISCLAIMERS [O1]This subtitle is relevant only for EN>ES translation. [O2]Please merge these two subtitles.", "label": [[58, 61, "SDN"], [123, 153, "Communications Service Providers"], [339, 362, "Cloud Service Providers"]]}
{"id": 57, "data": " We've talked about 5G and why network slicing is an important element of 5G. Let's look at how that translates into the network architecture, and what are the challenges in 5G. When we compare the 4G network with the 5G network, we see that the 4G network is fairly fixed in its architecture. There is an access network that is comprised of [INAUDIBLE]---- the base stations. And then there is a centralized core network. And beyond that, there is the internet and the cloud. This fixed architecture is very optimum to deliver the broadband services that 4G was built for. With 5G, however, because we have the need to deliver different types of slices, having a fixed network architecture is not an option anymore. And we need to look at specific needs of each of the slices. Whereas the evolved or enhanced mobile broadband may have a similar network topology and network architecture as the 4G network, the massive IoT and the mission critical IoT will have very different ideal architectures. When we look at the specific requirements that are being set for each of these slices and each of different types of services, we can see that, in the case of 4G, it was fairly straightforward. There were mainly two different types of KPIs. There was the data rate per user, and there was the end-to-end latency because there was one type of service. With 5G, we see a much wider variety of requirements and use cases.[O2] We can go from the broadband access in dense areas, where downlink requirements are 300 megabits per second and uplink are 50 megabits per second. The end-to-end latency is ten milliseconds. And mobility will range from 0 to 100 kilometers per hour. But there are multiple definitions of other type of use cases, where the KPIs, the user experience database, the end-to-end latency, and the mobility will differ. And this is something that really needs to be taken into consideration. And we can consider each of these use case categories almost like a network slice. So, when we look at the implications of implementing different types of networks slices in the network, we see that there are different aspects that need to be into consideration. On one side, there is the radio network. What is the ideal Radio Access Technology that needs to be used for each slice? For example, for mobile broadband, the typical packet size-- the typical end-to-end latency will be different than, for example, a massive IoT. Therefore, the Access Technology-- the Radio Access Technology can be different for each type of slice. In the case of that core network as well, it needs to be-- some occasions, you will want to have the core network closer to the user, closer to the base station, such as in low latency communications. In some other occasions, you may want to have a more centralized core network, such a the massive IoT. In some cases, you will have mobility. In some cases, you will not have mobility. So, the softwarization of the network elements and taking advantage of software-defined networking and network virtualization are a key enabler. Also, the separation between the control plane functions and the user plane functions are really able to help scale in different ways for each different slice. A key aspect of network slicing is about layered architecture, where you have a service instance layer that is responsible for the end-to-end user or business services supported in the network. There is a network slice layer that is responsible for the actual network characteristics required to deliver that service. And finally, there is a resource layer that is actually responsible for the physical and virtual network functions and the locations that are required to implement a specific slice instance. Let's look at some of the key enablers for network slicing. Starting from the top, we have software-defined networks, which is critical to enable programmable, and simplified, and automated operations of network management. Network function virtualization abstracts the hardware and provides scalability and flexibility to execute network functions where they're needed. [O6] On the radio access network, you can use different types of spectrum, different type of Radio Access Technology, to deliver the specific service that is required. [O7] In terms of devices, some devices-- some new type of devices will come with 5G. Whereas in 4G and previously, it was mostly the smartphone and similar type of devices, now, there will be new type of devices that will come to market that will be able to receive these different types of services being delivered by 5G. From the point of view of end-to-end security, different type of services may have different type of requirements. So, having end-to-end security service level agreements will be important. We test a little bit of management already. And it is really critical to be able to manage a 5G network that is very dynamic and very adaptable to automate some of the typical processes required in network management, such as performance management, faults management, et cetera. Another key aspect of network slicing will be energy. Energy consumption is an important aspect for every service provider that is managing a 5G network. And for the user, it's also very important that 5G delivers battery life at least equal or better than previous generations. And finally, resiliency and robustness that some use cases require, such as autonomous driving or remote surgery. Let's look at how 3GPP has architected the 5G system to enable the implementation of different types of network slices.[O12] On one side, they have clearly desegregated the User Plane functions from the Control Plane functions, which enables each type of function to be scaled independently and to be located in the network-- Let's have a look at how 3GPP has architected the 5G system to enable the implementation of different types of network slicings in the 5G network. On one side, they have clearly desegregated the User Plane functions from the Control Plane functions. This enable each type of function to be scaled independently and to be located in the part of the network where it is required. This type of modular design enables flexible and efficient network slicing by taking advantage of the SDN and NFB principles. [O15] At the same time, it was also critical to clearly separate the access part of the network and the core part of the network and ensure that network slices could be implemented on both sides. With the advent of network transformation, the 5G networks of the future will be comprised of distributed data centers, where the network functions defined by 3GPP can be placed as software functions. There will be different layers of these data centers. On one side, there will be the typical core network, big data centers, with some of the more centralized functions will be placed. But there will be further layers closer to the edge, such as the regional data centers and local central offices, where compute platforms will be placed, and some of the network functions that require more distribution can be placed. This will be specifically very relevant for management of the latency-- end-to-end latency for different types of services. We will typically see that the base station is located less than 5 milliseconds from the user, where the local central office is within 10 milliseconds, usually.[O17] So, these types of services that requires extremely low latency will need to be placed in a more distributed manner in this type of local central office compute platforms. Regional data centers are located a little bit further out. And therefore, the latency will be a little bit wider-- typically, within 40 milliseconds. And for those network functions that need to go all the way to the cloud, latency is to be expected to be around 100 milliseconds. [O2]Please merge these two subtitles. [ND3]Need to double check with client [O4]Please delete. [O5]Please delete. [O6]Please delete. [O7]Please delete. [O8]Please delete. [O9]Please delete. [O10]Please delete. [O11]Please delete. [O12]Please merge these two subtitles. [O13]Please delete. [O14]Please merge these two subtitles. [O15]Please delete. [O16]Please delete. [O17]Please merge these two subtitles.", "label": [[20, 22, "5G"], [174, 176, "5G"], [74, 76, "5G"], [218, 220, "5G"], [919, 922, "IoT"], [948, 951, "IoT"], [1354, 1356, "5G"], [2429, 2432, "IoT"], [2473, 2496, "Radio Access Technology"], [2836, 2840, "IoT"], [3035, 3049, "Virtualization"], [3636, 3661, "VNFs"], [3962, 3993, "NFV"], [4120, 4141, "Radio Access Network"], [4883, 4885, "5G"], [5212, 5214, "5G"], [5272, 5274, "5G"], [5506, 5508, "5G"], [5839, 5841, "5G"], [5924, 5926, "5G"], [5900, 5916, "Network Slicing"], [579, 581, "5G"], [2228, 2251, "Radio Access Technology"], [31, 46, "Network Slicing"], [4358, 4360, "5G"], [4596, 4598, "5G"], [5092, 5107, "Network Slicing"], [5567, 5581, "Network Slicing"], [6226, 6241, "Network Slicing"], [6269, 6272, "SDN"], [6438, 6452, "Network Slicing"], [6536, 6538, "5G"], [2061, 2076, "Network Slicing"], [3245, 3260, "Network Slicing"], [3434, 3447, "Network Slicing"], [3781, 3796, "Network Slicing"], [3829, 3854, "SDN"], [4202, 4225, "Radio Access Technology"]]}
{"id": 58, "data": " Network slicing is a concept usually associated with 5G. Let's have a look why. And let's start by asking, what is 5G? 5G is the next generation of cellular networks. And it's going to bring some new things and differences from previous generations. I we start with 2G and looking at what 2G delivered, it was mostly voice and telephony. 3G also delivered data. But it really was 4Gs the one that had brought with it the explosion of mobile broadband with very fast data rates and relatively low latency. [O2] 5G will continue that-- will still continue to deliver those types of services-- much higher throughput, much lower latency. But with it, there's going to be also other type of tools and other types of technologies-- like ultra-reliable, like massive IoT, like low latency-- that are more focused on deliver services to things and devises than just consumers. So 5G is going to have a much wider ring and a much wider umbrella of capability than the previous generations that were strictly more focused on consumers. Because of the breadth of capability that 5G brings, it is considered one of the key enablers of the new data economy and the digital transformation that is occurring at the moment in various sectors. If we look, for example, at autonomous car, and autonomous car generates one gigabyte of data per second. A smart hospital generates up to 4,000 gigabytes of data per day. And that even gets dwarfed by a connected factory that can generate up to 1 million of gigabytes per day. All this data that these smart and connected things are generating needs to be transported to where it can be analyzed and insights gathered from. 5G is a key enabler of that. It's one of the key technologies that can transport reliably all these huge amounts of data that is being generated by these types of smart things to the cloud, where it can be processed and analyzed. The 5G capabilities can be categorized in three different buckets. When we look at ultra reliability and low latency, often referred to as mission critical, this type of capability can enable many different types of use cases, such as health care-- remote surgery, for example-- or autonomous driving, or it can really provide a platform for emergency services to communicate safely and in real time. It can also provide the capabilities to a drone to self fly. The second type of capability that 5G brings the massive machine-to-machine connectivity.[O3] This type of capability that 5G brings is primarily focused on connecting a very big number of devices to the network that typically don't send a significant amount of data. They may send short bursts occasionally, and maybe sensors. But sometimes, they're not located close to the base station. Sometimes they're located far apart, such as smart agriculture. This is a big farm that has some sensors located out of normal reach of the network. Or in manufacturing-- a smart factory, where indoors, the coverage is poor. So, this is a different type of capability that 5G brings. Lastly, the third bucket or the third category of capability is enhanced mobile broadband. This type of capability can enable different types of use cases, from virtual and merged reality, to broadband at home, to gaming, entertainment, high-quality video streaming, and also, mobile office. Because of the wide variety of requirements that these different use cases bring to the network, is why we have, in 5G, the concept of network slice. Each different type of network slice can deliver different type of capability. The requirements of our wireless broadband network slice that is required to deliver high bandwidth, for example, to a device such as smartphone will be very different to the type of ultra low latency and real-time control that an autonomous car needs. So, the autonomous car will have a different type of slice to deliver that type of service. And the same goes for IoT and sensors that require very small bandwidth, very small energy consumption to ensure that there is low battery life, with very small amounts of data being transmitted in bursts every so often. And a different type of slice could be video streaming, where you are transmitting very high quality video to a larger device, such as a smart TV that is ultra-high definition. The concept of 5G and network slices are very related to the network transformation that we have seen happen in the market over the last few years. Network transformation is about moving from physical appliances to more of a software-defined network, where the use of general purpose computing is extended all the way from the core network to the edge. Because the 5G network and the different slices that it needs to implement will require many different architectures, with very different topologies. Being able to use software and being able to use general purpose computing platforms is going to be a key aspect of 5G. As a key example, Edge Computing, as an element of network transformation, it's going to be one of the key enablers of low latency communications.[O5] [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please delete. [O5]Please merge these two subtitles. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please delete. [O5]Please merge these two subtitles.", "label": [[54, 56, "5G"], [1, 16, "Network Slicing"], [116, 118, "5G"], [120, 122, "5G"], [762, 765, "IoT"], [874, 876, "5G"], [1070, 1072, "5G"], [1654, 1656, "5G"], [1888, 1890, "5G"], [2381, 2383, "5G"], [2469, 2471, "5G"], [3009, 3011, "5G"], [3428, 3430, "5G"], [3908, 3911, "IoT"], [4649, 4651, "5G"], [4903, 4905, "5G"], [4925, 4939, "Network Edge"], [4299, 4301, "5G"], [4306, 4320, "Network Slicing"]]}
{"id": 59, "data": " [O1] So, we have seen that partly because of the concept of network slicing, the 5G networks of the future will be built upon general purpose computing platforms, and leverage the concept of network function virtualization, and software-defined networking. Let's have a look at what Intel is doing to enable network slicing in our own general purpose computing platforms. First of all, we've started with our reference implementation of our Mobile Edge Cloud Node. On top of our hardware platform, we built our virtualization layer using Wind River Titanium Cloud. And we've implemented our collection of different VNFs. We have mobile access VNFs using Intel FlexRan that delivers 4G and 5G virtual RAM. We have some fixed network functions, such as virtual CP reference implementation. We have core network functions, both from partners, like Mavenir delivering 4G core, and some of our own internal development, like FlexCore delivering 5G core. In addition to that, we have some Edge applications, like CDN or IoT Analytics, amongst others. And we even have an Intel Cloud Adapter that allows public cloud applications to be hosted in these Edge Cloud Nodes. On top of that, we also have ONAP Orchestrator and a platform manager, which creates a full-fledged reference implementation of what a real Mobile Edge Cloud Node could look like. This reference implementation allows us to fine tune the configuration. It allows us to learn about the different optimizations that are required.[O3] And allows us to then help the ecosystem fine tune their own implementations. Looking specifically at our last generation of platforms, there are a few capabilities that will help implement different types of networks license, and control how different the resources of the platform are allocated to each of the network functions. [O4] First [INAUDIBLE] capability is the last level cache slicing. Using this capability, it's possible to allocate a portion of the last level cache to a specific application or VNF. Therefore, creating a service level agreement between the platform and the VNF to ensure that that portion of cache is uniquely allocated to that VNF, and there is no interference from other VNFs in the platform. Another capability in the platform that will enable network slicing is located in the network interface card, referring to packet classification slicing. Using this capability, the network interface card can assign resources to specific network slices. Therefore, delivering quality of service and delivering a service level agreement with the network license and then VNF implementing that network slice to ensure that the resources in the NIC are allocated appropriately to each network slice. [O2]Please delete. [O3]Please merge these two subtitles. [O4]Please delete.", "label": [[61, 76, "Network Slicing"], [82, 84, "5G"], [192, 223, "NFV"], [309, 324, "Network Slicing"], [512, 526, "Virtualization"], [616, 620, "VNFs"], [644, 648, "VNFs"], [690, 692, "5G"], [693, 704, "vRAN"], [941, 943, "5G"], [1015, 1018, "IoT"], [2005, 2008, "VNFs"], [2085, 2088, "VNFs"], [2156, 2159, "VNFs"], [2201, 2205, "VNFs"], [2460, 2474, "Network Slicing"], [2592, 2595, "VNFs"], [2704, 2717, "Network Slicing"]]}
{"id": 60, "data": " [O1] We're going to be talking about some of the server ingredients that are fundamental to virtualization, as well as some of the concepts and a little bit of the background that are our motivation behind virtualization in these next sections. So, let's start off with the server ingredients. Now, fundamentally when we talk about virtualization, if we're talking about it from the bottom up, we talk about the compute, the network, and the storage fundamental aspects of it. And when we mention the compute, what we're really talking about are the resources inside the CPU. And that CPU being a multi-cored element. So, there are multiple instantiations then of core processes inside that bit of silicon, inside of that CPU that this virtualization relies on. So, in our space, the Intel Xeon processor family, the 6200 and the 6100 family are well suited for the virtualization in the VI space, their specific SKUs as well. So, these are specific then releases of it with a certain number of cores and a certain configuration of their I/O that provide those compute resources that are well suited for our virtualization space. And then from a networking standpoint, so, these machines are connected obviously into some type of a network. And in that space, depending on the model of virtualization, we either need a very balanced configuration for that, serving those socketed CPUs in a balanced way. So, if you've got a dual socketed system, you would want to have your network interfaces similarly balanced from that configuration standpoint. Or there are other workloads where there may be an advantage to having it asymmetrical, where our network interface cards exist more heavily of these socketed CPUs, and then the workloads will run on the other. But in either case, the Intel network controllers of the 710 series, and then the 8259 series family for 10 GB and 40 GB interfaces are well suited for that. On the storage side of things, we've really moved away from the spinning drives. And the solid-state drives give much more reliability, higher performance, and better throughput than their predecessors. And in this case, the Intel SSD Optane D for data centers, particularly the P4800x series are, again, well suited for that type of virtualization. But similarly, from a compute standpoint, it's just not complete. There are interesting workloads that come into play that can take advantage of accelerators. Some of those accelerations might be encryption or compression, for example. In that case, technology such as the QAT accelerator cards can be added in again in some configurations. And an asymmetrical design is optimal. And others, you'd want to see a symmetrical configuration. In addition to some of the visual compute acceleration cards for workloads, it maybe intense in graphic processing. So, when we put all that together, we've got a strong foundation that provides us with that network compute, storage, and acceleration on which we're going to now start building the stack from a virtualization standpoint.", "label": [[93, 107, "Virtualization"], [207, 221, "Virtualization"], [333, 347, "Virtualization"], [737, 751, "Virtualization"], [867, 881, "Virtualization"], [1109, 1123, "Virtualization"], [1287, 1301, "Virtualization"], [2252, 2266, "Virtualization"], [3018, 3032, "Virtualization"]]}
{"id": 61, "data": " Virtualization is not the end game. When we talk about virtual machines in particular, it's a step along the way. It's part of this journey of going from those physical machines of the past, where inside an operational network they're very dedicated for a fixed function. Sometimes you talk about that. Sometimes we talk about it from that purpose-built standpoint. To one where we're actually utilizing those resources more effectively. There aren't stranded resources. We've assured that we've got tenancy separation because of the virtual machine. To one in the future where we see a more distributed virtualization, where we've got the concept of those compute, network, and storage elements I spoke of before that present themselves as a resource pool. And then from an orchestration standpoint, when we've got applications that need to be instantiated, we can probe our resource pool and find the right collection of services or capabilities from a platform and then instantiate those applications on top of that resource pool. So those are some of the motivations. Now, there's a penalty in performance when we do this. If it were a perfect world, there wouldn't be one. But this is a physical world that we're operating in. And when we start having multiple instantiations of operating systems, there's going to be some overhead. We've got network interfaces that we've got to concern ourselves with, and the traffic flowing in and out of those network interfaces. And they've got a greater volume of software, potentially, that they need to flow through. Similarly, some of those cores maybe nearer or further away from some of those physical resources. So, we may not see the actual capabilities from a native performance on those systems that were purpose built in the past. And, again, those may have stranded resources. So, this is the balancing game, if you will, on that teeter totter.[O2] But what we want to do is we want to raise that virtualization platform as high as possible. And sometimes we're going to have to do that with hardware assist capabilities. So, whether that hardware assist is something that's actually taking place down a layer or two low in the network driver, or whether that's acceleration that's in the software using virtualization capabilities like the DPDK packet acceleration capability, or whether that's other acceleration capabilities that come in from adjacency cards, like we mentioned the Quick Assist technology for encryption or compression. And obviously decryption and decompression as well. But in addition to that, one of the things that Intel has continued to drive for the last 15, 18 years or so in this virtualization space, on the server space, though, it's continued to improve the micro architecture with each iteration of those releases of the Xeon processors to enhance the performance that's needed in order to meet the goals of that virtualization. So, our roadmap continues to evolve as we learn more, and as we move forward with this virtualization effort. So, with that in mind, let's take a step back and talk about the hypervisors themselves. The hypervisors are an element that's necessary in this virtualization layer. And we're going to look at a couple of the diagrams as we dig down into this. And fundamentally, the computer science folks like to break hypervisors up into two areas. They talk about them as type one and type two. It's interesting that the hypervisor term itself is sort of the superior to the supervisor. And the supervisor was the concept of what the operating system provided. So, it's not unusual that we can now think about the hypervisor as being an extension to what takes place at the operating system level. And in those two types of hypervisors, type one and type two-- and, again, this is a purist's view of the world, not necessarily the actual application view of the world-- type one is that there may not be an actual host operating system.[O3] The hypervisor itself runs native on top of the bare metal. And in type two you will have a host operating system, and then a hypervisor, and then instantiated on top of that multiples. We've got examples. ESXi and Hyper-V are of the type one. And probably the more common would be KVM or VirtualBox, which you're going to find as the type two. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles.", "label": [[1, 15, "Virtualization"], [605, 619, "Virtualization"], [2666, 2680, "Virtualization"], [2903, 2917, "Virtualization"], [3006, 3020, "Virtualization"], [3174, 3188, "Virtualization"], [1954, 1968, "Virtualization"], [2261, 2275, "Virtualization"]]}
{"id": 62, "data": " [O1] So, let's touch on the virtualization concepts themselves. So, first of all, why are we even interested in virtualization? Fundamentally, if we look at what has happened in server technology in the past 20 years or so is that, driven by Moore's law, we've seen a transformation where we had a single core inside a CPU to where we've got a very large number of cores inside CPUs. And then similarly, from a single socketed server into multiple socketed servers. In order to utilize those resources, the design of applications themselves, from a historical standpoint where you had the hardware and the operating system and then a collection of applications running on that, needs to transform so that those systems can more fully utilize the resources that are available there. So, what we've done inside virtualization-- and this is not a new concept. This goes back 50 years or so. From a server standpoint in the high-volume servers that we look at today, it probably only goes back about 12 or 15 years or so. But the computer science theory of it, we saw this in mainframes a very long time ago. They realized that in order to maximize utilization of that physical resource that this concept that they now call virtualization came into play. And that is that we can have multiple instantiations of operating systems co-resident on a single hardware platform. And then on top of those instantiations of the operating system, interesting applications, or interesting services that can be there. So, before the virtualization came into play so we really had a single instantiation of that operating system. And that hardware and that software then were very tightly coupled in that that application itself was specific to that operating system that was very specifically tied to that hardware instantiation. And then running multiple applications that had a great degree of variance in them, not just additional instantiations of the same applications, but quite divergent applications, often led to conflicts in the utilization of those physical resources. There were stranded resources as well. So, we had conflicts and we had stranded resources in there. And then there was also some concern about security from a standpoint. So, if you think about multi-tenancy and the fact that if you've got applications from two financial institutions running on the same set of servers, they may be concerned about those coming together on that. So fast forward to server virtualization. This is one of the ways that we can actually break that dependency that I spoke of between the hardware and the operating system, and then the application that's running on top of it. And yet, we can still have a single platform, from a management standpoint, as opposed to just starting to copy multiple instantiations of that hardware in there. And at some point, it does allow us a certain degree of independency from that underlying network, compute, and storage of that hardware that we spoke of in the previous section. And it gives us a relatively strong isolation. So, this is a security statement. So, I gave you the two financial institutions model before. So, where multiple operating systems then have a greater security boundary on that hardware from that standpoint. So, while no system can be perfectly secure, it gives us a greater benefit of security from that standpoint. So, what are some of the key properties that we look for then in these things that we call virtual machines? And what we mean by that then is this application that's running in multiple instantiations on a single network, compute, and storage. First of all, it's that partitioning so that we can run multiple operating systems on one physical machine, if that's necessary and desired. And then we can actually divide those resources, and in some cases allocate and manage them separately for the workloads that are running on top of those different instantiations of the operating systems.[O2] That isolation that I spoke of from a memory management standpoint, from a security and from a fault, is one of the things that we can also ensure there. So, if for whatever reason a particular operating system has a critical defect and that application is going to restart, it doesn't take down the rest of the services that are running on that virtualized machine because of that independency. Whereas if they're all running on top of a single operating system, it's conceivable that one application could cause that operating system to segmentation fault. And then you'd lose the services of all the other applications that are running on that environment. Then there's the concept of encapsulation. So, the encapsulation gives us the ability to have a namespace, if you will, for the entire state of that virtual machine. And that can be saved and preserved so that we can actually generate replications of those. And that leads us then into the migration, and it introduces some lifecycle management capabilities to that application that we may not have had when we were on a purpose built system in the past. So, let's take a look then at that encapsulation concept to begin with. What does it allow us to do? It allows us to suspend operations. So, we could actually stop that instantiation from the operating system. Not just the application itself, but the operating system itself.[O3] Once it's stopped or suspended, you can do something with it. Let's restart it. We've made a change to it. Let's restart it. We could also snapshot it. When it's in a suspended state, for example, we may be able to go through and make an exact copy of it using something like a duplication, and then prepare that to be moved to another environment. And the reason behind that is I want to make a copy of it to clone it to another machine. And that's a very difficult thing to do, again, when we think about the support of our purpose built systems of the past where that operating system was a single instantiation and all the applications lived on it. And then the reason for that is maybe I am doing lifecycle management, and I want to upgrade the software. But I want to upgrade it on a trial basis, and see that that replicated instance can be upgraded, and it's going to provide the services that I want. If I like it, I continue to let it live. And if not, I may shut it down, and then move my traffic back to the previous instance of the platform. Some of the other things that we can do then are record and replay operations from the functional standpoint. And, again, the driver behind this is to improve the utilization of that hardware as we've continued to provide services of increasing capability driven by the increasing core counts, increasing memory utilization, increasing I/O capabilities, and that we can also then, again, from that isolation standpoint, maintain some type of a fault tolerance in our platform that's independent of the application itself.[O4] So those are all some of the things that we've got from a standpoint. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[29, 43, "Virtualization"], [113, 127, "Virtualization"], [810, 824, "Virtualization"], [1518, 1532, "Virtualization"], [2471, 2485, "Virtualization"], [1221, 1235, "Virtualization"], [4317, 4328, "Virtualization"]]}
{"id": 63, "data": " So, let's talk about a particular hypervisor just to dig down a little bit and understand how these creatures work from that standpoint, and KVM, or the kernel virtual module is a very common one. So KVM is a Linux kernel module. So that's sort of code speak, and it's something that gets pushed down in and runs at the same level as the operating system kernel itself, or at ring 0, into that Linux space. And then it allows guest operating systems to rely on its resources to provide access to that hardware. It does require some capabilities in the physical hardware itself is that the hypervisor needs to be able to access that hardware directly at that ring 0 level. So, there's some BIOS configurations that are necessary to be set correctly onto your server space, and obviously you've got to have the right server processor, the CPU, in order to support those capabilities. And certainly, all high end Xeon processors support that as it is today. And sometimes they talk about this as being sort of a paravirtualization that allows for access to the physical resources. The hypervisors are tightly integrated into-- KVM in this space is tightly integrated into that core operating system, that host operating system itself. And it does rely on some of the resources inside that host operating system. For example, the virtual CPU scheduler is one of them, and it will run actually as one of those threads down that kernel space, as well as some memory management, some I/O. So, the capabilities of the accessing device drivers and some of that networking is still relied on from the host operating standpoint. So, if we look at an architectural diagram, very high level architectural diagram, 'ou'll see that over on the right of that slide is that there is a KVM module t'at's sitting at the same level as the Linux kernel module where you find the drivers and those sorts of things. So t'at's the element that gets pushed in there, and th're's ways from that host operating system that you can actually see that that module has been pushed from that standpoint. And 'ou'll notice that t'ey've got some ring indicators there. Ring 0, ring 3 are very common from this standpoint, and then th're's this introductory concept of a ring minus 1. So higher from a security standpoint than what we find in that ring 0 standpoint. So, this is where the hypervisor, elements in that module, live from a security standpoint, giving it complete access, then, to those underlying physical elements inside that. Up at ring 3-- so this is a less secure area-- this is where we find those user applications that then would be running on a guest operating system and relying on elements inside that virtual machine-- virtual I/O or QMM-- for the memory management elements of that virtual machine. So here we see that introduction, then, of the KVM, and that's that kernel virtualization module that's gotten pushed into that operating system that relies on those hardware capabilities to access the capabilities of those multiple cores of those CPUs that are in there. And then that quick emulator for that element allowing for the memory management and a variety of other architectural capabilities. And then from an I/O standpoint, obviously, these machines don't do anything from a standalone interface point. So, they've got to be able to allow data to flow through and back so that application space will probably have an added type of an IP address, and then the physical host itself may have another IP address, and something has to manage the mapping of those as that traffic flows in and out of that system. So, from a very high level, this is what you might think of from a type two KVM type hypervisor. If it were a type one, then that hypervisor itself would take that entire ring 0 space that you see there. So again, from a conceptual standpoint, very similar. From a purist standpoint, we might talk about that a little bit differently. But this is an example of one of the very popular hypervisors you're going to find in a comms network virtualization, and even in some cloud spaces from a virtualization standpoint.[O2] NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles.", "label": [[4049, 4063, "Virtualization"], [4102, 4116, "Virtualization"], [2867, 2881, "Virtualization"]]}
{"id": 64, "data": " So, let's spend a little bit of time talking about cloud models as we look at how they apply into this journey that we're talking about from a virtualization standpoint. So, again, we understand from the previous sections we were talking about traditional platforms where an application itself is pretty tightly bound to both the host operating system and the underlying hardware that exists there. And we've got those stranded resources. We're not able to fully utilize those compute networking and storage resources that are in there. And that first step then of accelerating the consumption of those resources is that virtualization space. And we've spent a little bit of time talking about that. The next step really then is a greater desegregation to where these applications themselves start looking more like-- we use the term cloud native sometimes in this space. But where the applications themselves start becoming lighter and lighter weight. And the resources inside that networking, compute, and storage may span then from a virtualization into something that we're going to talk a little bit more about from a containerization so that the applications themselves start looking more like microservices from that design standpoint. And then finally, to a future vision where we've got applications themselves that are completely disaggregated from the underlying network, compute, and storage, from that hardware platform itself. And then there's this concept of an orchestration layer that's using some type of resource awareness itself, both from an application standpoint, and from a physical standpoint. And then it instantiates those applications on the appropriate resources On Demand, and gives us, again, a greater utilization of the resource pool inside that cloud environment. So, this is the journey that we're on in that virtualization space.[O2] And some will argue that we're somewhere between that first step and second step in that macro level, as we are today. Inside the cloud community itself then we've got a couple of different types of cloud. So, from a taxonomy standpoint, there are different services. And we've got a slide later that will talk about some of those specific services. But infrastructure as a service or platform as a service. And then the applications itself. And then in that cloud environment, we can ask ourselves, well, where are those resources physically that these types of services are running on? One of the most common that we understand is the public cloud environment. So, we can think of AWS, Google, or some of the other ones from that standpoint. So public cloud. And those environments would have that multi-tenancy. So, again, depending on the security issues, you may want to have virtual machines in that multi-tenancy type of an environment, as opposed to more of a micro type service where security may be less protected from a separate instantiation from an operating system standpoint. The next level is actually one where it's hybrid cloud. So, there are certain geographies around the world where we've got data sovereignty issues. And they may say that the data sovereignty has to reside within a certain geography. And yet, the compute resources may be out there in a larger cloud type of an environment. So, we're going to see applications in a hybrid type of cloud environment. And then we've finally got the concept of private, where all the cloud resources then are maintained inside that enterprise level itself. And then, again, they're utilizing this cloud concept to maximize the consumption of those compute network and storage resources that are in there. And then we've got to have the platforms themselves. So, the cloud platforms. Fundamentally, we've got two choices. You either roll your own, the do it yourself type model. Or you go with a commercial type of a package. And, again, the location, so we've already talked a little bit about the fact that it's not virtualized all the way down there. There has to be some physical instantiation of those compute resources. And some of that-- who's going to pay for the electricity? Some of that's going to be on prem at enterprise level, or off premises. So again, we mentioned a little bit about the types of services that exist out there. Infrastructure as a service, platform as a service. But ultimately what we want to get to is an application itself that's running, and either generating some customer value or some revenue generating, or some business value to us in that type of an environment. And we start building that stack up from that hardware to the system level to the virtualization orchestration, to the development type of an environment, and then, finally, that application environment. And those resources then can be accessed through services like AWS, Amazon Web Services, or Salesforce.com, Google types of environment, or a roll your own on something like a VMWare or an OpenStack, or some type of cloud foundation. So, we already mentioned some of this terminology. Software as a service. Platform as a service. Infrastructure as a service. So, we started at the high level. And we start working our way down on that other diagram from right to left. And then those resources in there, either the public hybrid or that private environment, all with that goal of either providing consumer capabilities, or from a resource standpoint. And you want to think of some examples like eBay, Twitter, or Instagram, Gmail. All of those would be consumer to consumer. Business to consumer. So, Netflix and Skype. Some types of collaboration. Your banking application may be a business to consumer. And then finally, the higher opportunity for a revenue generating standpoint may be in those business to business. So, there's the AWSs, the NetSuite, Microsoft Azure type of applications are examples of that. So, the point here is that we've got a transformation that's taken place, not just in the communication service provider industry, but in the enterprises, and in the consumer business space itself to maximize that performance, that storage capability, those web resources that drive our business operations, improving our operational efficiency of those physical resources that are inside our data centers. And I'll drive you towards our TCO. There's a really nice diagram over here on the right that shows some of the TCOs. Less than 60% is the actual physical infrastructure itself. And then you've got the power, network, and operation when you're looking at that. So, anything that we can do in order to drive those efficiencies are really good things. [O2]Please merge these two subtitles.", "label": [[144, 158, "Virtualization"], [622, 636, "Virtualization"], [1038, 1052, "Virtualization"], [3958, 3969, "Virtualization"], [4628, 4642, "Virtualization"], [1845, 1859, "Virtualization"]]}
{"id": 65, "data": " Hi. So, let's spend a little bit of time talking about one of those cloud capabilities that's in there, OpenStack in particular. Now, OpenStack itself is an industry consortium. So, there's an openstack.org that you can go out check out on the web. And it's a collection of projects that are managed through this forum itself. It provides a massively capable cloud and compute environment for both public and private or hybrid environments themselves and allows you to control an enormous number of compute, network, and storage resources, either through a graphical user's interface and dashboard itself or through API-- so from a program programmatic standpoint itself. So again, I suggest that you think about going out there and googling the openstack.org organization. Intel was one of the founding members of OpenStack back in 2011. Put this together-- there about 1,000 companies that are participating in the OpenStack org now. There are over 18,000 contributors to the software development that takes place into that environment and representing about 148 different countries give or take. There's a regular cadence that the OpenStack releases their software on their modules. Every six months there's a new release that comes out. And some of those releases are what they call Long-Term Support or LTS-- so, release 13 and released 16 and then 19 are going to be LTS, long-term support. And then the intermediate releases, for example, in 14 and 15, are going to have new features and capabilities developed in them. And you might want to think about those as being precursors or views of features that are going to come. So, if there's a critical defect, for example, in your long-term support release, you're going to have some capability of having that patched for many years in advance. And if you're on one of those intermediary releases, you might want to consider the operational considerations that your organization will take on if you've chosen to head down that path itself. So, we talked about projects themselves that are inside this OpenStack environment, inside the architecture. And in this table lists just a few of them. There are a lot more than this that's in here. But I want to highlight some of these projects. And the reason they're here, these are probably the foundation. They're the critical ones. They're the most important itself. So, I mentioned the graphical user's interface, or that UI that's in their horizon, is the project that provides that graphical dashboard that allows for the management of those cloud compute resources through this virtual interface instantiation management that is an OpenStack environment itself. For the compute resources themselves, Nova is the module that provides that. Networking is very critical. That comes from a neutron standpoint. Again, if you think about virtual machines, there's something that has to take control of the network translation from the host operating system's IP addresses, for example, and those that may be IP addresses that are associated with the virtual machines that are up there. Neutron is the mechanism that does that inside the inside OpenStack. Swift, Cinder, and Glance or the three modules that provide the storage for object, block, and image. And then, from a security standpoint, there's a package that's known as Keystone that provides the identification and authentication so that you can be assured that the system itself is locked down and is the one that you're operating on. We mentioned orchestration a little bit before, Heat is the module that provides the orchestration inside OpenStack. And you might hear people talk about Heat scripts themselves. So, these really are-- they're human-readable scripts that tell us how to apply the configuration and instantiation to create those compute resources that are in there. Obviously, when you start looking at this desegregated cloud environment, the compute resources, the performance of them and of that environment are going to be critical. And the project that provides the information to manage and control that is known as telemetry. And then finally, databases would be supported through the environment called Trove. So again, these are just some of the highlights of the key modules that are inside the OpenStack environment.[O2] I certainly encourage you to, if you're just in that, go ahead and Google it if you're not already familiar with what's happening in OpenStack-- tremendous amount of information that's available to you. [O2]Please merge subtitle 103 with 102 to avoid orphan words.", "label": [[105, 114, "OpenStack"], [135, 144, "OpenStack"], [194, 203, "OpenStack"], [747, 756, "OpenStack"], [816, 825, "OpenStack"], [918, 927, "OpenStack"], [1135, 1144, "OpenStack"], [2058, 2067, "OpenStack"], [2640, 2649, "OpenStack"], [3146, 3155, "OpenStack"], [3604, 3613, "OpenStack"], [4285, 4294, "OpenStack"], [4445, 4454, "OpenStack"]]}
{"id": 66, "data": " [O1] So earlier, we used the term VIM, or the Virtual Infrastructure Manager. This is a term that we find inside the ETSI utilization of the description of this cloud type of an environment. And we mentioned that the OpenStack provides one of those VIM capabilities that's in there. If we are using our terminology, double-click on that a little bit and take a look at the larger elements inside a well-defined cloud environment, you'll see that the VIM, the OpenStack instantiation in our previous example, is this bluish-shaded section in the lower right-hand corner where we find those elements of, for example, of Neutron and Nova, and Glance and Keystone. A variety of those projects were in that OpenStack environment. And that's a very critical element here, but it's not the entire cloud environment. It's a portion of it that allows for that management-- the M portion of that management. So over in that orchestration layer-- so this is the column that exists in that right-hand side. We're going to have VNF managers themselves. So, the application that we're running has to interface with a module that's higher than that OpenStack type of environment over in the orchestration layer. And that's where we're going to find those elements inside that VNF management layer, according to the ETSI model. And this is a well thought out, well-defined model. So, we may stretch the metal a little bit differently around some of this, but the applications themselves which are instantiated are still going to have to be managed at that higher level. And that exists over there in that MANO or that management orchestration layer inside that vertical stack on the right that would sit on top, for example, of that OpenStack environment. And then if we look at the actual compute platform itself and those resources that are running, that's sitting on the larger block diagram to the left of that-- and again, building it from the bottom, we've got the network interface cards, the compute and storage that are in there, then those hardware resources that are being utilized. And then some type of virtualization layer on top of that, whether it's type 1 or type 2. Again, if you think about KBM as a great model for that type of an environment. It's going to be sitting on top of a host operating system, but still has access to those resources. And it's an element inside that kernel module that provides that virtualization layer. And it's that encapsulation then that is managed by the VIM. But then in order for this to be useful, we've got to have what I call an interesting application or an interesting collection of applications running on top of that. And those would be things like inside the coms space, virtual gateways, or MMEs inside the mobility management element environment, or deep packet inspections, or border gateway types of functionalities. And those applications themselves can be single applications of a collection of processes, or multiple processes that are running and would have an element management type of an aspect in there that's providing for the consolidation of the information about the functionality of that application itself on top of that virtualization layer, back across to that VNF management model that we've got in there. And then, as we move even higher into the stack and then off to the left again, inside a coms environment, you're going to find the elements that we typically talk about as being the OSS's and BSS's. So, these are the back office billing systems, and these are the operational systems that exist inside that environment. So, this is what would be encapsulated then in the entirety inside that module of an ETSI mapping if we're using OpenStack. There are other models that can exist that may be simpler than this. For example, a distributed data center may have a simpler orchestration model that allows for microservices inside that data center to be centralized inside that OpenStack type of an environment. And you may see simpler architectures in some of the cloud environments.", "label": [[218, 227, "OpenStack"], [460, 469, "OpenStack"], [703, 712, "OpenStack"], [1135, 1144, "OpenStack"], [1262, 1265, "VNFs"], [1590, 1594, "MANO"], [1603, 1627, "MANO"], [1718, 1727, "OpenStack"], [2101, 2115, "Virtualization"], [2415, 2429, "Virtualization"], [3187, 3201, "Virtualization"], [3229, 3232, "VNFs"], [3709, 3718, "OpenStack"], [3951, 3960, "OpenStack"], [1016, 1019, "VNFs"]]}
{"id": 67, "data": " So, let's take a quick  look at a very insignificant but enlightening application of components and scenarios inside OpenStack. So, we've sort of flipped the diagram here a little bit on you is that now the VIM that is the OpenStack is living on the left hand side of this diagram. And this is where we would find the runtime applications of the Horizon and the CLI, so the user interface, the Swift, Nova, and Glance, the Cinder, the Keystone, Neutron, in that VIM layer. And then over on the right-hand side, we actually see the interesting workload example, if you would, and that is the compute. For example, we're doing some type of UCPE type of environment, the compute that would have the functionality of the router in that environment, the DPI and the gateway. Similarly, from a storage standpoint, you'll find that portions of the Swift that will actually be running on that compute resource itself in order to manage that object storage, for example, in this type of an environment. If we look at what happens if we're going to instantiate, for example, a storage type of an environment inside through the use of OpenStack is that VIM layer, there are a couple of cases. First one is if the no VM exists, is we've got to instantiate the VM itself. We've got to get that spun up. In order to do that, we've got to use the resources of Nova itself in accessing the capabilities of hypervisor on that targeted platform itself to allow that VM to come into instantiation. So, requests that a VM is created. And in doing that, then we've also got to use Neutron's capabilities and to configure the traffic for the networking standpoint, communicating with a virtual switch that would be out there as part of the instantiation of that compute node resources. And then finally, we're going to access the resources to speed up the capabilities inside Cinder. And each of those then is interfacing with the capabilities of the Keystone element for security and management inside that environment as we go along. So, this is just sort of a step by step module of what really happens in that type of an environment as that VIM aspect of the OpenStack is creating the virtual machine, plumbing the network interface, so that machine can access the real world, if you would, and then creating the instantiation of the interesting workload, in this case, a storage workload that's out there. So, in summary, we've got OpenStack is that open-source platform for cloud management in that VIM layer and for that virtual infrastructure manager, as we see that terminology, and it's a portion of that overall orchestration and management of elements in there. And those major components, as we've touched on several times before, Nova for the compute. Neutron for the networking. Cinder and Swift and Glace for the storage. And then Keystone for that authentication. Horizon for that user interface is going to be very critical from a management and operational standpoint. And then finally, the heat templates for that resource and orchestration, so that this is reproducible type of an environment. So, it's not a GUI click, click, click in order to make things instantiated. But we can actually programmatically create those instantiation type of environment. And then OpenStack provides those services to deploy and control or manage the lifecycle of those elements. And they're separated actually from that host platform itself. And this is one of the critical elements when we talk about desegregation of those purpose built platforms in the past is that desegregation of the management of that element itself from that and then the applications above it. So, that'll give you a quick example of creating a storage application inside the OpenStack environment and a summary, again, of those critical elements that would NOTICES AND DISCLAIMERS", "label": [[118, 127, "OpenStack"], [224, 233, "OpenStack"], [639, 643, "uCPE"], [1125, 1134, "OpenStack"], [2142, 2151, "OpenStack"], [2416, 2425, "OpenStack"], [3265, 3274, "OpenStack"], [3737, 3746, "OpenStack"]]}
{"id": 68, "data": " Hi, so we talked a little bit about OpenStack. Let's double click on that, then, and take a look at OpenStack as a platform, and start looking at some of those modules that are in there. So again, what we talked about is OpenStack really gives us that VIM layer that's in there, that allows us to provide that management capability-- that now we're computing storage. And from a management standpoint, that's also configuration and operational issues that are inside there, as we begin to build those interesting applications on top of it-- whether those applications are separated from a multi-tenancy standpoint. For example, if we're looking at a universal CPE and that type of an environment, we may want to have multi-tenancy as we're providing the capabilities from the switching, the routing, the deep packet inspection, or the firewall into that type of environment. Or whether we're building applications that go deeper into the core of an operational network. For example, the EPC capabilities inside a 4G or 5G type of a network, where we've got packet gateways, serving gateways, or mobility management elements, themselves. And we're operating those on a virtual structure layer-- to, again, maximize our utilization of our resources that are in there. And we're going to do this using the capabilities that we might find inside the technology that we call SDN, or Software Defined Networking, and in applying that into their NFV, or the Network Function Virtualization space. So, OpenStack is an environment that allows us to get that VIM-- that VIM, that virtual information infrastructure management layer-- as a portion of that management infrastructure-- of the MANO, as we looked at that ETSI model before-- as we build this type of environment up into our-- into our Cloud environment. So, let's take a look at those elements again in that OpenStack architecture itself, and double click a little bit on the functionality that's in there. The first and probably the most critical is the Nova layer. And the Nova layer gives us that management aspect of those compute resources. It provides the fabric that allows us to maintain that functionality that's in there. So, it's probably the most critical element inside that entire environment. Secondly, though, is that, in order for this virtual machine to be providing any useful work, is it's got to have access to the network itself. And Neutron is the module that provides that capability of allowing that traffic to flow from the host operating system IP address interface up into the operating system interface that would be associated with that virtual layer that's in there. The storage itself comes in a couple of different flavors-- block storage under Cinder. And we've got Glance providing image storage, and then Cinder providing, again, that block type storage in that computer environment. And then object storage under the Swift environment. And then we've got security that comes out of identification applications, so we know that the server that we're running this functionality on has not been misappropriated or corrupted in some way. Keystone is the element that provides identity management inside our environment. And then finally, from a user standpoint, the Horizon element provides that user interface-- that dashboard-- from an operational standpoint. And all of this leads up to that orchestration layer that we spoke of in the past, that's provided by the Heat modules or the Heat templates or the Heat scripts that are in there. And in order to monitor the functionality of this environment, we see the capabilities of Cinder-- I'm sorry, of ceilometry providing those metrics and those operational capabilities in that type of environment. And as we stitch all of this together, it's really important to understand that the OpenStack is providing that VIM layer that's in there. And it sits off-- it's not that it's not the computational layer itself, but it's the management aspect of those compute networking resources into that type environment. So, Neutron for the networking and Nova for the compute. The storage for block is Cinder and Swift. And then ceilometry itself-- as that system is humming along and doing some useful work, ceilometry interfaces will be providing interface communication back across to the orchestration of the management layer that's in that VIM and on up in the stack and into that ETSI model.", "label": [[37, 46, "OpenStack"], [101, 110, "OpenStack"], [222, 231, "OpenStack"], [651, 664, "uCPE"], [988, 991, "EPC"], [1020, 1023, "5G"], [1371, 1374, "SDN"], [1379, 1406, "SDN"], [1440, 1443, "NFV"], [1452, 1483, "NFV"], [1495, 1504, "OpenStack"], [1681, 1685, "MANO"], [1861, 1870, "OpenStack"], [3824, 3833, "OpenStack"]]}
{"id": 69, "data": " So, let's talk about some of the features that are available to us in the OpenStack. The first one really is related to the compute. And it's probably the most critical. So, under the Nova Scheduler we've got a collection of features that are available to us. The first is the page size. So, this has to do with the pages that are allocated[O1] in the translation buffer. So, support for huge pages, the 2 megabyte and 1 gigabyte. And this is important for the packet acceleration. The huge pages are necessary in order to keep thrashing to a minimum, as we start moving these data packets from the interfaces up to the application layer. The second is a CPU policy. And that is whether or not the cores themselves can be shared or whether they're dedicated. Dedicated sometimes implies we've pinned these workloads to these cores themselves and not allocated them to other resources. A third one is NUMA awareness. So, we've talked about multiple sockets in these high volume servers.[O2] And in those sockets themselves can be aligned with the network interface cards, or with memory, or with other features, with accelerator cards. And we may want to have those workloads specifically aligned for NUMA alignment. And what that does, is that minimizes the amount of traffic that goes from CPE to CPU, internal to the compute and gives you a greater throughput for those workloads that take advantage of that NUMA alignment. Another feature is that we talked about hyper threading. So, we can share virtual CPUs for workloads that can take advantage of a hyper threading type of architecture and increase the speed of the throughput of those workloads. And then there's additional CPU capabilities for scheduling and priority. One of the other really important issues is the networking balancing. As we've spoken before, these become interesting when they're plumbed to a real network. And network balancing is a critical element inside here. So, there's also robust support for live and cold migration, as well as a collection of other features that are available inside here. So, let's take a look at a capability inside OpenStack that we call Flavors. Flavors allow us then to, if you will, double click on some of those types of capabilities. This diagram gives us a little example of what we would see through the user's interface. So, using Horizon, you might be able to see this. And you can create particular Flavors then of that Nova capability that would be dependent on the requirements of the specific being VNF, or the type of workload that you have in there. So, with inside Flavors, then we can go down and take a look at, for example, the VNF descriptor or the VNFD, as we call it, and create specific Flavors then that we can instantiate inside that OpenStack. So, let's take a look quickly at some of those VNFDs that are available to us in OpenStack. So, the policy itself gives us a collection of very high level capabilities.[O3] And let's look at the VDU. So, this is the descriptor for the CPU itself. And that expands to another set of descriptors themselves. And we're going to take a look at this the CPU requirements. So, at the final stage, we've got the CPU requirements. So, we can specify, for example, the model of the CPU spec from binding standpoint. Now we can specify instruction set compatibility or the model of the CPU itself. So, if we had a workload that had special instructions in it, ABX512 for example compiled into it, we may want to make sure that we've chosen a CPU that is at least capable of providing that ABX512 set of instructions set. Similarly, we can get a minimum clock speeds, number of cores, or variety of capabilities that the provider of that network function, of that VNF would say these are the minimum requirements that we need. And then it becomes responsibility of Nova as it starts to instantiate that virtual function to ensure that we've allocated resources that meet the requirements of that feature. So, in a high-level end, from a summary standpoint these Telco workloads that we've spoken of, they have performance requirements that may differ significantly from those that are found in traditional IT workspace. And because of that, the features have been created inside the OpenStack environment that allow us to specify those requirements and ensure that we're going to allocate resources from our cloud environment that meet those requirements. OpenStack provides several work aggregation options, such as regions or host aggregates, so that we can establish zones that meet those requirements and when we allocate those resources, we can allocate them into those into those predefined regions or zones. And then, of course, we spoke very briefly about the flavors themselves. I encourage you to take a look at that if you've got access to that user's interface through Horizon. And walk through some of those tables and see what those features are. And those are all optional capabilities that you can specify as you spin up those workloads to ensure that you're targeting the systems that meet those minimum requirements for that particular workload. [O2]Please merge these 2 subtitles. [O3]Please merge these 2 subtitles.", "label": [[75, 84, "OpenStack"], [2125, 2134, "OpenStack"], [2657, 2660, "VNFs"], [2769, 2778, "OpenStack"], [2861, 2870, "OpenStack"], [4425, 4434, "OpenStack"], [4031, 4036, "Telco Cloud"], [4252, 4261, "OpenStack"], [2522, 2525, "VNFs"]]}
{"id": 70, "data": " So, let's take a little deeper dive into some of the performance options inside the OpenStack type of environment, specifically inside the workloads that I've talked a little bit of. We've used the word private cloud and public cloud and hybrid cloud. And oftentimes we associate those with what we might consider more of an IT type of an environment. And then, similarly, we've also used some terminology that implies interest inside the communications or the telecommunications type of an environment, the UCP or the EPC environments. It's important for us to understand that while we can apply the same concepts and the same management aspects, that there are some fundamental differences that take place inside that NFV environment for a cloud, inside that telco world, versus those that you might actually find and what we might classify now as the more traditional or the IT cloud environment. And specifically, in that IT world, those applications themselves typically require a lot of computational resources or CPU resources, and CPU efficiency is very important to that environment. And when we look at the environment that takes place in that NFV cloud environment, it's really more of an East to West traffic that requires a high I/O capability and memory access and the resources in order to move that traffic efficiently. While we're providing some touch, we're providing some compute capability on that traffic, but it's actually the traffic flow itself that drives that environment. So fundamentally, we see that there's a difference there. And because of that, we may fundamentally look at our architecture in those servers themselves as being different. In one, we may have to have a balanced I/O, and in the other we may choose to have an asymmetrical type of an I/O where we've got compute resources that are heavier on one side of the configuration of that hardware then will be on the others. And through our orchestration environment, we may choose to instantiate our workloads themselves depending on the nature of that workload and target those type of environment specifically for that. Similarly, one of the differences that we've got inside the cloud environment versus the NFV comms environment is that the focus there really in that IT environment tends to be what we call application centric. And in that sense, the applications themselves may come in and out of scope very rapidly. Whereas in a network centric type of an application, even though we may have multi-tenancy in there, the workloads themselves maybe much longer lived, and in some cases can be very long lived. We're talking weeks or months or even greater periods of time where we wouldn't expect necessarily that same model to apply. And because of that, the resources and how we utilize and manage those resources in these two types of environments are driven somewhat by those workloads themselves. And then, typically, we've also got some concerns about the workloads themselves. We think of the IT type of environment as being an endpoint and more of the NFV cloud as being a middle of the road of the traffic. Sometimes we talk about cul-de-sacs and superhighways, where the IT workloads, the packet will arrive into a termination or an endpoint, and it will spend some time there. That's as a compute resources that take place at that endpoint before that packet returns. And it may egress in the same direction from which it came. Whereas if we look at the telecom workloads, the East to West traffic that we talked about is more of that superhighway, where packets travel symmetrically, more in one direction than they will in another in type that environment. And that egress direction is not necessarily the same as the indirect ingress direction. And because of that, we've got functional requirements of those workloads themselves that may drive us to do things differently. One of the things that Intel has introduced is-- before we mentioned the capabilities inside the silicon that we drive in order to improve the performance in this virtualization-- is hyper-threading. And hyper-threading is a technology that allows us to have a single core inside our silicon, inside our CPU, that is capable of providing the services of two cores, so that if one core is blocked for a memory I access or an I/O type of an application, that core itself, while the memory management is taking place in the background, can be used to perform computational resources on another task that are available inside that type of an environment. And in addition to that, There are capabilities that allow for those core resources to be optimized into that type of an environment. So, for example, if you look at some of the interesting workloads, hypothetically, we've got without threading, these workloads are the blue blocks, the light blue. The dark blue would represent an application that's running. So, as you see, the application running, it's going to take a little bit of latency when it's performing I/O or memory access caching. Then, the second core running that application has to wait till that first core is completed. But with hyper-threading, you can see that those two blocks, the light blue and the dark blue, can run simultaneously so that while the dark blue is in a wait state because of this I/O or this memory application, then the light blue can run. And then, when the dark blue is read to run, it can take over again. So, this total throughput then of those workloads, whether that's a computational workload or a throughput workload, can result in a fewer cycle time or more rapid completion. And ultimately, this allows us to access our resources more effectively. So, let's take a look then at some of the things that we can also do from-- I've mentioned multi-tenancy before. That implies segregation for performance standpoints. And there are a couple of reasons that you would do this. Some of them would be business reasons. Some of them may be driven by regulatory issues that come into place there. And they're all fine reasons for having that. And virtualization is an area that comes into play for that. So, the workloads themselves can be virtualized into that type of an environment. So, OpenStack providing that compute node segregation. If we've got geographic regions-- and again, this may be a regulatory issue-- allows us to take those availability zones from that concept and have multiple instantiations of those critical resources. The Nova resources for the compute, for example, allow us to instantiate those into a physical region in that availability zone based on the host. And then we can do similarly in another region. We can instantiate that into those geographical regions. So, from a single management perspective, we can have different host aggregations and apply those regulatory constraints or the business constraints into that type of an environment, or whether those are governmental restraints-- constraints based on some type of geographic-- Also, you can think of data sovereignty or personal information types of sovereignty into these types of environments. NOTICES AND DISCLAIMERS", "label": [[85, 94, "OpenStack"], [509, 512, "uCPE"], [520, 523, "EPC"], [721, 724, "NFV"], [762, 767, "Telco Cloud"], [1155, 1158, "NFV"], [2203, 2206, "NFV"], [3058, 3061, "NFV"], [6220, 6229, "OpenStack"], [6170, 6181, "Virtualization"], [6077, 6091, "Virtualization"], [4049, 4063, "Virtualization"]]}
{"id": 71, "data": " So, we've been talking about OpenStack and some of the capabilities in it, and now we're going to work our way up from the Vim layer a little bit higher into the management orchestration and look at a model at least that maps into the ETSI model that we'd find more in the telecommunications type of environment. And in particular, let's look at a network service onboarding process from that standpoint. And the example that we've got here is a virtualized customer premises environment. For example, instantiating a creation of a home DVR. Now, you might want to think about this home DVR itself as residing in the cloud as opposed to physically residing at the client's premises itself, into that type of an environment. So, the surfaces, then, that are in there need to be described in that upper layer of that management so that they can communicate to the Vim layer down below for that instantiation that takes place. And in the ETSI model, this is done through what we call a network services descriptor, or an NSD. And one of the components of that NSD is inside that VNF package, which includes the elements that would tell us about the capabilities of that software itself in addition to the images that are associated with that software instantiation itself. Because remember, that appliance is not functional yet at this point in time, so there has to be a repository there's out there. And in fact, where that repository is even ETSI is silent on how you manage that. But there's this interaction that needs to take place. Typically, that interaction, then, starts with the OSS/BSS. So a customer has placed an order for service. And through that ordering and that onboarding service in that billing system, these systems then trigger the management and orchestration layer for that particular home DVR capability, then. So, there's going to be some descriptors that exist inside there. So those onboarding descriptors, then, are communicated down to the network function orchestration layer inside that upper portion of that MANO layer, and then that NFVO has access to the particular software modules inside the repository that we spoke of. So, they can pull the correct instance of that application, depending on what that customer has ordered and begin the process of building up, if you will, that application through this MANO layer and then out down to the Vim and then across to the servers that exist in that type of a cloud environment. And part of that is going to be the elements, then, that are necessary to create the networking interfaces that plumb those IP addresses, if you will, from the virtual machine to that physical machine that exists below, and then communicate that back pressure back up into that environment, to the OSS and BSS environment to what is completed. That type of an instantiation. So, let's take a look at that NSD just a little bit more, and that network descriptor is validated during that onboarding process. That capabilities of those systems to the NFVOs are going to be met from the criteria that is found in that NSD. So, there's some validation to check and integrity to take place in that environment as we instantiate that. So those processes, then, are onboarded, and brought to life, and created a capability for that service that exists there. If we double click a little bit further on those processes that take place and that instantiation, again, it all starts with an order being placed from a consumer, potentially from a customer. And then the OSS/BSS then triggers that instantiation in that NFVO, and then the first step of that NFVO is to determine whether or not we need to create a VNF. If that VNF already exists, then we may need to change the software that's running on it, or we may need to actually create the interesting application of the software that's running on top of that. So, the VNF being instantiated communicates that backup to a VNF manager. The EMS element may be communicated as part of that software, that configuration. The resources would become allocated into that environment. We move on, then, into the plumbing activities that are necessary in order to tie that together. And then finally, the service has reached its maturity point and it will indicate some acknowledgment back up into that management system that communicates its way back into the OSS/BSS system. And the reason that's critical is that this typically would trigger a billing activity that says, OK, not only have you requested it, but we've verified that we were able to instantiate that service. We've got the plumbing, we've got green light, if you will, for that service, and now the customer is able to start accessing it. And that would trigger the onset, then, of a relationship that would allow billing activity in that coms service provider to take place. So, in summary, those network service descriptors that we find inside the ETSI model are necessary for that onboarding, and they contain all the dependencies that allow us to instantiate that specific service on top of that type of an environment, then, to create the validation. It provides us with the capability of providing that end to end networking service instantiation so the plumbing is correct. And then that traffic will flow. So, it's not just that we've got warm applications just sitting there, but that the traffic is, in fact, flowing based on the information in that NSD. And that we may have done that through technologies, for example, from an SDN standpoint, allow us to have that programmatically described. So, if you're thinking about VLANs or  QN2  types of things, NOTICES AND DISCLAIMERS", "label": [[30, 39, "OpenStack"], [1077, 1080, "VNFs"], [2040, 2044, "MANO"], [2342, 2346, "MANO"], [3661, 3664, "VNFs"], [3674, 3677, "VNFs"], [3873, 3876, "VNFs"], [3926, 3929, "VNFs"], [5502, 5505, "SDN"]]}
{"id": 72, "data": " So, in our conversations about virtualization, we've fundamentally been talking about virtual machines, and in type one or type two, and some of the characteristics around that. There's another option for virtualization. And it's in containers. So, we're going to spend a little bit of time now talking about containers. And the container revolution, if you will, has fundamentally come about within the last decade or so. And it's because of the drastic change in the way we use our devices fundamentally is that the revolution is from very large scale applications that could reside on purpose built or dedicated appliances, to a much broader ecosystem of applications that get spun up and spun down very quickly as we use them. So, whether that's a Netflix type of viewing, or whether that's checking some type of social media, it's checking Gmail, or a variety of other things. And virtual machines themselves, while they provide a lot of advantage, they're heavyweight. They use a lot of resources. They take a lot of time to spin up, sometimes in terms of minutes, or many minutes. And in containers, there's a transformation that's taken place that allows us to instantiate those workloads much more quickly, and have them more dedicated to the specific user that is accessing them in real time or near real time. And because of that, there has actually been a paradigm shift that's taken place inside large scale data centers in the way we provide those services. So, whether it's an email application, or whether it's some type of back office application accessing some corporate database, we found that containers actually allow us to maximize the utilization of those compute resources much more quickly. And there are some dependencies on there. Of course, there's no free lunch in this environment. There are some trade offs that take place. And we're going to talk about some of that as we look at both containers, and contrast them a little bit to the virtual machine environment. So traditional enterprises, as I said, would have had dedicated servers to the applications themselves, whether it's a back office HR type of system, or whether it's an email system, or whatever it happens to be. But what we really want to do is we want to disaggregate those applications from the hardware, as we've spoken before, and then have those applications meet the demands of the users in real time, and maximize our compute resources. So, inside the cloud environments we see that it has evolved. And containers are the term that we use now to talk about how that software has been created, instantiated, and operates inside those data centers. So, there's now less reliance on a one to one mapping between an application and the hardware appliance that it may be running on itself. But rather that that software application can be spun up on demand on a variety of host applications in that standard, high volume server environment that we see in there. What's that do for us? Well, this gives us greater scale. It gives us greater control of our environment. And also it gives us the ability to migrate hardware forward at the pace of the hardware transformations, and to migrate software forward at the pace of software transformation, and not have those bound together as we did in the past. So, our motivation behind this transformation is speed. It's really to create these applications quickly, to spin them up, to spin them down, to install and cycle them, and to have those on demand taking place in milliseconds, and not weeks or months in some types of cases where we would have had purpose built appliances in the past. In theory, there's a reduced complexity. We've driven the complexity from the system itself into some complexity that takes place in an environment that we're going to talk about in DevOps later on. But we've changed that paradigm of where that complexity resides. There are some security points that come up when you start looking at containers that we'll talk about as well. And, again, as I said before, there's really no free lunch here. There are going to be trade offs. And we'll talk about that. So, in summary, in this environment we see that server applications in the new world are really smaller units of functionality. And they scale both in breadth and depth. And that is as users come online, we spin up more resources. We consume more power to do that. And as the load is shredded, then we can spin those down and conserve power in that environment, or reallocate those compute resources to other workloads as they come online. And containers are the underlying technology that we talk about that allow us to do that as those server apps are transformed into this environment. So, the example, as I mentioned before, more customers used to spin up more containers, more instantiations of it. And you can do that across your data centers pretty easily. Inside the coms space, there's a difference that we'll talk about as well, in that many of these examples really are the end point examples that we spoke of, so are sort of the back office of the IT or the over top examples.[O1] And inside the comms network we find that, even if we look at the container capabilities, maybe we're not going to spin these things up, but it's short lived. The containers themselves, we can take full advantage of those resources, but they're probably longer lived than we'd find in a back office or an endpoint data center. And similarly, new applications, whether it's in the comms or in the data center, they can be brought into scope more quickly without having to rack and stack new equipment in order to provide the functionality of that appliance.", "label": [[32, 46, "Virtualization"], [206, 220, "Virtualization"]]}
{"id": 74, "data": " So, while we've mentioned Docker and Rocket as two of the container options, there's another one that we want to mention very briefly. And this one tightens up some of the security, not all of it, but some of the security that we mentioned before. And that's Intel's Clear Containers. So, Intel's Clear Containers uses are Virtualization Technology, or Intel VT, instead of the Namespace for isolation between the containers. So, this tightens up some of the concerns that can exist at that level. And it provides an extra level of routed hardware security between the layers and the containers themselves. So that's down at that ring zero level that we spoke about. And it does support the advantages of a container model. So, if you've got that compromise, where virtual machines are too heavy, and containers, you're concerned about the security elements of it, the ease of development and deployment to the manager inside Intel Clear Containers NOTICES AND DISCLAIMERS", "label": [[304, 314, "Containers"], [324, 338, "Virtualization"], [415, 425, "Containers"], [585, 595, "Containers"], [708, 717, "Containers"], [802, 812, "Containers"], [939, 949, "Containers"], [274, 284, "Containers"], [59, 68, "Containers"]]}
{"id": 75, "data": " So, let's touch, again, on some of the container benefits. Packaging is a big one. And that is the effort that's necessary in order to assemble the elements that become that containerized-- the instantiate of an application. So, the bundle dependencies and the configurations of that application. So, instead of pushing an application to the host and then configuring it, the containers allow us to contain some of the information of that configuration as part of that packaging. So, there's less need to have dependencies on the installation of the application for that. The RPM is a very common spec that allows us to deploy these instances. And inside the Ubuntu environment, we'd find .deb packages for containers. Fedora, obviously, would be the RPM type of environment. So, we can use the familiar sourcing or gets that we have in order to pull those types of containers into that environment for an experimental phase of our own-- of our packaging. Some of the virtualization benefits that I've-- I've sort of touched on those-- is that containers spin up very quickly. They come into scope in about the same time as a collection of processes, so you want to think about milliseconds that's in there. So, as they run on that environment-- if they're very lightweight applications in that container space, they may only consume a thread or two of that process. And you can have hundreds or thousands of these running simultaneously on large scale systems. The building blocks of containers, then, allow us to dig down deeper into an area that we consider microservices, where we start stitching, possibly, multiple container applications together in a microservice environment to create new or innovative services into that-- into that environment. We've used the term DevOps a couple of times. The theory behind containers is that if you own the source code of those types of applications, then your development team, in fact, can deploy containers into the production environment very quickly. They can test things out. They can-- in the live network. And they can determine whether or not the functionality is suitable to be upgraded. So, you can see actual applications upgraded multiple times a day in some environments, as opposed to maybe two or three times a year, as you'd see in a traditional IT or a traditional comms environment. Versioning of containers allows for that easy rollback of the functionality, in addition to the fact that these containers themselves, in that microservice environment-- as I said-- for that DevOps. If you don't like that container, you can simply roll back to the previous release. And then your infrastructure will instantiate the previous functionality into that environment. And the advantage of that environment, then, is that that quick setup and tear down, and the fact that we can associate it with users. So, if we've got the ability to control our load balance, even in our production environment, we can spin those up into a production environment. Have friendly users or known users run through that entire exercise of testing out that container, and then determine whether those are suitable for live traffic, and then spin that environment down. So, these are all some of the advantages that would drive us into that container environment. The key point here, though, is you've got to access the code. You've got to be able to get to the source code itself. And if you've got an existing application, simply thinking about driving it into a container environment may not be as easy as we would like. That may be more applicable to what we've seen historically in the virtual machine environment, where we can deploy those applications on top of a hypervisor. But with a container, we're really talking about the development of new code and the progress of creating that environment that allows us to drive towards more of a cloud-native environment where we talk about microservices. NOTICES AND DISCLAIMERS", "label": [[40, 49, "Containers"], [175, 188, "Containers"], [377, 387, "Containers"], [708, 718, "Containers"], [867, 877, "Containers"], [969, 983, "Virtualization"], [1045, 1055, "Containers"], [1296, 1305, "Containers"], [1486, 1496, "Containers"], [1622, 1631, "Containers"], [1820, 1830, "Containers"], [1946, 1956, "Containers"], [2363, 2373, "Containers"], [2461, 2471, "Containers"], [2571, 2580, "Containers"], [3097, 3106, "Containers"], [3280, 3289, "Containers"], [3504, 3513, "Containers"], [3733, 3742, "Containers"]]}
{"id": 76, "data": " So, let's spend a little more time talking about containers, in particular containers as they apply to the Linux environment. Before we dig too deep into that, though, we need to contrast the advantages and disadvantages of containers versus virtual machines. As I implied earlier, virtual machines are heavier weight. The packaging of themselves, if they don't contain the guest operating system, they at least need to contain all of the libraries associated with that application, that application itself. And the operational context of it, we've got the hypervisor that's sitting on top if it's a type 2, on top of a host operating system as well.[O2] So, the burden on the system itself is going to be greater. Virtual machines require a lot more memory, they're going to require more disk space, they're going to require more compute resources. In containers themselves, the container of the application, when it's packaged up, is going to be the executable code itself. And then, if necessary, and only if necessary, any additional libraries that would be specific to that application. And in the runtime environment, the containers, being lighter weight to begin with, lesser memory footprint, which means I can run more of them under my machine environment. And the library issue is one that, if the libraries are shared, if we've got DOLs that are in there and we've got multiple instantiations of the same application in different containers, they can actually use the same code space of those shared DOLs. That, additionally, reduces the amount of memory that's necessary, but it also reduces the complexity and the overhead of running that environment so we get better performance from that system. So, containers themselves, again, lighter weight, higher speed, higher performance possibly than those virtual machines. And that's one of those large advantages. Specifically, some of the other trade-offs that are in there are shown in this table. And when we look at containers versus virtual machines, the virtualization takes place in containers at the operating system itself. So, you want to think of containers as being processes on-- collection of processes on steroids where we've got that C group and namespace separation.[O3] But they run a lot like processes, very close to the operating system and to the metal itself. Under virtual machines we've got-- server hardware that has a virtualization layer that's implemented there. The abstraction takes place at the operating system or the operating system into the hardware onto virtual machines. The guest environments-- inside containers one of the constraints is then, though, that all those applications that we're going to be running on it have to be specific to the host operating system themselves. Under virtual machines, it's conceivable that we have multiple guest operating systems running on a host operating system. So, the applications themselves can be dependent on various host o-r-- guest operating systems. So t'at's sort of an advantage that we would find under the virtual machine environment is that the applications themselves 'on't have to be directly associated with the host operating system. The density we implied earlier that we can get a lot higher density in containers. If these are very lightweight processes, hundreds or thousands of containers spun up in a back office or an OT type-f-- over the top type of data center into that environment. Where virtual machines, we typically think of those as being in the tens or twenties in some environme-s-- inside to those environments. So significantly different order of scale t'at's in there. And again, the instantiation time in containers is very quick, and virtual machines may take minutes or several minutes in some cases. And depending on that environment that'we're operating in, that could be a critical decision factor as to whether'we're going with containers or virtual machines. In network communication service provider spaces, t'at's not just critical because, as'we've mentioned before, their workloads have a tendency of being long term stationaries, that t'ey'll be spun up and t'ey'll live for many days or weeks or even months at a time. Whereas in an OTT type of an environment, a contai'er's advantageous because t'ey're associated just with a transactional user utilization and they may only live for a few seconds or a few minutes or in some worst case maybe a few hours if it's associated with some sort of streaming video. So, to review that again then, some of the concerns that we'd have with containers are the security. The isolation is not as good as it had been under virtual machines. So that's one of the trade-offs. If we need very tight security, very high bit of separation for multi-tenancy or business reasons, then virtual machines would have an advantage over containers. The networking itself. Networking is a critical element in any of this virtualization. And networking in containers is still evolving, it's still developing, particularly in the communication space where we might have multiple ports that are necessary-- multiple physical ports that are necessary because of the East-West nature of the traffic as opposed to the ingress egress traffic that we might find in a back office or an IT application or a cloud type of an environment that's an endpoint that simplex interface is sufficient to it. The maturity level. Containers are still developing from an application standpoint and from an infrastructure standpoint. In certain market spaces and certainly that's the case in comms as we're just now starting to see containers emerging into that space. It's probably going to be a few years before widespread adoption. And then we've got-- we've got compatibility issues, as we said before, is that the container application itself is going to be tightly associated with that host operating system. So, the evolution of the lifecycle management of those host operating systems implies lifecycle management to the application of the container itself. So, when we compare containers of virtual machines, again, our choice doesn't have to be necessarily mutually exclusive. There are examples where we have containerized applications running as the application space inside virtual machines. So that's one of the options we're seeing today as a migration point along the way. The development of those container applications is going to be simplified, but we've got some constraints that are in there and one of those happens to be about both security and the networking that's in there. As we spoke before, containers can be quickly created and destroyed, virtual machines have a tendency of living longer into that type of environment. And we're seeing functionality getting pushed into the OpenStack community as we speak that is going to support containers. So, it's still-- it's still in its infancy, but it's evolving very rapidly. The resource management inside containers is a little more mature in the sense that we've got greater granularity of control of the groups and the namespaces, and containers themselves give us some advantages in the way we build and deploy NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles.", "label": [[50, 60, "Containers"], [76, 86, "Containers"], [225, 235, "Containers"], [1716, 1726, "Containers"], [1981, 1991, "Containers"], [2021, 2035, "Virtualization"], [2051, 2061, "Containers"], [2119, 2129, "Containers"], [2406, 2420, "Virtualization"], [3955, 3985, "Communications Service Providers"], [4936, 4950, "Virtualization"], [5424, 5434, "Containers"], [5810, 5820, "Containers"], [6040, 6049, "Containers"], [6078, 6088, "Containers"], [6406, 6415, "Containers"], [6612, 6622, "Containers"], [6797, 6806, "OpenStack"], [6854, 6864, "Containers"], [6973, 6983, "Containers"], [7105, 7115, "Containers"], [854, 865, "Containers"], [1129, 1139, "Containers"], [1442, 1452, "Containers"], [2602, 2612, "Containers"], [3262, 3272, "Containers"], [3340, 3350, "Containers"], [3683, 3693, "Containers"], [3912, 3922, "Containers"], [4573, 4583, "Containers"], [4853, 4863, "Containers"], [4970, 4980, "Containers"], [5624, 5634, "Containers"], [6212, 6225, "Containers"]]}
{"id": 77, "data": " So, let's talk about some of the core elements of containers in a little bit more detail and give a very quick example of one of the capabilities inside the containers. So inside containers, one of the key elements is that container engine. It is part of the kernel itself that you'd find in the host operating system. So many mature Linux operating systems today have a capability for supporting containers. And inside that then, we've talked about the name spaces, the control groups, and the file systems.[O2] And we're going to scratch those in a little bit more detail. So, the namespace, just as a quick reminder, this is where we find things like the process ID, the IPC there, process communication mechanisms, as well as some of the file system management for mount points, to access additional mount points that might exist in that system. And this allows us to create that isolation at that namespace level between different container instances that might be running on that environment. Inside the control group, this is more about the resource allocation and managing those resources that are on that compute network and storage system that is the standard high-volume server. So, from a CPU standpoint, we can allocate and manage the cores-- how many cores, whether those cores are going to be pinned and shared in NUMA alignment capabilities, as well as allocate the amount of memory that the container should be able to access into that environment. And then, over on the advanced file system-- so, we've got fast route file systems that allow us to interface with the normal things which we think of from an I/O standpoint that doesn't reach into the network environment inside that. So, the namespace-- inside the Dockers environment, for example. The PID-- the Process ID. So, if you're familiar with Linux itself and have used it, then you understand that when an application is in scope, it's going to have a very specific process ID, or sometimes we call that the PID-- so the ps minus ef grep-- and then you can find the PID of your application. And that process itself may in fact be multi-threaded, but the application may be a collection of multiple processes. So, it's those processes in those PIDs that need to exist inside that namespace that are unique into that container environment. From a network standpoint, we want those network interfaces, again, to be isolated and separated. And this is an area that we're going to talk about more when we dig down into some of the specifics of containers because there are some nuances that are still being developed inside containers around networking as it applies to the communication service provider environment.[O4] We also mentioned the Inter-Process Communication. So those processes, those PIDs that we've got that form an application set, if you will, may need to communicate to each other. And in a native Unix or Linux environment, this concept of IPCs or Inter-Process Communication allows them to send signals and data structures back and forth through the kernel itself. Inside containers, those are maintained inside that instance of the container space and don't have scope outside that particular namespace that exists in there. And that's a critical element for what I call the heavyweight or the interesting applications you'll find in the comms space where these container applications themselves are not in the microservice yet, but rather they have a larger context. From a file system point, the namespace allows for multiple mounts to be unique across-- or independent of those other containers that may exist there. So, if you've got a file system that needs to be mounted by a specific name known to the operating system, those are actually unique instantiation because of the namespace separation inside the containers. Inside the control groups are the C groups. Again, we've talked about the CPU utilization that's very critical so that we can maximize that critical resource on these compute resources and not oversubscribed them. So that's one of the things that we manage in that environment. Memory. So, we can set thresholds and allocate the actual amount of virtual memory, which ultimately translates into real memory. And it has implications for the caching associated with that inside these control groups. Pinning and reservations, whether or not these cores can be allocated in a hyper-threaded type of an environment, if we've got the right workload to support that, or whether they need to be dedicated. And then, similarly, we've got networking trafficking options that we can manage from inside that control group. So, with that, let's take a step back for the moment and take a look at, for example, block storage example inside Dockers. So, in this area, we've got a hypothetical application that is going to do read-write into a storage type of an environment. And we've got a common build that allows us to access that storage environment and forms as those building blocks as you start to think about one of the simplest applications into that environment. So, this example shows us a particular instantiation on Ubuntu 15.04 using containers and multiple instantiations, where we've got separate images of the read-write operation into that basic storage environment. One of the other applications that comes along from a Docker includes the capabilities of a MariaDB. So, this is one of the popular databases that we find in the container environment, and it was created and is maintained by the original developers of MySQL and is an open source environment. And we've got a simple sample here of how, if you've got a Linux environment and wanted to pull the MariaDB into a Dockers file using a GitHub script, you simply reproduce this environment and then you're create an instantiation of a MariaDB inside the Dockers environment under and an Ubuntu environment if you're so interested to do that. So, this should be an easy cut and paste created as a script itself, and then source that script in. And this should, if you've got connection to the real network, the internet out there will pull in that MariaDB and you can begin experimenting with some of the capabilities underside of Dockers environment. And one of the fun things to do there would be to actually create two instantiations of that in different namespaces spaces of the container and see that you can create two different databases with the exact same name. And a select on both databases, and you'll see that they're actually separate instances of those databases. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[51, 61, "Containers"], [180, 190, "Containers"], [224, 233, "Containers"], [398, 408, "Containers"], [937, 946, "Containers"], [1409, 1418, "Containers"], [2294, 2303, "Containers"], [2518, 2528, "Containers"], [2598, 2608, "Containers"], [2648, 2678, "Cloud Service Providers"], [3067, 3077, "Containers"], [3128, 3137, "Containers"], [3583, 3593, "Containers"], [3810, 3820, "Containers"], [5156, 5166, "Containers"], [5455, 5464, "Containers"], [6367, 6376, "Containers"], [158, 168, "Containers"]]}
{"id": 78, "data": " So, there are a number of options available to us we start talking about containers. And one of them is Linux containers. So, if we just do a quick Wikipedia search on Linux containers, we're going to find that they describe for us that it's an operating system level virtualization for running multiple Linux containers under the control of the host, under that kernel itself. Now this is an area where you might want to be cautious. While it's lightweight, it may not have the full support of some of the other types of containers that are out in the commercial space. So, we've mentioned Docker, and Rocket, and in Intel containers before. So, you might want to take that into consideration when you start looking at this. But containers themselves, there's tremendous amount advantage in the utilization of containers, both in the deployment and the operational context, and in maximizing those resources in a virtualized space. So, let's not get too hung up about which container we're looking at or using, but rather keep track of the fact that it's a high-level concept that leads us to that DevOps environment and into an environment where we've got micro services that give us more of a cloud native NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[74, 84, "Containers"], [111, 121, "Containers"], [175, 185, "Containers"], [269, 283, "Virtualization"], [731, 741, "Containers"], [915, 927, "Virtualization"], [976, 985, "Containers"], [523, 533, "Containers"], [625, 635, "Containers"], [311, 321, "Containers"], [812, 822, "Containers"]]}
{"id": 79, "data": " So, let's spend a little more time talking about the cloud environment for Containers and the ecosystem that's in there. So pictorially, let's keep in mind that we've got a large collection of servers, potentially, that are out there that are supported by the host operating system, and a Container Engine that, either a commercial Container engine or standard Linux Container engine, that is part of that. And then we've instantiated interesting applications on top of that. And in that cloud environment, what we really are concerned with is managing it, is spinning those Containers up, spinning them down, and managing those types of resources. So, there's some type of orchestration that needs to take place that is outside of that host environment.[O2] So, if you remember the modules that we were talking about under virtualized functions, we had this VIM layer. Containers need a similar capability to manage that virtual infrastructure layer and then and then allow for the creation of those containers on that interesting-- on that workload itself that's in the network. So, we're going to dig down into that in just a little bit more detail.[O3] So, the Container engines themselves then supported on top of that host operating system gives us that ability to create those images, whether that's from a Docker or a Rocket, or if it's a native Linux, the LXC Container, or even Intel Clear Containers from that type of an environment. That's at the runtime level. And we want to be able to manage and log those facilities in an orchestrated layer that is outside or external to that environment. And that's a critical element when we start talking about the scale that needs to take place inside the Container. So, let's look at what happens and what the responsibilities are, then, in that orchestration layer that's in there. So obviously, it's responsible for managing the resources across multiple hosts. And when I say multiple, you can think of in thousands or tens of thousands in some very large types of data centers that are currently running in a Container type environment. And they need to configure the network and the storage and manage those resource allocations so that we maximize the utilization of the platform itself so that we don't end up with stranded resources. If you've got a large number of cores that are out there on a particular host, you may want to consume all of those resources before you start moving onto another host inside your environment and begin spinning up those types of applications. So, another critical portion, then, is that with Containers-- and we start looking at them if it's a lightweight process-- we do want to think about things about reliability. And there may be some use cases where we don't want to spin all of our Containers up on one host first and the second or the third host, but rather we may want to stagger those in some sort of round robin or distributed fashion to ensure that we have an ability to get feedback as to whether those hosts in fact remain healthy during that time. And there are some strategies that might be applied operational in that case in addition to the functionalities of scaling and monitoring. The previous one that I was mentioning a little bit is that-- is the host healthy? Is you've got some type of Container instantiation that is rolling through all the hosts that are in that type of an environment, it may in fact speed up your fault detection if you've got a faulty host that's out there. And then you can schedule that host to be either taken out of service or scheduled for some type of a maintenance. And all that needs to take place through some type of orchestration and schedule environment that is outside the actual Container operation itself. So, what are some of those orchestration projects that exist out there? Some of the very common ones that we find in Docker itself. There's one called Swarm. And this is very tightly coupled with the Docker Container operation and is very popular in a variety of data centers. Kubernetes is one that is developing very rapidly. And you find that Kubernetes is being integrated into the OpenStack features as we speak of today. So, inside some of the comms workloads that would be OpenStack dependent and begin transitioning from virtual machines into containers, Kubernetes entities would be an orchestration layer that would take the place of that functionality that we found inside the VIM over in the OpenStack, or more precisely is becoming integrated into that VIM layer in the OpenStack type of model. And Google and Red Hat are key players into that area. A couple of the other ones you see listed here on the slide and their associated operations and where they come into play. So, the orchestration-- again, when you start talking about very large-scale data centers and managing these containers, it's not just that we get the workloads themselves and that microservice concept, but that we've got to be able to manage and operate that. And that pushes requirements back onto the orchestration, and that implies functionality like Docker Swarm or Kubernetes or Mesos JuJu in each of these environments. So, the orchestration is a critical element to pay attention to if you're considering heading down the path of a Container type deployment. [O1]Please delete. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles.", "label": [[76, 86, "Containers"], [290, 299, "Containers"], [333, 342, "Containers"], [368, 377, "Containers"], [576, 586, "Containers"], [825, 836, "Virtualization"], [1401, 1411, "Containers"], [1711, 1720, "Containers"], [2069, 2079, "Containers"], [2590, 2600, "Containers"], [2787, 2797, "Containers"], [3310, 3319, "Containers"], [3739, 3748, "Containers"], [3974, 3983, "Containers"], [4044, 4054, "Kubernetes"], [4113, 4123, "Kubernetes"], [4153, 4162, "OpenStack"], [4247, 4257, "OpenStack"], [4330, 4340, "Kubernetes"], [4318, 4328, "Containers"], [4471, 4480, "OpenStack"], [4550, 4559, "OpenStack"], [4862, 4872, "Containers"], [5124, 5134, "Kubernetes"], [5293, 5302, "Containers"], [871, 881, "Containers"], [1166, 1175, "Containers"], [1002, 1012, "Containers"]]}
{"id": 80, "data": " So, let's talk a little bit about the public cloud, and containers, and the infrastructure that they provide, and then some of the services that are behind that. So, a lot of what we're doing and learning in containers, inside a variety of enterprises or in the Coms space, is a result of what we learned inside the cloud, the large cloud providers themselves. And each of them provides a capability for deploying containerized applications into their cloud environments.[O2] So, Amazon, in their ACS environment, for example, allows you to deploy Docker's containers that allow you to then to run your applications as an element in a managed cluster, under those EC2 type of instances in their environment. Microsoft under there Azure offering also has a container service, ACS. And this lets you create clusters of virtual machines that act as hosts, along with the ability to have master machines that are used to manage and control that environment. So, you want to think about the orchestration layers. You can have independent management of that orchestration as well into that environment. And Google also is a player in this space with their Container Engine.[O3] And this also allows you to build and deploy through Kubernetes from an orchestration standpoint and run Docker containers into that Google Cloud environment and then schedule and manage those elements based on user defined requirements. So, this is an example where an enterprise itself may be looking at moving some of their internal IT operations into a public cloud environment, where that enterprise has a DevOps capability to manage, develop, and deploy that operation, but they don't want to be responsible for the network compute and storage applications, and choose to use the resources inside a public cloud environment to that. And these are just three examples of that very large enterprise supported cloud offerings that are out there. And there are others as well. So, let's transition a little bit and talk about some of the terminology that we find inside Kubernetes from that cloud standpoint. In these environments we actually have potentially a large number of servers and a large number of applications we're maintaining. And one way of doing that is by using a hierarchy. So inside Kubernetes our hierarchy allows us to talk about clusters themselves. And then clusters are those compute resources that run those containerized applications, so a collection of services. And then pods are a homogeneous collection of clusters. So, it's multiple clusters that then allow us to share configuration data and apply constraints to the deployed configurations on those clusters. So, you may want to think about the replication of that environment, inside a pod environment. And this allows us then to have an ability to manage the lifecycle of those pods, to ensure that we've got the right number of pods running at the time that's necessary. And if you think about what I've talked about, about how we can have containers in a DevOps type of an environment, you would typically want to think about constraining your pods into similar instantiations so that you're not mixing development environments of multiple releases across those types of pods. One of the critical elements, when we start looking at the scalability, we've talked about the fact that you may be able to spin these up on demand as your users come online and go offline, is the fact that load balancing is a critical element that comes into place here. And this type of abstraction is necessary in order to route traffic to those pods where the services are in there. And this is one of the development type of an environments themselves, the applications in containers are often stateless. And that state information either exists at the client itself or out towards the edge of the network until the load balancers. So that as we begin to scale that, we don't have to maintain that state information in the containers. And that's one of those concepts that we talk about a little bit, in a cloud native type of an environment, where the state information associated with that is at the edge as opposed to deep inside the network. And that allows us to have these services that scale and are more dynamic in that environment. And similarly, we've got a concept called labels inside Kubernetes. And these give us the identifiers that help us to identify those homogeneous pods as we're looking at performing specific tasks in that environment. Docker as a transition, has a Swarm architecture-- we talked a little bit about that-- as its instantiation or orchestration process here. This diagram gives us a little idea of where we would find those functionalities inside a Docker's environment. And the servers themselves at the bottom are the workload applications that are in there. And then the Swarm manager is that orchestration that's worrying about the scheduling and the discovery of those services so that it allocates those functionalities into that Docker type of an environment. So, some of the operating systems that we'd find traditionally in play here would be CentOS and instantiations, derivatives of the Red Hat then, or Ubuntu from the canonical offerings. And in the container environment, CoreOS, Rancher, ad Ubuntu, and Project Atomic are all some of the emerging areas that we see coming about inside that container environment. And then finally, we've got-- where do we get this information if we're interested in experimenting or acting as a development environment that's starting to get in here? Github is a tremendous resource that's available for Dockerfile, Scripts, you can get source code that's available in there. A tremendous amount of information available in that Github repository, whether that's from the Docker type of an offering or some of the other competitive container offerings. I would encourage you to have a quick look and see what's available out there in those Github resources.[O4] NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[57, 67, "Containers"], [209, 219, "Containers"], [241, 252, "Enterprise"], [1151, 1160, "Containers"], [1226, 1236, "Kubernetes"], [1285, 1295, "Containers"], [2045, 2055, "Kubernetes"], [2276, 2286, "Kubernetes"], [2406, 2420, "Containers"], [3000, 3010, "Containers"], [3716, 3726, "Containers"], [3966, 3976, "Containers"], [4340, 4350, "Kubernetes"], [5244, 5253, "Containers"], [5386, 5395, "Containers"], [5861, 5870, "Containers"], [415, 429, "Containers"]]}
{"id": 81, "data": " [O1] So, some of the concepts that we talk about inside this disaggregated and cloud native environment used the terms DevOps occasionally for Developments and Operations, and also, microservices. One talks about how we build and manage and operate the applications[O2]. The other one talks actually about how those applications are designed and how that software is constructed in that environment. So, let's scratch a little bit more at DevOps. And here we've got to a bit of a definition that you can read through about what we mean when we talk about Development and Operations. But fundamentally, what we think about is that the enterprise software, the phases that we go through and how that software is continuously upgraded, improved, and pushed out into production is much more agile than what we've seen historically in our environment, whether that's inside an IT operation or an OTT operation, or even in the comm service provider operations. We want to drive towards that agility to where we can make modifications and improvements to our functional code on a very rapid fashion. And DevOps is the model that we use to do that. So traditionally, we looked at what was often times called the waterfall diagram or 2716 type of development environment. We have a development and a unit test phase, functional test, and then structural testing, the performance testing, the fault testing that takes place, what some people might call non-functional testing, but to me that sounds like it's the broken part of the testing. But that structural type testing, and then some type of user application testing. And then, finally, operational testing before an application went into deployment. And that lifecycle in many times was weeks, months, or in some cases many months or years. And what DevOps does is DevOps looks at a development environment that shrinks and concatenates those efforts together[O3]. But in addition, once an application has been deployed, it's still the responsibility of the original development organization to maintain that application as opposed to having a wall built between a service operation that provides maintenance applications to a deployed operate of functionality and a development operation that's working on, for example, the next major code release. So, it simplifies and reduces that waterfall cycle that takes place and we have more of a continuous integration phase that takes place into that type of an environment. So, DevOps is about shrinking that lifecycle maintenance activity that takes place and drawing the operational responsibility out of a dedicated organization and back into the development organization, tearing down some of those walls or silos that have existed so that that code that's live and in production in that environment is still the responsibility of those organizations that originally offered it-- authored-- offered that code. One of the concepts as well that we bring into scope, then, in this cloud native environment is that of a microservices. As earlier, I talked about that one of the goal inside Containers is to remove the state operation from inside the container itself and then move that out further to the edge so that that Container itself is more readily scalable. This goes into that concept of that microservice, where those containerized applications themselves have been minimized to the smallest functional element. So, you think about writing a novel. Applications in the past were novels, and once they were written, an entire novel existed and had to be maintained in that context. But the microservices, you want to think more about the fact that chapters have been broken down into paragraphs, and maybe paragraphs have been broken down into sentences. And any sentences are the smallest element of functionality that gives a single concept, that communicates something useful in that environment. And inside that collection of microservices, we can stitch these sentences together, if you would, into paragraphs. Then we stitch those paragraphs together into chapters in order to construct a service itself out of that. So, it's a desegregation. And it's also the mechanism that we use to allow the inner communication of those applications, and we need that. Stitching together to be relatively lightweight so that it's a high performance. So, what I've been trying to describe is the architecture of the past where we had-- in the last millennium when dinosaurs roamed the Earth and some of us started our career in coding, is that we had very heavy processes involved in the development. And we've created these very heavy applications that were dedicated functionality that lived on a very specific instance of an operating system.[O4] And as the operating system moved forward, we would have to go into do development work to move onto the next generation of operating system and the next generation of hardware. And in the early thousands, we began to see that desegregation to those virtual machines, where we started recognizing the fact that there was a better way to write this code. So just like we started threading applications, we began accessing multiple cores inside those applications and began more of an East to West scaling of those applications. And that forced us to change the way we were developing our code. And we were thinking about turnaround times in the terms of two to three months on these elements, and we felt that that was a great progress from the environment that existed in the previous millennium. And now into this environment where we've got very lightweight applications in these microservices that are running as containers that can be spun up and spun down very quickly and that have tremendous amount of scale, working on a multiple thousands of these applications or applets in that microservice environment existing inside our network, compute, and storage. And those get stitched together dynamically through technologies such as software defined networking in order to realize or implement a service that can be maintained and operated. So, here's another example pictorially of what we're talking about in that monolithic application or SOA to MSA transformation as we think about how to stitch those microservices together in order to get an interesting set of applications that can be built up from that. So that transformation from that very large, monolithic instance of the past into those microservices in the future is that goal of that transformation. As we look at it in the comm space, we're a long way off. This is a journey and not a destination for us as we look forward to that. Because the standards and the specifications we operate in in that space actually have to have stateful information in them that's contained deep into the core of the network. And we need to work to desegregate that functionality so that it's moved more towards the edge. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[3083, 3094, "Containers"], [3143, 3152, "Containers"], [3216, 3225, "Containers"], [3321, 3334, "Containers"], [5661, 5671, "Containers"]]}
{"id": 82, "data": " [O1] So, let's spend a minute or two talking about some[O2] of the other noteworthy projects inside the Container environment listed here. And I'm not going to try and read all this out. You'll have access to the material. But inside OpenStack and Containers, Magnum is an OpenStack environment for Containers that you might want to consider. Kolla-- again, OpenStack IN for Containers. And then Stacknetes is another one that is more applicable probably into a Kubernetes type of an environment. So, you can Google all of those and dig down to your heart's content. Some of the other projects that we see inside here for integration efforts Stacknetes.[O3] And rktnetes into the Kubernetes environment is another interesting effort that's underway. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles.", "label": [[235, 244, "OpenStack"], [249, 259, "Containers"], [274, 283, "OpenStack"], [300, 310, "Containers"], [359, 368, "OpenStack"], [376, 386, "Containers"], [463, 473, "Kubernetes"], [681, 691, "Kubernetes"], [105, 115, "Containers"]]}
{"id": 83, "data": " So, let's spend a minute or two talking about some of the industry standardization efforts that are going inside Containers. So, there's the OCI, the Open Container Initiative. And their mission really is to attempt to standardize Container formats and their runtime requirements. This is not moving as quickly as some of the industry would like it to see. There are a number of companies that have taken the lead here. And it's an area that probably needs a little more emphasis. The reason behind this, really, is that when we look at the very large cloud service providers, they're self-sustaining. They can manage their environment without the need of standardization. But when we start looking at the disruption that's taking place in that desegregation of the applications from the purpose-built solutions in the past, there's some opportunity for some acceleration that takes place there. There's some links that should be hot in your slides here. You should be able to click on that and dig down into it and learn a little bit more about what's happening in that space. Some of the other initiatives are the Cloud Native Computing Foundation.[O1] And in this environment, the charter is really more about stitching together Containers in that ecosystem. So, we talked about the microservices and how those microservices need to interact with each other. If you think about what's necessary for multiple vendors who might have different microservices in order to stitch them together, this is an area when Cloud Native Computing Foundation may be able to play a role in tying those types of efforts together.[O2] This is well integrated with some of the efforts going on in the Kubernetes projects. And there are some labs that are available out there. And again, this is all very much state of the art and forward looking. So, I encourage you to-- if you're a developer into this environment and starting to get into the Containers, I certainly encourage you to become familiar with what's happening out there in this area of standardization. [O1]Please merge these two subtitles. [O2]Please merge these two subtitles.", "label": [[114, 124, "Containers"], [1233, 1243, "Containers"], [1686, 1696, "Kubernetes"], [1930, 1940, "Containers"], [156, 165, "Containers"], [232, 241, "Containers"]]}
{"id": 84, "data": " So, in summary, we're going to touch on a couple of the key points in containers. So, this is a reiteration of some things that we've talked about in the past, but we're just going to summarize it here a little bit for you. Containers rely on some of the technology we're familiar with. We talked about processes and how containers, you might want think of them as being processes on steroids that allow us to control segmentation both in the control groups, and the name spaces, and in the advanced file systems, and the containers have a simplified packaging of those applications for more of a lightweight virtualization so that we can scale a larger number of containers onto our environment than we might have seen inside a proper virtual machine type of environment. Containers are a way to introduce a new service-oriented abstraction in that that micro services, that ultimate disaggregation to the smallest element. I give the examples of a book in chapters, and in paragraphs, and finally, the sentences. And what we're really looking at in that container environment is a way to develop the smallest functional element that has meaning to it. And that's a journey as opposed to an actual destination. And then the fact that the container environment is evolving very rapidly. As we would expect in a rapidly evolving environment, there's a large ecosystem that's out there, and there's some judicious choices that need to be made either from a roll your own under the Linux container or from some of the commercial offerings that are out there that are all great ways to go. And some of it is dependent on your resources. Your development resources you may be able to bring to bear, and your ability to support those type of environments. And then finally, we touched very briefly on some of the standardization that needs to take place. Sometimes we hear the debate of whether we're looking at open source or standards, and this isn't an either-or question. You can have standards and you can have open source simultaneously, but the standardization inside containers is still nascent. And I think that goes back to the fact that there's a tremendous ecosystem that's still out there that's evolving very rapidly in that environment. So, we would expect to see that the maturity of containers will continue to evolve rapidly, and that we're going to congeal on certain standards as we begin to sort out what works best, and best being both a technical and an economic question. So that's kind of a summary of what we're seeing in the industry, and where we're going with this grand vision of containerization as we start looking at a more cloud native type of environment, either in the OTT, or the IT, or the enterprise environments, or more specifically in the com service provider environment. And the latter is probably further ahead than the former in that expression. NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[225, 235, "Containers"], [322, 332, "Containers"], [523, 533, "Containers"], [610, 624, "Virtualization"], [665, 675, "Containers"], [774, 784, "Containers"], [1057, 1066, "Containers"], [1240, 1249, "Containers"], [1486, 1495, "Containers"], [2070, 2080, "Containers"], [2295, 2305, "Containers"], [2605, 2621, "Containers"]]}
{"id": 1, "data": " So, we need to touch on some of the things that network function virtualization is driving into containers, and in particular into Kubernetes. So, let's take a look, first of all, at container networking from a topological view, is that if we look at some of the key areas or challenges that we face in containers when we're considering the deployment of this in a bare metal environment, some of those challenges are in the areas listed above. Down that first column is that we've got-- we've got multiple network interfaces are required for your typical VNF in a coms type workload. This is a significant deviation from what we might find in an OTT or an IT environment where we talked about the cul-de-sac. That is, the traffic ingresses on a particular port and egresses on the similar port. We talk about that as being a north to south type of traffic. And that's great. Containers are very well suited for that environment. But in a coms environment, that traffic is more of an East to West in many cases. And then as it flows in on one port and it's got a destination that is heading in a different direction, that's the west side, So, it needs to flow out another port. Kubernetes, and containers in general, are evolving rapidly to address this issue, but it's one of the shortcomings that needs to be addressed. Similarly, we've got-- we've got challenges inside the acceleration aspects of things. Again, coms service provider workloads typically have a high flow rate of I/O associated with them and lesser on the compute side of things. So, we need accelerators for data plane acceleration for that east to west traffic as we spoke of. And then there's actually orchestration of the management of those platforms themselves. We identified in our earlier discussions about how the orchestration in containers have descriptor capabilities in their feature set that allow us to identify capacity or capabilities of the host platform that we're targeting. And we need a way to pull those resources from those platforms so that they can be accessed in that container orchestration environment.[O1] And capabilities that we might find, for example, in the Intel enhanced platform awareness, also known as EPA, are critical elements to allow the orchestration environment to pull those host machines to identify what resources are available in those machines, and also, at a higher level, to identify what resources are being consumed at any time as we begin to look at optimizing the workloads that can be deployed there. And to that consumption level, that drives the issue of telemetry and functionality. For example, like Collect D is a resource that allows the orchestration and the upper layer of elements of the management system to identify the functionality thresholds that are being consumed by those machines. So those are some of the challenges that we see in some of the technologies that we bring to bear in addressing the evolution of the development that's taking place inside the container environment to meet the needs of those coms service provider or those NFE workloads. So, in the networking area, one of the interesting areas is the the multi network interface. This is an area that's being driven by Intel and others into that environment that extends beyond the traditional single interface that originally would have found-- a zero interface, if you will-- in a Kubernetes environment to allow for multiple interfaces. And that is whether those interfaces are bonded to give us greater capacity or if they're bonded for resiliency or redundancy in that environment, or whether we've actually got interfaces that are plumbed to different networks. That is, an east network and a west network. And in addition, to the out of band network interface oftentimes that we find for the OAM and P functionality that's associated with these VNF functions in that comms environment, and that is the management or the MS interfaces typically would reside on an outer band network interface as opposed to being superimposed on the East interface or the West interface as we spoke about that. And the reasons for that is that if you look at the functionality inside the data center, you may have multiple layers of switches that are pointed in one direction or the other and there's an isolation that takes place. Some of that's due to NAT'ing and others are simply that, as the traffic traverses these networks, it's a hop along the way and we're going to perform some computational calculations on that.[O2] But, nevertheless, our responsibility is to move that traffic through a network as opposed to terminating it to an endpoint in that network. So, those are some of the use cases. Multus is an effort, a project that's underway, to address the functionality that takes place. The element there is that-- we spoke earlier about the pods inside the containers-- and those pods have that one to one association with that network element. And there really is that one to one and we need to expand that so that the pods themselves aren't owning a single interface but rather can have access to multiple interfaces. And in the future that those multiple interfaces can actually be shared across multiple containers. One of the efforts that's in there is known as the CNI, and that's the Container Network Interface plug-in module. So inside containers we have this concept of plug-ins, and those plug-ins are an industry solution to that East to West traffic. So, we can complement some of the limitations that we find in the existing packages-- for example, in the container environment-- that limit us to both packet acceleration and to the single interface through these efforts for the Multus CNI. And Intel's actively engaged in some of those areas to accelerate that. The acceleration point, there are lots of different technologies for acceleration. One of them is DPDK accelerated Open vSwitch. So, we talked about container applications needing to interface to each other, and one way of doing that is by running traffic through an accelerated vSwitch where the traffic doesn't need to leave that host but can actually move from container to container as long as it's inside a common pod. Or if traffic's moving North to South through the platform itself, off of that interface and being sorted through DPDK OVS application and other acceleration applications, for example, like FDjo. And these are the efforts that are underway. And there's a link to a reference down here for a GitHub to some user space CNI plug-ins in your container environment. And the intent here, again, is to accommodate a container design that would enable better throughput in those East to West traffic loads that we'd find in those coms spaces. We've mentioned DPDK a little bit. One of the other packet accelerations is a little closer to bare metal is the SR-IOV. That's the single route I/O virtualization functionality. And even in the SR-IOV there's an effort underway to integrate this in more tightly into the Kubernetes design so that we have access to the acceleration that an SR-IOV based acceleration model can give us. And, again, in that space, you may also find that SR-IOV can rely on DPDK at the application layer for the pulling mechanism. And there's some hot links in here to some of the work that's going on. Again, GitHub is a great source for that, and Intel and Redhead are very actively involved in that packet acceleration CNI plug-in NOTICES AND DISCLAIMERS [O1]Please merge these two subtitles. [O2]Please merge these two subtitles.", "label": [[557, 560, "VNFs"], [3864, 3868, "VNFs"], [97, 107, "Containers"], [877, 887, "Containers"], [1195, 1205, "Containers"], [1811, 1821, "Containers"], [4873, 4883, "Containers"], [5224, 5234, "Containers"], [5361, 5371, "Containers"], [304, 314, "Containers"], [132, 142, "Kubernetes"], [1179, 1189, "Kubernetes"], [3394, 3405, "Kubernetes"], [184, 193, "Containers"], [6332, 6336, "DPDK"], [6768, 6773, "DPDK"], [7025, 7036, "Kubernetes"], [7207, 7213, "DPDK"], [2066, 2075, "Containers"], [3004, 3013, "Containers"], [5307, 5316, "Containers"], [5586, 5596, "Containers"], [5892, 5896, "DPDK"], [6157, 6167, "Containers"], [6171, 6181, "Containers"], [6866, 6872, "SR-IOV"], [6948, 6954, "SR-IOV"], [7094, 7100, "SR-IOV"], [7189, 7195, "SR-IOV"]]}
{"id": 2, "data": " Hi, so let's wrap this up a little bit, talk about some of the other challenges that are facing us if we choose to head down the container path for our interesting workloads here. So, resource management inside Kubernetes, one of the things that we could possibly run into is that while the clusters are deployed on a wide array of heterogeneous environments, this implies that there are different hardware resources below. So, we can have different versions of CPUs, whether those are in the same generation of family or even different generations of family. The amount of memory that's available on those machines may, in fact, be different, as well as some of the core resources, whether this is hardware acceleration or whether there's a particular instantiation of a net  card.[O1] There are a number of things that can be different as we look at these large scale computer environments in that environment. So, one of our goals, then, is to introduce a way of managing those resources so that not only can we identify what those resources are, but then we can target specific workloads if they're dependent on features or capabilities in that resource to those workloads. And now we've mentioned Intel's EPA, or enhanced platform awareness before. This allows us to identify, for example, the CPU instances themselves, but also, some of the features that may exist on the card, whether those are large pages, whether those are any particular NUMA alignment for physical resources or accelerators that are involved in those resources as well as some of the other deeper capabilities in their instructions and architectures. So AVX 512, for example, if you've got a feature that choosing that type of capability for pattern matching or some type of security capabilities that are in there. So, there are lots of features that are available to us that in that resource management we need to be able to access, and this is an ongoing effort for the Kubernetes environment under the container work to continue to keep up with those various capabilities that show up in the deployment of those standard high volume servers from generation to generation. One of the other areas that's a challenge, we spoke about networking, and this is going to get into some of the aspects associated with that is the device plugins. So, if you have different network cards, even though you're going to have drivers in there, you may want to access particular acceleration capabilities that those intelligent cards are able to access. In order to do that, then the vendor of those devices may, in fact, have to write custom Kubernetes code in order to allow that device to be integrated into that system so that we can pull those resources and identify them from a management and orchestration layer.[O2] Some of the use cases that we may find for that do exist in the com space, whether those are rand capabilities that might need FPGAs, or whether those are things that, for example, you might find in an APC where we could take advantage of some of the packet acceleration. So, these NFE limitations can be addressed by the device plugins, but again, it places a burden, then, on the provider of that core technology to ensure that their product not only has drivers for the host operating system, but it has been integrated in so that they have Kubernetes awareness that's in there. So, what are we doing? The industry is moving forward in this area. Intel is helping a number of vendors in this space to provide integrated solutions that allow for the easy deployment of those workloads and those plug-ins into this environment. And this is an ongoing area of development research. So, before you assume that you're going to be able to move forward a particular accelerator card, if it's in a Kubernetes or a container-based environment, it's worth your due diligence to dig down into it. We talked about some of the packet acceleration before, and SR-IOV is a key element normally in that packet acceleration.[O4] It allows us to get near bare metal capabilities. Intel has been working very closely with Red Hat in the area of supporting the SR-IOV capabilities inside a Kubernetes environment. And here you see a little example of some of that work. So, the areas that are ongoing are to provide for that plug-in so that we've got the ability to advertise to multiple consumers above.[O5] Because SR-IOV implies single route. That is, it's a single card. But that the resources of that card could be multiplex across multiple pods then, and we want to be able to allocate certain resources for that on a pod by pod basis as those get consumed by a platform without over committing the resources there. So, this is an area of ongoing research, and there's a reference down here on the hotlink that will allow you to dig into that a little bit further. As you look at the future and we begin to look forward to it, SR-IOV's not the only solution, but there is certainly a good model for the container network interfaces in that we expect to be extended into additional pluggable modules allow for us to multiplex more than one port and multiplex resources of that port into these environments as we go forward. So, we can address pod scheduling, the SLAs for the VNFs that are actually in there so that if we allocate 4 gig of resources to a particular VNF, we can guarantee that that amount of resources is going to be allocated to that VNF and not consumed by other VNFs, even though there's a common resource on in the porting that's below that. So, this is all an active area of work that that's going on among the community to support these types of networking loads that are more unique in that com service application than you would find, for example, in a traditional IoT over the top environment. So, there's more to come in this area. I do encourage you to think wisely about this NOTICES AND DISCLAIMERS [O1]Please merge these two subtitles. [O2]Please merge these two subtitles. [O3]Please delete. [O4]Please merge these two subtitles. [O5]Please merge these two subtitles.", "label": [[5254, 5258, "VNFs"], [5458, 5463, "VNFs"], [211, 222, "Kubernetes"], [1952, 1962, "Kubernetes"], [3334, 3344, "Kubernetes"], [3782, 3793, "Kubernetes"], [2609, 2619, "Kubernetes"], [4163, 4173, "Kubernetes"], [4906, 4912, "SR-IOV"], [5344, 5347, "VNFs"], [5767, 5770, "IoT"], [130, 140, "Containers"], [1985, 1994, "Containers"], [3799, 3808, "Containers"], [3939, 3945, "SR-IOV"], [4134, 4140, "SR-IOV"], [4390, 4396, "SR-IOV"], [4981, 4991, "Containers"], [5429, 5432, "Containers"]]}
{"id": 3, "data": " Data Plan Development Kit started as Intel project. And it was open sourced very early with, at that time, huge help from 6WIND. Initial open source community got created. And already, in 2014, there was the release that included support for other CPUs and NICs. And this was done fully in open source. At that time, the versioning was 1.7. Later over time this thing, moved to Ubuntu like versioning of year dot month. And this was also when the first DPDK Summit was held. Over time, more and more vendors joined the community and contributed to the code base. And in 2017, it was all transferred as proper open source community under Linux Foundation, which also further ensures this openness. And in 2018, in particular, focus was to do a lot of contributions that are abstracting what is then all DPDK user space application from the underlying platform. In this way, making it more cloud friendly for those environments. The statistics of the community in terms of vendors joined and contributions and commits look very healthy. And there is growth to be observed over the years. And today, DPDK is the primary way NOTICES AND DISCLAIMERS", "label": [[453, 458, "DPDK"], [803, 807, "DPDK"], [1098, 1102, "DPDK"]]}
{"id": 4, "data": " Welcome to our session on Data Plan Development Kit or shorter DPDK where I'm going to go and cover the DPDK fundamentals. Where this thing fits, in general, is on their journey to run multiple workload categories-- very broad categories on volume ingredients in volume server on Intel architecture. So, this is, in particular, focusing on packet processing part of now very old 4 to 1 vision where, initially, we were running and then more and more migrating over the years. Application processing control processing already then in comes vertical. And with DPDK, this allows us to run packet processing type of workloads also on Intel architecture-based servers. So, the fundamentals of DPDK APIs in libraries are, for example, that this supports both run to competition and pipeline models. It does not include scheduler. So, for example, when it needs to access devices, it is pulling them in endless loop. So, it supports various operating system 32-bit, 64-bit different types, Intel Atom and Xeon processors. It recognizes required number of cores, and instructions include that type of memory and channels to that.[O1] So optimal packet allocation across all those different channels then happen. It supports huge pages so that it's easy to access memory in one go instead of going there multiple times. And it uses bulk concepts so that a lot of packets are processed simultaneously, and it's licensed as BSD primarily and then partly of the GPL because some parts are in Linux kernel. So, it is useful for building very optimized workloads in communications vertical for packet processing. It can also be used for similar type of workloads in Cloud, enterprise, and government, and the big categories of those modules are, for example, core libraries. They are taking care of memory management, software rings, timers, and similar. Packets classification is big category, then accelerate the software libraries, statistics, quality of service, and package framework is also there. So, on the bottom part of this diagram, you can see all the different type of devices that are supported. And this is, for example, starting with Ethernet devices, then there are the crypto devices, event-driven devices with their pull mode drivers, security compression, and broadband devices were added recently. So, this type of environment is including a lot of sample applications. They are easy way to start when called BSD license needs to be adopted. And readily they will be also useful on their own. For example, a lot of performance characterization we are doing with layer 2 and layer 3 forwarding applications from there. But then these type of code samples are then integrated by all the different type of vendors into their solutions. And today, we have a great adoption of those, and there is really a lot of products available during excellent packet processing using this type of approach.", "label": [[64, 68, "DPDK"], [105, 109, "DPDK"], [560, 564, "DPDK"], [690, 694, "DPDK"]]}
{"id": 5, "data": " For data plan development kit, we have a lot of both open source community projects that are adopting it, and also vendors in their commercial products. So, there is, for example, list of virtual switches and virtual routers. Open virtual switch is something that Intel teams patched with DPDK many years ago. It's now included in all the usual distributions of Linux. So, they're listed, for example, here in the middle. On Red Hat, Ubuntu, and so on, it's all included. Then Tungsten Fabric, or what was previously OpenContrail, is also in the meantime patched with DPDK with further investments that we are putting into that to have the vRouter there running really well and optimized. VMware vCloud environment, in particular vSphere 6.7 with NSX-T 2.2 is supporting and including the DPDK for packet processing. And then we did quite a lot of work inside FD.io, including DPDK, and then there are the other communities and switches that are also supporting it. In terms of adoption in the operating system, beyond what I mentioned-- Red Hat and Ubuntu-- there is also adoption in Free BSD, in Wind, and Windows, Fedora, CentOS. A lot of package generators are benefiting from blasting packets by using DPDK libraries. So, a TRex package generator are, for example, some of those that we used a lot. There are the other ones. And it is also helping in storage environments, in storage workloads. So, there is particular version of it. And this is also something that we contributed into Ceph. And then there are inside operating systems, TCP/IP stacks, that are being optimized with some of the examples listed here. Inside DPDK, there is also support for accelerators in sense of having generic API, where the application in user space is then hitting the API. This protects the software investments for the software vendor that is doing that. And then through this device framework, there are different drivers that can be plugged in. And this can be, for example, the implementation on Intel Xeon processor or it can be QuickAssist Technology type of chipset or adapter, accelerating, compression, and encryption algorithms. It can be some sort of SmartNIC, or it can be other devices. So, the idea is to have this type of device API abstracting NOTICES AND DISCLAIMERS", "label": [[290, 294, "DPDK"], [568, 573, "DPDK"], [790, 794, "DPDK"], [878, 882, "DPDK"], [1208, 1212, "DPDK"], [1629, 1633, "DPDK"]]}
{"id": 6, "data": " Welcome to our session on Fast Data I/O. Fast Data I/O project, or abbreviated FD.io and read as Fido, is Linux foundation project consisting of scalable network stack, open benchmarking effort, and integration into NFV and cloud environments. Scalable network stack is based on vector packet processing, VPP, which is scalable, fast, and feature-rich network stack. Open benchmarking environment is an effort consisting of methodologies, tools, and publishing results of measurements in well-defined labs. And NFV and cloud integrations mean integration into various NFV communities, into appropriate cloud networking communities, and is coming as part of the open source Linux distributions like Ubuntu, CentOS, and openSUSE. FD.io has five targeted use cases for adoption. It started historically as a physical appliance, where the VPP stack control plane parts are in production in routers since 2002. The second one is for building virtualized network functions, where the network stack can be used for faster programming of these types of elements. The third one is to use it in NFV infrastructure and with it build virtual switch or virtual routers, and already exists integration with OpenStack that KVM. The fourth one is to build containerized or Cloud Native virtual functions. And the fifth one is to use it in containerized platforms with a vRouter for containers and there is already integration in Kubernetes and Docker environments. So, with these type of use cases, we can build various appliances, virtualized network functions, or containerized network functions like Load Balancer, Carrier Grade NAT, firewalls, elements running on universal CPE, and IPSEC Gateways and SDWAN and other different VNFs. Out of the box functionality for building these use cases is very rich compared to some other environments. And here is a comparison of how, for example, a software development kit for building discrete appliances or for NFV infrastructure, containerized platforms, VNFs and CNFs, how does it compare to DPDK, which we introduced in another session, which is a software development kit for packet processing. It is integrated in FD.io. Obvious DPDK is, for example, part of NFV environments for virtualized or containerized versions.[O3] And with FD.io, as mentioned before, we can build all of that because of a huge versatility NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please delete. [O3]Please merge these two subtitles.", "label": [[80, 85, "FD.io"], [2152, 2157, "FD.io"], [2270, 2275, "FD.io"], [1717, 1721, "VNFs"], [1989, 1993, "VNFs"], [1944, 1947, "NFV"], [1241, 1254, "Containers"], [1414, 1424, "Kubernetes"], [1551, 1564, "Containers"], [1964, 1977, "Containers"], [512, 515, "NFV"], [569, 572, "NFV"], [1085, 1089, "NFV"], [1324, 1337, "Containers"], [1367, 1377, "Containers"], [2027, 2031, "DPDK"], [2167, 2171, "DPDK"], [2197, 2200, "NFV"]]}
{"id": 7, "data": " VPP, as foundation for FD.IO, is doing, as the name says, Vector Packet Processing. This means that it takes whole vector of packets and processes it through the graph node. And that way it goes from node to node and completes the whole graph, which is much more efficient way of doing that than taking individual packets and trying to do it, because in this case, when whole vector of packets is being processed, the first packet warms up the instruction set. And everything else goes much faster then. This also gets it to the predictable latencies and predictable run rates. So, the graph itself can be reorganized. It can be extended with plugins for the nodes that are done in software or they can have also some hardware acceleration.[O2] And the way how the packets are ingested is that there is a DPDK element as input. And then the whole set vector of packets is being processed in this rough node. And then this is how eventually it completes the whole graph. To configure this and to manage it, there is honeycomb management agent that exposes interfaces like NETCONF/YANG, REST, and BGP for further SDN integration. So, the universal network stack is covering Layers 2 to 4. It has control plane, traffic management, overlays, and other functionality, support for Linux and FreeBSD, kernel interfaces. It works in containerized and virtualized environments. And it can be used to build appliances, VNFs, CNFs, and infrastructure virtualized and containerized. So, it has extensible modular design, where modules are pluggable. And the whole graph can be reorganized easily through the system of plugins, extended. As mentioned, it is very fast, and scalable, and deterministic. So fast is because of the method that it uses to process packets, scalable because it's very parallelized and modular in design, and deterministic because of what was mentioned, that the instruction caches, everything gets pre-warmed. If the run rate loses a little bit, it will catch up in next rounds. So, it is also developer-friendly and all nicely packaged and documented. High-level view on the VPP is presented in this diagram-- So, devices are coming, of course, at the bottom of the stack. So, support for DPDK, QuickAssist Technology, and other type of interfaces is there. The three layers of IPSEC are-- so, for Layer 2, there is usual functionality of discovery, bridge, Access Control Lists, VLANs, MPLS. For IP, and in particular for IPSEC, there is version 4, version 6, and all the other related functionality that fits there. For Layer 4, it can be made stateless and stateful. For traffic management, there is appropriate functionality. For control plane, I already mentioned BGP. The other functionality is supported there to build real elements out of the whole stack. And then overlays are also supported so that we can get to nice virtualized environments. For controlling all that, supported is OpenDayLight, and Contiv, and Neutral ML2, and a few other environments there. So, re-combining different functionalities included in network stack, already mentioned appliances-- and virtualized, containerized elements can be built. Here are some examples of what needs to be included to build environment for universal CPE or for Load Balancer in Cloud environments or Broadband Network Gateway, BNG, or Intrusion Prevention System.[O5] So, the right choice of functionalities allows for fast integration of these types of elements based on very mature code in VPP. For performance in scaling, performance, one based on the algorithms that are being used for Vector Packet Processing is fast. But then also, because it uses the instruction sets for vector instructions like AVX, it does have full packet processing integration of DPDK. And because of highly parallelized and modular design, it comes to linear scaling. These types of numbers are then being measured in each release NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles. [O3]Please delete. [O4]Please delete. [O5]Please merge these two subtitles.", "label": [[24, 29, "FD.io"], [1411, 1415, "VNFs"], [806, 810, "DPDK"], [1327, 1340, "Containers"], [1345, 1357, "Virtualization"], [1458, 1471, "Containers"], [1442, 1454, "Virtualization"], [2206, 2210, "DPDK"], [2845, 2856, "Virtualization"], [3094, 3105, "Virtualization"], [3107, 3120, "Containers"], [3742, 3746, "DPDK"]]}
{"id": 8, "data": " As part of FD.io Continuous System Integration and Testing effort, there is open benchmarking, and as any performance characterization that we like to call benchmarking. It needs to follow the properly defined testing methodology. It needs to be repeatable, not be tuned for particular vendor product marketing outcomes, and also needs to have realistic use cases in realistic traffic scenarios.[O2] So, for FD.io CSIT, it is based on standard industry benchmarks and tools, like appropriate RFCs and open-source tools are included. It is easy to replicate in different labs and it works on different platforms from different vendors and it does provide realistic outcomes in terms of some numbers that are representative of deployed environments. And this is really taken seriously in the FD.io community, and every release is publishing appropriate reports with all the tools. So, if somebody likes to revalidate that, they are free to do that, contribute it back into next releases for FD.io. These type of results have breadth of test use cases, depth of all the detailed information in there, and then also are predictable because it can be then, I said, rerun and compared with what you are getting. This way and based on all the performance optimizations, it comes to what is, in marketing terms, used as terabyte elements on Intel architecture-based server NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles.", "label": [[12, 17, "FD.io"], [791, 796, "FD.io"], [989, 995, "FD.io"], [409, 414, "FD.io"]]}
{"id": 9, "data": " [O1] For previously mentioned five target use cases, there is a decent amount of vendor adoption from top tier of vendors, like Cisco and ZTE, for example. And this is coming in different implementations of those use cases. So physical appliances, for quite some time, Cisco is using the stack to build these types of products. In virtualized, there is-- for infrastructure-- faster building of it. Cisco has their version of OpenStack. They also have a virtual topology system that is based on Red Hat OpenStack and properly integrated over there. Alibaba and Yahoo have cloud load balancers based on it. And then in containerized environments for the functions, ZTE based quite a lot of their work on it. And then also, there is a Cisco Container Platform. And the vendors that are building these types of products, also here you can see a quote of happy engineers, how much work this type of environment saved them because they did not have to go and rework this type of functionality inside the network stack. A summary, FD.io is a very mature and robust network stack for integration of elements and environments for NFV and cloud. It includes inside this CSIT open benchmarking effort, which is very comprehensive. It addresses five key use cases, which are building appliances, virtualized network functions, virtualized infrastructure, containerized network functions, and containerized infrastructure platforms. And it can be used out of the box NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[1025, 1031, "FD.io"], [332, 343, "Virtualization"], [1123, 1126, "NFV"], [619, 632, "Containers"], [740, 749, "Containers"], [1285, 1297, "Virtualization"], [1317, 1328, "Virtualization"], [1345, 1359, "Containers"], [1381, 1395, "Containers"]]}
{"id": 10, "data": " Hi. In this session, we'll cover introduction and basics[O1] of cryptography, give a little history and how we're using it. So even now, when you're accessing our website here, this is being secured. So, if you click next to the HTPS, which is Hypertext Transfer Protocol, secure version of it, the way how HTTP protocol is running over transport layer security, you can go there and click and check with your own browser that this is a secured web. You can click on your certificate and see that proper certificate authority that your operating system and browser are trusting issued to our website such security and that the whole thing is properly configured. [O1]Please merge these two subtitles.", "label": [[65, 77, "Cryptography"]]}
{"id": 11, "data": " History of cryptography started many, many years ago. So even in ancient Egypt, anthropologists discover that hieroglyphs were written in a way that some were substituted with the goal of somehow concealing the message, obscuring the meaning of it. In many centuries later, there was also, for example, in the middle of this slides, there is cipher that is using cylinders and leather rope, leather band. So those letters written on the leather would be wrapped around the cylinder. And once unwrapped, to really read it again, you would need to have the cylinder of the same characteristic. And in Roman Empire, there was a Caesar cipher, which is shift cipher. At that time, there was already an alphabet. And when they were shifting it by number of letters, they would be using decoder wheel to get the meaning of the message back again. This, of course, over time developed further and got more complicated so that it's much harder to get the meaning of the message without having all the right tools in place. So in the Middle Ages, there was a Vigenere cipher, which is much more complicated version of a shift cipher being used with substitution of letters. And then there was very complicated construction of Enigma machine that could change the letters in many trillions of ways. And still, even this type of machine-- there was another team that, once they got hold of it, they could disassemble it, reverse engineer, and figure out how it works and pretty much intercept the messages by doing that. NOTICES AND DISCLAIMERS", "label": [[12, 24, "Cryptography"]]}
{"id": 15, "data": " To help exchanged encrypted messages in very distributed environments, like Internet, for that, we have public key cryptography, which is form of asymmetric cryptography in the sense that the encryption key is not the same on both sides of the communication channel. We have public key and private key. So the sending party has private key, which needs to be really well-protected to encrypt the message. And then there is the public key that is, as the name says, publicly shared so that the receiving party can use that key to decrypt the message. And this way, we can, without sharing the same key, we can share the encrypted messaging between two different parties. Of course, it goes the other way around. So if the receiving party needs to send something back and needs to have similar type of mechanisms, we can implement it both ways. So the other related concept to that are digital signatures so that the messages, by having the private key, we can do the digest of it. And this way, we ensure the integrity of the message. And the receiving party, by using the public key, can go and verify that this message is really originating from the sender and that it was not tampered with in the transit. And to help with establishing trust between multiple parties, there are certain amount of very trusted certificate authorities that have the role of giving these type of certificates to include the identification that is really verified by certificate authority and the public key and the proper domain name qualifier for the, for example, the website. And this is what I was mentioning at the beginning of this whole section. This is how you could verify that over there, the HTTPS with a small logo, view certificate, that the right certificate authority that you also trust is involved in ensuring that the website and the message, and no-tampering, and the right algorithms are supported by delivering this type of web content. NOTICES AND DISCLAIMERS", "label": [[116, 128, "Cryptography"], [158, 170, "Cryptography"]]}
{"id": 17, "data": " than handling RTL upgrades, and so on. Why Hyperscan? There are other regular expression engines to be used. So this one is most optimized, and not just for raw matching speeds. The teams focused also on different modes of operation being streaming and non-streaming. The non-streaming is block on small writes so when very small text needs to be found, like searching for user agent is example here, for performance under high match rates, when there is a lot of matches happening and callbacks are going to be triggered, scaling to multiple cores and threats so that its design is paralyzed and takes advantage, software takes advantage then, of available processing resources. That pattern database is compiled in short time. And that the byte code size and the stream state and overall memory utilization is then highly optimized. Long list of Intel architecture instruction set is being used. Some are here, so streaming extensions and vector extensions, for example, which, in performance-required environments, means that Hyperscan can then replace other type of engines. There are different modes of operation that are supported. So there is block mode, where the data is taken as a block. And then it's being scanned for patterns by using the database of compiled patterns. There is vectoring mode, where data is not just taken in blocks, but there are vectors of those blocks. And then there is a streaming mode, which is streaming data and applying vectors on the stream. And all three of those are optimized with Hyperscan. The benefits is that it is very feature-rich. So many different types of regular expressions are supported. It does cross-compilation to multiple architectures. It does support different modes of operation, like block and vector and streaming. And it is robust with extensive features, used in many different RegEx, regular expression, constructs. It supports different environments in terms of hardware platforms, so Atom, Core, and Xeon processors. And compared to hardware acceleration, like FPGA being look-aside in, for example, some adapter, latencies are much better because this is all then in CPU caches and in memory. It is much more flexible, easier to program being C and C++ libraries.", "label": [[44, 53, "Hyperscan"], [1030, 1039, "Hyperscan"], [1526, 1535, "Hyperscan"]]}
{"id": 18, "data": " Welcome to our session on Hyperscan, industry's fastest regular expression and literal matching algorithm, based on Intel platforms, available in open source, and licensed with BSD. This is really well-tuned and optimized on a variety of Intel architecture processors, from Atom over Core to Xeon, supporting or integrated in various operating systems-- so various Linux distributions, FreeBSD, Windows, and OS X. It is itself based in C and C++ and has language bindings for Go language, for Java, for Python, and integrated in variety of open source communities and commercial products, like intrusion protection systems, intrusion detection systems, like Snort and Suricata, in deep packet inspection engine within SDWAN or for network and web security. This is improving throughput, allowing optimization of memory usage or reduction of CPU cycles for doing this type of regular expressions and an example of Snort, which is IDS/IPS product and involves large amount of literal and regular expression work. When it was integrated with Hyperscan, the throughput went up three times. Regular expressions are patterns used to match characters in text. And here are a few examples. We can look for exact matches or searching for some characters, replacing them with different strings. We can find also the strings in different streams of words, how many times they would repeat, all the different types of combinations are possible, creating a lot of different patterns to look for. And Hyperscan engine can use anywhere between one to many thousand of those. And it's based on libpcre Perl-compatible regular expression, which is used as semantic basis for fuzzing and for automating testing. Hyperscan, as a regular expression matching library, is running well-optimized on variety of Intel processors. It has BSD licensing for open source. It is having those match rule sets, all the different patterns that are then compiled, with input also being what kind of mode do we use for Hyperscan into byte code, which is database of those patterns, which is supporting multiple platforms then. It's byte code. And then the engine itself is going to be using the pattern database, is going to be looking at those in available text as per mode of operation. And when needed, it will do the callback if the match is found. And this makes the whole environment very flexible and powerful. NOTICES AND DISCLAIMERS", "label": [[27, 36, "Hyperscan"], [1040, 1049, "Hyperscan"], [1488, 1497, "Hyperscan"], [1695, 1704, "Hyperscan"], [1985, 1994, "Hyperscan"]]}
{"id": 19, "data": " In summary, Hyperscan is available in open source with BSD license on http://hyperscan.io. It provides substantial benefits to various projects or products that are using regular expressions and literal matching and need high performance of that, like IDS/IPS. An example of Snort improvement in throughput was three times. Suricata was six times. So it improves throughput. It reduces the memory footprint compared to some other engines doing similar, or it reduces required CPU cycles for it. And it is mature and solid and available in a big number of commercial products. NOTICES AND DISCLAIMERS", "label": [[13, 22, "Hyperscan"]]}
{"id": 20, "data": " With Hyperscan, we have different types of operations that will have appropriate performance impact. So first operation that we need to do is to compile initial input set of patterns into the bytecode. Compiler is implemented in C++. It does dynamic memory allocation. The compilation time will depend on complexity of the patterns. And anything unsupported will be given immediately to the developers. So the result is the bytecode database, which is then working on different target environments. The next operation is then the runtime. It can be in different modes. So here, example is the block mode, which is very popular. So it has the runtime components of scratch space, which is the working memory where the engine is doing the comparisons. It uses the compiled bytecode from the previous step as read-only data. And then it works on the blocks of input data. If something is found in those regular expressions and literal matching, then the matches are returned with a callback. And it is only predictable memory allocations that are being done. So it's not dynamically going to, for example, use too much memory. So the runtime is in C only. And if we want to do the other mode of operation, this is streaming. It is similar to the block mode. But also, the stream state needs to be maintained. So there are additional operations added, like open, write, and close. And there are also shortcuts like, for example, to reset the stream. An appropriate diagram is then presented here. For performance tuning, this is as in other similar engines of this type. And number of these tips are pretty intuitive. So the block-based matching on predefined blocks is going faster than for anything streaming. The scratch usage is to allocate the space once for your patterns database and then use it at compile time for multiple scans. If we are paralyzing the functionality with multiple concurrent operations, then each of them will have to use their own scratch space. The pattern syntax exact match with literals is, of course, faster than any wild cards. And especially if we are searching for longer literals, this ones are easier to identify within the text. And if we are searching for something particular, it's better to put it at the beginning of the pattern than to have it after a couple of wild cards at the end. And in terms of flags, it is faster if there is a single match and if we are not asking where exactly in the text there is start of the match, which, in number of cases, is acceptable. And this can be used that way. And of course, more details on tuning are available on the web.", "label": [[6, 15, "Hyperscan"]]}
{"id": 21, "data": " Welcome to the section about hardware ingredients, configurations, and how does an appropriate stake look like. Here we will start with the ingredients. And at the beginning, just the little introduction about the difference between normal IT and cloud environments, compared to what we're looking at network function virtualization computing is that in cloud computing, there is little data requests coming in. There is significant amount of compute, and then there is a real-- a comparatively small amount of data coming out. For example, SQL requests into relational database is one of the workloads that we are having over there. On the other side, we have for network virtualization, and we are virtualizing network appliances. Then we have high data rate of data pull in traffic. And this is a number of cases going through the element. So, the amount of data high going in, high going out. It can also go bi-directional and corresponding compute per the amount of data is much lower for NFV. So, this is then the resulting in very different hardware configurations that will be explained throughout this presentation. So, looking at where we are today with NFV is a lot of control plane workloads are virtualized now in production for many years and number of data pulling workloads is in production much lower. This is what for some time we are doing, and now we have good success putting it in. And the reason is that control plane was both technically easy to do compared to data plane, and we'll explain why in the second. And also, it was fitting on the IT configurations of servers so that the usual IT departments, when they are purchasing servers from the vendors, technically, this workload looking like other IT workloads. It was fitting nicely onto these type of hardware configurations, so it already existed over there. While data plane on the other side is characterized by this high data plane traffic rate-- examples are gateways and routers as network elements-- we measure performance there in packets per second. The hardware configuration that is required there is throughout this presentation called data plan server. It consists of normal IT volume server. In this configuration, it needs to have appropriate number of network ports in configuration that we will call balanced I/O so that both sockets are being fed with traffic. And because of the software that we use, typically, we'll include data plan development kit, and pull mode driver is spinning a core. They will need appropriate amount of cores in the CPUs, So, we will use high count CPUs, like 20 or 22 core, on previous generations like E5 or on the current one, on Xenon scalable, also this goes to, for example, 61, 52 or 61, 38 are those 20 plus core CPUs there. And current network adapters that you are using there is multiple ports per adapter, even number of adapters feeding data plan traffic into the server on 710 series controller. Here we summarized what are the major requirements on NFV infrastructure. So, the server already described components that we are using to build it. Beyond the usual CPUs and network adapters, we can put additional accelerators where needed and where they are beneficial. It can be fixed function already introduced in other presentations was QuickAssist technology, or it can be programmable like FPGA. And there we need to optimize server for high amount of traffic coming from the outside of the server getting in and then also between the virtual machines. If they're chained, then also we have east-west traffic. So, on top of it, we don't just have simple basic workload placement. We need to have a proper layer that does the orchestration, which needs to be both platform aware so that it knows how to recognize all the platform configurations and appropriately place the workload and also needs to be service server, meaning that if service consists of multiple elements, it needs to know how to instantiate them, configure them correctly until the whole service is available. And some of the differences to IT environments are, for example, we need much more deterministic performance. We have much stricter key performance indicators on these high throughputs and also, on low latency and control jitter[O3]. And then we have typically compared to most IT environments in comms vertical, we have much higher availability requirements. And then the resiliency both on complete failure or on the platform impairments. And then there are additional being regulated vertical. We have regulatory environments. Geolocation needs to be well determined, and this makes it generally, both technically, and how it gets implemented in processing organizations-- very different type of environment for data plan workloads. So, we could go and spend quite a lot of time optimizing all of these layers. So, what after many years of practice came up as a conclusion was optimizing where needed and abstract where possible because putting proper abstraction between those layers makes onboarding easier, Llifecycle management over years of production becomes easier. And then still, a number of cases, we will need to go and do very detailed optimization of those stakes. NOTICES AND DISCLAIMERS", "label": [[674, 688, "Virtualization"], [701, 713, "Virtualization"], [319, 334, "Virtualization"], [994, 999, "NFV"], [1165, 1168, "NFV"], [1209, 1220, "Virtualization"], [2992, 2995, "NFV"]]}
{"id": 22, "data": " So, those mentioned several configurations are being built out of volume ingredients and we always use processors and network adapters for these types of workloads. So, now we are using normally Intel Xeon scalable 61, 52, or 38 for data plane or then we are using some for control plane and other usual IT workloads somewhere in the middle of the range of the available core counts and appropriate chipsets that are being delivered in those servers. For Ethernet controller now, current one being used is 710 series, and this allows connectivity of 10, 25, and 40 gigabits on Ethernet ports. It allows multiple ports per adapter, and it is now very stable in production for quite some time adopted by all the usual server manufacturers. In some configurations where we're looking for either more input/output operations per server or we are looking for reduced annual failure rates, we are using solid state drives, instead of normal hard-disk drives, and they come in different categories. They are the SATA ones. They are the PCIe ones. And they are the obtained PCIe ones. So, and then depending on the workloads requirements, sometimes it makes sense to put acceleration cards that can accelerate both the software platform and the application itself.[O2] And here example is fixed function accelerator like QuickAssist technology. And to make this whole software platform also optimized in software, we have different libraries for this hardware acceleration and other algorithm primitives like data plan development kits or QuickAssist patches are available in all the right communities and products. And then also, for storage, we have a storage acceleration library.[O3] So, let's now go in each of these ingredients and introduce which versions of different hardware products that we're using. Over time, we transitioned the Intel Xeon E5 and E7 series into Intel Xeon scalable processors normally for data plane and higher end of control plane, we're using the gold product line. Rarely, it's being used with platinum product line that, like previously, E7 compared to E5 is priced higher. And this is for normal dual socket servers that consist the volume of IT in cloud hardware, and this was by the original definition of  NFE to run the volume hardware. This is how we blended that and deployed most of it. And then we have system on the chip type of products that are used, for example, for universal CPE on-premise environment that are also virtualized, running their workloads. For the Intel Xeon scalable processors in communications workloads, for data plane, primary value comes from higher I/O bandwidth, 50% compared to CPUs and Also, higher memory bandwidth, also 50% so just by running the same workloads on the current platform compared to what was with Xeon E5. It's simply packets are simply passing more easily through it, So, we get much faster data rates on it. And we also have the crypto and compression acceleration in some of the version of the chipset. Or if not available in the server chipset, it can be added on the adapter on the PCIe card. And then we have also other platform ingredients that are improving overall the workload situation on those platforms. So, regarding Ethernet adapters, we have here sample of those that are being produced by us as either single port, dual port, and quad port on different connectivity and protocols. So, you can see here, for example, support for 10 gig, 25 gig, and 40 gig Ethernet ports. Those are Roman numbers. So, for example, X means 10. XXXV means 25 gig Ethernet port, so that's easy to read. And then we have also these type of Ethernet controllers available on products that are coming in blades. In most cases, this is also still produced by Intel but for according to the spec of appropriate server OEM blades. For QuickAssist technology, this is accelerating encryption and compression workloads. Here is a list of all the relevant encryption workflows that many are mostly here listed used in the comms workloads. And the way how the technology is offered is either as a separate chipset where this can be added in, for example the storage devices, we have designed bins for that.[O4] Or now, this is included in some of the server chipsets. It also is available as PC Express plug-in card so that normal rack-mounted servers can also have it, and this is certified with most of the usual IT server OEMs-- this type of PC adapters. And then you also have system on the chip version. So, integrate that together with everything else that is coming on the same piece of silicon. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[2459, 2470, "Virtualization"], [2505, 2535, "Intel Xeon Scalable Processors"], [3809, 3831, "Intel QuickAssist Technology"], [1868, 1900, "Intel Xeon Scalable Processors"], [1314, 1338, "Intel QuickAssist Technology"]]}
{"id": 23, "data": " [O1] The concept of platform awareness required for correct placement of data plane workloads we call Enhanced Platform Awareness or shorter EPA.[O2] And here is an example of physical server layout that because the volume is with dual socket servers, in this case, we have it here. And the way how the different buses and ingredients are connected is that we have two NUMA nodes and BCA buses connected to the appropriate NUMA nodes and memory equally so as per the definition of NUMA. And then this being the physical layout of the server, now, regardless of which workload placement we use.[O3] And originally, all the patches were done for OpenStack. In the meantime, all OpenStack distributions that are relevant are including it. So, it started initially with Wind River and then all the other relevant distributions like RedHat and so on included it. Integrated OpenStack has the required pages to understand how to correctly place data plan workloads on it. And now, recently, we also did similar patches for Kubernetes environments for bare metal where you have here example of good placement where the workload is pinned to the appropriate cores on the server in the green here. And memory is accessed using huge pages on the same NUMA node. And then also the traffic is-- data plane traffic is fed in and out over the adapters that are connected to the PCIe buses on the same NUMA node. If we don't connect the adapters onto the same NUMA node, we need to pass the interconnect. And even way below the bandwidth limitations or interconnect, the way how the environment is really highly tuned, we will already have packet drops, and this is simply not the way to place the workloads for a data plane-- high throughput. Equally so we need to have the memory configure it with huge pages and assets. On the right, NUMA node not passing the interconnect-- again, going to the other side of it. So, this requires that the server has what we call balance I/O, meaning that adapters can be physically connected to both sockets, and this is something that more and more servers are having. But it's still worth checking because there are some of those that initially were used only for IT workloads. It worked perfectly well for control plane, but it cannot really do cost effectively the data plane ones.[O4] So, lists of features that need to be taken care of in placing the workloads include a number of cases for connectivity configuring Single Route I/O Virtualization or SR-IOV, configuring huge pages, understanding the NUMA nodes, and also building virtual CPUs two cores. And it can also include further features of hardware and software. Some of them are listed here. So, CPU models to be chosen with the right instructions, taking care of the caches, and the buses in the server, virtual switch, real-time features, trust execution technology, and similar ones. Long time ago, we measured what is the impact and came to the conclusion that for data plan traffic, this simply needs to be done and also came very early to further conclusions that workload placement-- every workload placement needs to be aware of this type of physical layout and that simply descriptors of those functions need to announce the requirement for that. And then the lowest layer of the orchestration stack needs to be aware of those descriptor requirements and hit appropriate APIs on the Virtual Infrastructure Manager layer. So, this is, for example, OpenStack API is exposed, but they need to be appropriately consumed. Or now when we have similar in Kubernetes environments, also to be consumed over there. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles.", "label": [[1017, 1028, "Kubernetes"], [2463, 2477, "Virtualization"], [3547, 3557, "Kubernetes"], [2481, 2487, "SR-IOV"]]}
{"id": 24, "data": " For systems that require high number of input/output operations per server or lower annual failure rates, we are using SSDs, and SSDs generally fill the void that existed between system memory and hard disk drives, traditional ones, where CPU's caches were, of course, the fastest from the access time than it was memory. And then there was the big gap to get to the hard disk drives. And they were first initially based on 3D NAND technology filled with SSDs at that time SATA and faster PCIe1s. And recently, we introduced Also, the update SSDs, which have much faster response time. And now, most recently, we introduced also obtained memory.[O2] Both of those obtained SSDs in memory are based on 3D XPoint technology where one is coming in the form factor of SSDs, and other one is coming on the DIMM slots. And also memory can be made persistent. So appropriate positioning for different type of workloads is presented on the slide here where on the top, we have obtained SSDs, which are much faster in IOPS in access times. So lower latency has higher quality of service and are also longer for endurance. And then the 3D NAND-based SSDs, PCIe-based one in the middle-- they have higher capacity. And then we also have the where needed where we still want to have SSDs but don't need these type of best features in category.[O3] We can also made it on the SATA-based ones. And an example of obtained SSD drive on PCIe interface, in this case here, is P4800X, as in marketing terms, the most expensive data center SSD. [O2]Please merge these two subtitles. [O3]Please merge these two subtitles.", "label": [[119, 124, "Intel SSD"], [130, 134, "Intel SSD"], [456, 460, "Intel SSD"], [543, 547, "Intel SSD"], [674, 678, "Intel SSD"], [765, 769, "Intel SSD"], [979, 983, "Intel SSD"], [1141, 1145, "Intel SSD"], [1272, 1276, "Intel SSD"], [1408, 1411, "Intel SSD"], [1521, 1524, "Intel SSD"]]}
{"id": 25, "data": " Welcome to our session on introduction on single route I/O virtualization-- shorter, SR-IOV. For beginning, let's just say that when servers are physical and connected to physical network switch [O3] which, in this case, for example, ethernet cables-- we have 1:1 relationship between application and the hardware. When we virtualize those environments, then there is one physical server connected to physical switch, and then there are multiple virtual machines running. They think they are talking to some adapter. But this adapter, in case of virtual machines, is also virtualized. And here in example is where inside the host operating system there is virtual switch exposing different ports to different virtual machines. To connect virtual machines to network we have different methods. We can directly assign network interface card to the virtual machine. Or we can also expose the same card to multiple virtual machines. Or if you're using virtual switch, then we are exposing virtual interfaces to the VMs. Direct assignment is one of the methods where the whole network device is given to one virtual machine. So, this means that the virtual machine will own fully this network device. The data is sent from the VM directly into memory using direct memory access. This is one of the functionalities provided by Intel virtualization technology for direct I/O, or VT-d.[O4] And this allows for near-native performance due to the connectivity being direct from one VM owning full network device. And in this case, then, of course, device cannot be shared with other virtual machines or be used for work by the operating system because it is fully assigned.[O5] This is not the most flexible setup. And this is why PCI-SIG, which is a peripheral component interconnect special interest group-- at that time led by Intel and a number of other companies-- defined the standard called single route I/O virtualization and sharing. Normally, we abbreviate it only to SR-IOV, where the objective is to expose such PCI device as looking to the virtual machines then supported by the server BIOS and hypervisor with this type of virtualization technology for direct I/O, or VT-d, which allows it to be kind of partitioned and exposed as multiple devices to all the different virtual machines. In this way, we will be sharing the device. To define that, there is a physical function which needs to be addressed. And over there, we can then define multiple virtual functions. And then, those virtual functions are exposed to the virtual machines so that it talks directly to virtual function, and by this way avoids the overheads of using NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please delete. [O3]Please merge these two subtitles. [O4]Please merge these two subtitles. [O5]Please merge these two subtitles.", "label": [[60, 74, "Virtualization"], [86, 92, "SR-IOV"], [324, 334, "Virtualization"], [572, 584, "Virtualization"], [1328, 1342, "Virtualization"], [1906, 1920, "Virtualization"], [1969, 1975, "SR-IOV"], [2128, 2143, "Virtualization"]]}
{"id": 26, "data": " [O1] A summary of mentioned hardware ingredients and related software ones, we can see that most of the data plan configurations and control plane configurations that we have today fits very nicely on Intel Xeon Gold type of CPUs. Where I was mentioning those 20 cores for data plan, we can have the middle count, of course, like 12, 14, on Xeon scalable for control plane and other controllers and managements and IT workloads. And then, we will put appropriate number of ethernet controllers for data plan on balanced I/O, meaning feeding both sockets. And if the workloads also require higher amount of I/O per seconds, then we will start putting more and more SSDs into the nodes. And we also have option of using the quick assist accelerator to accelerate some of the encryption algorithms. So, this is summarized here on the left side in this diagram. So, we have the server configuration down there. The hypervisor is then going to allow running multiple virtual machines on top of it. Some of the hardware ingredients if used, for example, Quick Assist here or few of the other ones or the ethernet controllers also can be exposed through SR-IOV directly through the virtual machine, or hypervisor can also have virtual switch that then needs to be optimized with Data Plan Development Kit feeding packets over the virtual switch or virtual router, for example, for control into the VMs. And the applications there to be efficient to data plan also then need to use Data Plan Development Kit or other mechanisms that increase packet processing. To have this whole stack properly configured, we need to use the concepts like enhanced platform awareness so that we recognize the physical layout of the server replace the workloads accordingly. And then the whole orchestration stack is going to configure all the elements and put them as needed into the service chain. The important concept here is that, as there are many layers of the stack, the higher we go, the more abstracted it should be, and expose minimum that is required for performance reasons. And because here we talk about high amount of data plan throughput, in a number of cases, it turns out that there is quite a lot that needs to be exposed and configured, and to get to fully automated environments, it is quite some effort. And we are handling this in both ways. One is getting smarter and faster in handling it the current way. The other one is investing in decoupling those layers so that the on-boarding and the utilization of these type of environment becomes easier and we start moving towards more of the cloud environments.", "label": [[665, 669, "Intel SSD"], [1148, 1154, "SR-IOV"], [2494, 2506, "Utilities"], [1633, 1660, "Enhanced Platform Awareness"]]}
{"id": 27, "data": " In more details in SR-IOV architecture we have hardware virtual ethernet bridge, which is where all the physical and virtual functions are being connected. Physical functions have physical function driver exposing them as a PCI device to the OS. And then, virtual functions have equally so virtual function driver exposing them into the higher levels of the software stack, so to the operating system there. And the way how this thing is really stitched together into the robust tech is that there is Intel virtualization technology for direct I/O-- or shorter, VT-d-- which is providing the functionality of mapping I/O devices to the virtual machines. It is also doing the direct memory access and interrupts remapping[O2]. And it improves reliability and security because it does the isolation of the data parts for different functions underneath. The benefits of using SR-IOV is that a single physical device can be exposed as multiple virtual devices to the OS, and then to the virtual machines. And this is all based on PCI-SIG specification. And we get to near-native performance between the virtual machines and the external ethernet ports because this is completely bypassing the hypervisor. It does protect the data with isolation of Intel VT-d. And it does the automatic receive and transmit load balancing with round-robin function scheduled with also bandwidth limiting transmit function where needed. And each virtual function, that gets dedicated resources, its own receive and transmit buffers, and its own descriptors. This way, we can, between virtual machines for the traffic, implement different connectivity scenarios on the same physical host. So, the traffic, that is then called east-west. We can do with virtual switch and with virtual functions. Inside the adapter then also switched, there is between different hosts, So, traffic exiting physical server. This is North-South, also supported through virtual switch and through virtual functions. And then, you can combine the two into traffic that runs inside the server. NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles.", "label": [[19, 26, "SR-IOV"], [508, 522, "Virtualization"], [874, 880, "SR-IOV"]]}
{"id": 28, "data": " In more details on how SR-IOV gets implemented here on example of Intel Ethernet Controller from 710 family is the adapter itself has number of supporting functionalities. So, there is virtual Ethernet port aggregator. There is virtual Ethernet bridge. And there are then physical functions and virtual functions. Number of supported virtual functions per device is 128. And then, appropriately, either over virtual switch or connected with SR-IOV, we will have appropriate number of VMs connected. Logical equivalent is if we are doing SR-IOV in containers. And there, we have virtual station interfaces connected to --- to those virtual functions. If we look at how network traffic would be flowing when using virtual function direct assignment to the virtual machines, we will have examples starting w-h-- on the same host. So, for East-West, we can get-- using 40-gig adapters because of the PCI limitations, we can get on dual port adapters to two times that much. But let's say that we get to 44 gigabit per seconds. And the numbers will, of course, vary per packet size and-- but this way, we can then-- inside the adapter, until we reach the bandwidth limits, we can get this thing through the virtual Ethernet bridge. When we do more of that, it is eating into the same aggregate bandwidth. So, the numbers get smaller when we are starting to chain more functions there. And eventually, as we continue chaining those functions, the numbers get further smaller because the aggregates needs to stay the same. And when we have, like, six virtual functions utilized and, between them, we are only sending-- in this case, instead of 40-something, we are sending 6-point-something gigabit per second. When we start also combining this thing with traffic leaving the server, then from-- in this example here, we have one physical and four virtual functions used. The transmit is used for the-- only for the default traffic of netperf. And the VMs are talking between themselves, for example, with 11 gig. But then the external traffic going out is then also 11 gig. And the rate of this external traffic, by design of the adapter, can be limited by using low-level tools, like given here in the example. And here, it's important to note that, while the adapter itself-- and firmware has a lot of functionality-- we also need to look at how well-supported this firmware is and can we get these type of adapters from what is the normal adoption channel, where, normally, communication service providers will be  buying from the OEMs and the telco equipment manufacturers. What is available in this type of firmware there? What is supported in drivers that are being used for different environments? And then also very important is to have the whole stack fully managed and automated. What is supported in the desired software-defined networking solution?[O2] So beyond just pure functionality of the adapter, it's important to look at the whole stack because only then we can fully automate it. [O2]Please merge these two subtitles.", "label": [[24, 30, "SR-IOV"], [442, 448, "SR-IOV"], [538, 544, "SR-IOV"], [1563, 1571, "Utilities"]]}
{"id": 29, "data": " Looking at how ethernet controller 710 series implements different network virtualization and NFV models, we need to look at, in particular, control parts and data parts and the concept of network virtualization edge, which on the underlay of the network sits and uses the underlay to connect physically to the real network while exposes its parts to the virtual machines. And so, on the underlaying, the layer 3 is being used. And to the virtual machines it talks real internet frames. And this is how pretty much the VMs think they are having the real network. And this NVE can be implemented in virtual switch, in physical switch, and also in other parts of adapters here. So, we have here Model 1 is for kernel data paths. It is not being optimized with DPDK, so, it would work for a very low rate ofpathsonly. The second one is using more data paths where virtual switch example here given is obvious. It can be also other virtual switches or equivalent virtual routers like VMWare, Contrail or similar optimized with DPDK, where then NVE is providing this type of functionality inside the virtual switch, and then also in the real switch. And all this needs to be properly configured with some goods as the end controller is supporting that example here is with OpenDaylight. But then, also OpenStack has appropriate interfaces for it. We have Model 3, which includes the bypass and the trusted VNFs. So, this is where we're exposing those virtual functions with SR-IOV and bypassing the hypervisor virtual switch for data traffic. And then, inside the VM, directly mapping it with appropriate drivers to the virtual functions of the adapter. And then, we have the fourth model, which is also using the bypass. And then, the difference is that NVE concept on the third one is sitting inside the VNF. NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[76, 90, "Virtualization"], [95, 98, "NFV"], [198, 212, "Virtualization"], [759, 763, "DPDK"], [1024, 1028, "DPDK"], [1470, 1476, "SR-IOV"], [1402, 1405, "VNFs"]]}
{"id": 30, "data": " [O1] In summary, SR-IOV is used to directly connect virtual machines to the adapter, and this way by passing the hypervisor. The drawback of doing that is that in VMF, we have the driver as hardware definition of the adapter. So, it brings challenges in lifecycle management over multiple years of production. And this is performance trade-off to using the virtual switches, that for data plan would require a certain amount of CPU course to be used, but would provide proper nicer obstruction of what they owe to the VMs. As we are getting more and more optimized virtual switches and virtual routers with BTK, we expect that for what's in NFE we are doing with 10 gig and 25 gig ports, we will be using more and more of those virtual switch solutions, compared to what, up to now, most of the deployments for performance reasons we're using SR-IOV. So already mentioned, primary benefit is that we are avoiding the virtual switch. This also has drawbacks because virtual switch is providing quite a lot of functionality when it comes to network virtualization with overlays. The data protection is coming by isolation with Intel VTD. Transmit load balancing is having round robin scheduling for our recent takes with bandwidth for transmit rates can be limited per virtual function. And then also, per virtual function, we are getting dedicated resources, transmit and receive buffers and cues, NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[18, 24, "SR-IOV"], [844, 850, "SR-IOV"], [1048, 1062, "Virtualization"]]}
{"id": 31, "data": " Welcome to our session on Intel QuickAssist Technology. This technology is, in short, supporting number of encryption and compression algorithms. In particular, for encryption, it supports cryptographic ciphers and authentication for symmetric encryption, and then also for public key cryptography it supports asymmetric cryptography and key protection. In addition to that, it also supports the compression algorithms that are lossless for data in flight and data at rest. In particular, the list of algorithms supported for symmetric cryptography acceleration is many popular ciphers and then also many authentication mechanisms. For public key acceleration, also the most popular ones are supported. And then, for compression acceleration, we have DEFLATE and Huffman algorithms. QuickAssist Technology is coming in hardware in three different ways. It can come as chipset, which is either a separate chipset or part of the server chipset in some of the recent generations of platforms. There, it connects to CPU via onboard PCI-e lanes. Another method is to put this chipset onto PCI-e adapter. And this one will then also connect with PCI-e lanes. And the third method is to integrate it in system on the chip, like, for example with Xeon-D or Atom-C series. The application of QuickAssist Technology is in different domains. So for cloud, we can secure networking. We can use key protection technology with platform trust technology. It can help in VM migration and also for the database security. For networking, typically we use it for encrypting these type of communication protocols that are supported, secure routing, web proxy, WAN optimization, again, key protection technology, and then also in accelerating wireless infrastructure. In big data usages, we have patches for Hadoop that improved the performance there, and then also for secure data transfer. And then, for storage we have a number of design wings for the chipsets because it helps to both encrypt it and also to compress it in storage devices. Example of how it works on IPsec use case is that there is packets arriving that goes into the processing to handle the packet descriptor. The security associations are being looked up. And the packet is then queued for the cryptoacceleration on the QuickAssist. The accelerator takes the, in this case, incoming encrypted packets and performs the decryption of it in hardware, and then does direct memory allocation so that it gives it back directly into memory. And then, of course, retrieves the non-encrypted version of the data. And then, it can also be transmitted directly out of the server. So this is how IPsec decryptions work. Equally so, we can do it the other way around to encrypt the packets. NOTICES AND DISCLAIMERS", "label": [[33, 56, "Intel QuickAssist Technology"], [190, 203, "Cryptography"], [286, 298, "Cryptography"], [322, 335, "Cryptography"], [537, 549, "Cryptography"], [784, 807, "Intel QuickAssist Technology"], [1284, 1306, "Intel QuickAssist Technology"]]}
{"id": 32, "data": " QuickAssist Technology in current generation of servers that are based on Intel Xeon scalable processors can either be part of the server chipset with some of the comms focused ODMs, in particular, or more applying to the IT OEMs, we have qualified the adapter version of QuickAssist coming on PCI-e cards. So then, it can be slotted into the appropriate slots and accessed by what is, in this case, over PCI-e devices. It's going to expose SR-IOV virtual functions to the virtual machines. Here are two examples of the cards. One is produced by Intel. We also have design winds that this type of chipset is coming on adapters produced by other vendors. And it supports, in different versions, 50 gigabit per second of, for example, IPsec or 100 gigabit per second, depending on which versions of chipset and adapter is being used. In terms of software, there is quite a lot of adoption. So it is supported in operating systems for various Linux distributions, for Windows, for FreeBSD. And hypervisors like KVM and VMware can expose SR-IOV virtual functions of QuickAssist's engine to the virtual machines. And then we also, with Kubernetes, we have device plugins in different projects for cryptography. This is exposed. Very popular is, for example, to use it through OpenSSL. The way how it works is the application is using normal OpenSSL APIs and then the right version of OpenSSL is going to recognize if there is a hardware engine underneath, start using it. And if it's not there, then it's simply going to continue working as before using CPU cycles on the cores. And we have it in Linux Kernel, in patched different web servers like Apache Web Server and nginx. It is used for SSL reverse proxy and for a number of other usages. Some examples are given here. And then, we have it also in appropriate frameworks and applications for storage big data and for compression. Examples are the cryptoframework in Linux Kernel, file system compressions, parallel file compression, and then also Hadoop environment. NOTICES AND DISCLAIMERS", "label": [[1, 23, "Intel QuickAssist Technology"], [75, 105, "Intel Xeon Scalable Processors"], [442, 448, "SR-IOV"], [1035, 1041, "SR-IOV"], [1132, 1142, "Kubernetes"], [1193, 1205, "Cryptography"]]}
{"id": 33, "data": " Performance of QuickAssist Technology is measured on all the target workloads. And examples given here are for security benchmarks on OpenSSL, how many RSA 2K decrypt operations per second it can do better than without hardware acceleration. IP Sec forwarding and SSL WebProxy throughput is also significantly improved. On big data benchmarks with compression, how software Snappy can do compared to QuickAssist Engine. And for compression ratio of big data example, also we get much better compressions with QuickAssist compared to what their default, Snappy, is doing. Primary value prop of QuickAssist is that we use the engine for accelerating compression encryption algorithms. This way, we save the CPU cores for work which is revenue-generating versus cranking security or compressing data. And here are given examples for cipher and authentication, for public key, compression. And you can see that, in many of these examples, significant savings on the CPU core side is allowing us to cost-optimize this type of environment because the engine itself in different integration of hardware is way more cost-effective than spending a lot of Xeon cores. And still, those Xeon cores we can then use for revenue-generating parts of the workload and services. To proceed with that, there are three iterations of hardware that one can get hold of. Study available resources starting with Getting Started Guide. And this way, proceed with further ecosystem enablement or with early deployments. NOTICES AND DISCLAIMERS", "label": [[16, 38, "Intel QuickAssist Technology"]]}
{"id": 50, "data": " Part of QuickAssist functionality useful for asymmetric cryptography is key protection technology. And the way how it works is that, when we are doing, for example, secure websites without key protection, the key needs to be given to the web server. And this key then, of course, can be used for QuickAssist acceleration underneath. But the issue is that, in virtualized environments that are hosted at third parties, well, they could get hold of such keys pretty easily as a hosting service provider on it. And depending on what is being protected with these type of environments, the trade-off might not be acceptable for that. So with key protection, the key that initially was in PEM file now is in wrapped version of PEM file. So different APIs are used. Software was appropriately paged for that. And the key is then not being exposed to the web server directly this way. And the key is then only unwrapped in hardware underneath in QuickAssist engine. So the way how we get to that in step is that, first, with the key protection technology, we first need to generate the key itself. And then, we need to generate symmetric wrapping keys based on that. And this way, we can go to secure key transfer where remote admin provisions this wrapped secure key on the key protection technology with the platform trust technology so that it then sits safely inside the QuickAssist engine. And then, the run time usage is that this is being only there in the chipset itself and not being exposed to other software. So while this concept is there in hardware and in open source software, like anything else, we need to get to a commercially deployable product. So some of the solution enablement still needs to be completed for this. NOTICES AND DISCLAIMERS", "label": [[57, 69, "Cryptography"], [359, 371, "Virtualization"]]}
{"id": 51, "data": " After covering NFV definitions and the benefits, let's also talk about, how does it get deployed. What are the different options for that, and what the capabilities required after choosing one of the other options? So here we have a point of view from Orange, a couple of years ago. Nicely visualizing the difference, what is being called steps. But those are different deployment models. And on one side, on the left, it starts with vertically integrated, where the model is being consumed the same way, where purchasing operations is handling the same vertically integrated system, which now has a hypervisor. It does bring benefits of software upgrades. It allows to move in direction of automated, but does not decouple between multiple vendors, and having all the different layers fully distribute the network, as some of the other steps. So, step number one here is where the NFV infrastructure is different from the VNFs on top of it. This requires VNF onboarding effort. Step two here, in the middle, is, at that time, with additional orchestration on top, is what Orange deployed a couple of years ago for business services. And this has normal volume servers and switches, with hypervisor. And then, the third part, the VNFs on top of it orchestrate the video orchestration layer. In this case, it was not represented here. But that's how it got deployed. And then the further desegregation, as it goes, is, of course, more desirable for their Comm Service Provider to take advantage of the virtualization, transformation. Also, to introduce new vendors in the ecosystem, and to decouple those previously vertical systems. This includes adding the NFV orchestration layer. Also, in the right layer, it's ways where it can also have a generic VNF manager. And then it goes into fully distributed network, where the orchestration can, depending on the requirements from the VNF, typically, it's somewhere in the VNF descriptor, simplified, can decide where exactly to instantiate this type of functionality in the whole Telco cloud, distributed. So, going further to the right requires way more studying and skills, and help from the right partners that are willing to have this type of model implemented. And it requires, also, very big changes in the procedures. So this means, what is here represented Operational Model needs to change. So, from managing Silos, to managing layers, is typically how it's said. And also, the purchasing needs to change, because it's not the old model, buying all from the same vendor. Now it's being, using IT equipment inside the carrier network, with some of the IT virtualization vendors. And also, it can have third party VNFs. And for the customer facing services, also, the sales procedures are changing. And required capabilities for all that? Here, we simplified those five models into the three ones. On one side is vertical, integrated by telecommunication manufacturer. In the middle is integrated platform, with onboarded VNFs. And on the right is everything layered and multi-vendor. So required capabilities for that are, there are some that fit into the baseline category of capabilities. This has to be convinced on the overall value of this type of transformations, be able to sort the hardware configurations and connectivity options for data plan, transcoding, eventually some of the hardware acceleration. Third is to have software applications that are at some stage of friendliness here. We call it cloud radius. So this means that they're virtualized. They can scale up and down, to a certain extent. But they don't have to be perfectly decoupled from the cloud platform, decomposed, and so on. And there needs to be skill of building additional data center capacity, or there needs to be such capacity in the already provided data centers. So, there's the baselines, regardless of which deployment model is being used. And then the capabilities required for this vertical integrated model will typically come from the Telco comm manufacturer, who is providing this vertical integrated stack. They will help with financial and planning tools, sizing characterization data provided, and integrate it into the current environments of IT and OSS/BSS. So this is the easiest way to deploy a number of communications service providers. After realizing what all the capabilities at a given point in time, decided to go with this type of model. So, the model where their platform is integrated, and we need to onboard the VNFs, is, from the communications service providers, or the appropriate partners who are willing to complement this type of model, requires, additionally, on top of those baselines and basic financial tools, requires getting to the right balance between the separation and integration costs. These types of financial planning tools, for revenue cost sizing, is for multiple use cases on the shared infrastructure. Now, because of the VNF onboarding effort on this platform, people start to become aware of the VNF onboarding methodologies, requirement for other common data model that is going to help in this type of effort. They start to experience more of the artifacts of the VNFs, so they start asking for those to be resilient of what are the impairments in the cloud platform. Also, to be decomposed into the functionalities in software modules. They start looking at multi-vendor orchestration, with integration into the current IT OSS/BSS systems. They need to figure out how to operationalize this multi-vendor ecosystem. And at this point in time, these are already enough capabilities and the change requirement Is really worth addressing organizational culture changes that they're going to further this whole transformation, and make it successful in this case. So, for those that still want to go further and inside the platform itself, make it multi-vendor and layered, there is additionally required to make this balance between the separation and the integration costs in the platform. And also, the TCM models will have to be built for the multi-vendor NFV infrastructure. And now, this multi-vendor NFV infrastructure needs to be integrated into existing systems, and operationalizing also on the level of the multi-vendor. So this is a slide that was created to help reflect on the required capabilities, to get to the right planning and resourcing, especially to fit it in time, on how the upscaling, or the partnering, is going to go during this transformation. And it's also for this planning on where to invest in CommSP old skills, or where to partner with the right ecosystem vendors that are capable of bringing this type of pure play system integration, it is typically what we are talking about, and bringing complimentary business models to make this whole thing successful. This is also helping those that might not have these types of capabilities, to realize it faster, and still do what this vertical integrated model that can be done now, and is, in terms of required capabilities, much NOTICES AND DISCLAIMERS", "label": [[15, 19, "NFV"], [882, 886, "NFV"], [923, 928, "VNFs"], [957, 961, "VNFs"], [1231, 1235, "VNFs"], [1659, 1662, "NFV"], [1752, 1756, "VNFs"], [1883, 1886, "VNFs"], [1920, 1924, "VNFs"], [2669, 2674, "VNFs"], [2978, 2982, "VNFs"], [4482, 4486, "VNFs"], [4916, 4919, "VNFs"], [4992, 4996, "VNFs"], [5161, 5166, "VNFs"], [6054, 6057, "NFV"], [6101, 6104, "NFV"], [6521, 6527, "Communications Service Providers"], [4501, 4533, "Communications Service Providers"], [4264, 4297, "Communications Service Providers"]]}
{"id": 52, "data": " With NFV and other transformation applied to the carer network, let's also cover what is the intended business outcome of all that. So, the high-level goal of the virtualizing automating is overall digital transformation. In the communication service provider, key prerequisite for that is to transform the carer network. So on top of that, is also to build all the other channels to become data driven provider for smart roles so that service is addressing it, on demand-- very needed. Self-provision is needed, trusted by exposing all the right platform capabilities up to the brand, and providing these types of all connected experiences and having innovative workforce. So those are all the required prerequisites for successfully doing this type of digital transformation. And critical parts in this is to have it fully done on the culture or its procedures. So an example of this is to look at the metrics that matter, so key performance indicators, example of some our customer self provision services. How many of that is delivered from the cloud platforms? How many of them of that is truly digital revenues and initial KPI that is very useful is that particular service furthering desired organizational and culture change? So, this will then contribute to the overall successful digital transformation. Another view on similar, a little bit more detailed here with a picture from HP webinar is the journey of multiple transformations to do. So in the past, we talked about connectivity provider using the existing environments as they are, going through the step of Telco Cloud with on demand services, automated on flexible infrastructure into this digital service provider that has diversified those services and has business agility. And there is a lot of benefits from that to be achieved. Type of partners that are required to work with will be the ones who enabled similar transformations in IT and cloud environments that have complementary business models to the communication service providers and will help CommSPs upscale their people and will have also positive effects 49 50 NOTICES AND DISCLAIMERS", "label": [[5, 9, "NFV"], [230, 260, "Communications Service Providers"], [1983, 2014, "Communications Service Providers"], [2029, 2036, "Communications Service Providers"]]}
{"id": 53, "data": " So, in this section, we're going to talk about what management and orchestration means in an NFV context, and really what it means to a Telco. Here, we're going to look at the three major layers of management and orchestration, which by the way, usually get shortened to MANO, which is probably how I will refer to it from here on. And we'll map those three major layers to the NFV reference architecture model that was proposed by ETSI in one of the early NFV white papers back in 2014. So, MANO describes a very specific architecture, consisting of three main layers. Those layers consist of an entity called an NFV Orchestrator or NFVO, which is primarily responsible for resource orchestration, a VNF Manager, which has more of an application or VNF centric view, and a virtualized infrastructure manager, which is more commonly known as a VIM. So, taking these layers from the bottom, let's start with the VIM. Probably the most common example of a VIM, at least from the open source world would be OpenStack. And by OpenStack, I really mean the core components of OpenStack, such as Nova, Newton, and Glance. The VIM is responsible for controlling and managing the NFV infrastructure by either compute, storage, and networking resources that the VNFs are going to be deployed on top of. It's the entity responsible for the allocation, the upgrade, and release of resources. And it plays an important role in the capture of telemetry from the platform. Next up the stack is the VNF manager or VNFM. This is responsible for the lifecycle management of VNFs, and it will include responsibility for actions, such as configuration, start, stop, scale up, scale down, those types of functions. Lastly, the NFV Orchestrator or NFVO is responsible for NFV infrastructure or orchestration across multiple domains. That could be across multiple sites, for example. Or it could be across multiple different VIMs. And it's responsible for higher level actions such as rolling back an entire deployment. So, these three layers together are what make up what is typically known as NFV MANO. But to complete the picture, we need to briefly describe some other layers that come together to create a complete solution. Now, these were mostly held to be out of scope for the original definition of MANO, as proposed by ETSI, but they remain important components of a complete solution. So, at the top, we have the Service Orchestrator. Now the Service Orchestrator works at a higher level, and it's focused on the end-to-end operator view of a service. There is a connection to the MANO layers, low as one of the likely requirements of a Service Orchestrator will be to drive consistency between the resource allocation and the VNF configuration. Lastly is the SDN controller layer. Originally, this was also considered to be out of scope by the NFV standards organizations. But that's somewhat changed over time. The SDN or Software-Defined Networking controller is, of course, responsible for network management. It's also responsible for the ability to do fine-grain control of network resources, to enable things like service function chaining. It's responsible for the network underlay and overlay management and for the debug of those network components. And the picture here shows three possible deployment points for an SDN controller in an NFV architecture. For example, as in example one, it could be deployed as part of the VIM where neutron in OpenStack would be a good example of this deployment model. So, moving on, let's consider the purpose of MANO and really think about what problem statements it's trying to help solve. So, if we try to identify the steps that an application or VNF goes through during its life cycle, we can maybe simplify things as in this diagram. A VNF vendor will need to design and develop and deliver an application to their customer. And then the next step really is the onboarding of that VNF into the customer or service providers' environments. The creation of an entry in the service providers' service catalog and various testing and validation steps to ensure that the VNF will run the infrastructure and perform to its required key performance indicators. So, this really is the point at which MANO starts to come into play as a service provider on board a VNF into their environments. After the onboarding process, the service provider will need to instantiate or do a Nova boot of that VNF for the first time. And then we move into an operation phase, where configuration policy will be applied. The next major step in the life cycle is once the VNF is being configured and is moved from its day one state into its day two running states. We need to continually assure that he is continuing to meet its key performance indicators. And as these VNFs typically will live in the network for a long time, there will be a maintenance phase where upgrades and patching will need to be applied. And then finally, there will be a phase where the VNF is being retired and the resources need to be returned to the pool and cleaned up properly. So, although this is a high level and simplified view, I hope it gives a picture of the parts of the lifecycle of a VNF NOTICES AND DISCLAIMERS [O1]Please delete empty timecodes. [O2]Please delete it. [O3]Please delete it. [O4]Please delete it.", "label": [[94, 97, "NFV"], [272, 276, "MANO"], [379, 382, "NFV"], [458, 461, "NFV"], [615, 618, "NFV"], [701, 705, "NFV"], [751, 754, "NFV"], [1172, 1175, "NFV"], [1253, 1257, "VNFs"], [1484, 1487, "VNFs"], [1557, 1561, "VNFs"], [1706, 1710, "NFV"], [1750, 1754, "NFV"], [2073, 2077, "NFV"], [2078, 2082, "MANO"], [2287, 2291, "MANO"], [2571, 2575, "MANO"], [2717, 2720, "VNFs"], [2750, 2753, "SDN"], [2835, 2839, "NFV"], [2907, 2910, "SDN"], [3317, 3320, "SDN"], [3337, 3341, "NFV"], [3550, 3554, "MANO"], [3687, 3691, "VNFs"], [3779, 3782, "VNFs"], [3924, 3927, "VNFs"], [4108, 4112, "VNFs"], [4235, 4239, "MANO"], [4298, 4301, "VNFs"], [4429, 4432, "VNFs"], [4589, 4592, "VNFs"], [4787, 4791, "VNFs"], [4981, 4984, "VNFs"]]}
{"id": 54, "data": " So, for this core network transformation, let's look into how this standard development organizations are contributing to that, and then also number of open source technologies in different open source projects, complimentary to that. For this, I will use Visual, coming from Linux Foundation, being one of the major umbrella organizations for a number of those open source projects, and one of the white papers that nicely visualized how this traditional methodology look like for standards development, where a lot of vendors and leading communications service providers will go and define the standards, implement in some products, get to detailed requirements, get to the architectures out of that, and then feed it into the furthest standard. So, this is how the typical standards of organization look is going on. And this is based on the design principles of this vertical. And sometimes it can really take quite long cycles to get some new, innovative revenue making services. So, what you are looking for here is how to complement this thing with the open source principle, where this is heavily based on the contributions of the code and the architectures and designs. So here we have, on the right side of it, is suction and roll methodology, with contributions into the open source components. They are being built into the reference platforms out of that architecture. So the finds and use cases are being implemented. From that, we see what are the next component is required. And this is how that loop is going on. So, the ideal situation for standards developments in open source is, standard developments are defining some of these requirements, and open-source are really contribution based, implementing that fast. So here, it's another diagram, representing how the different layers in different projects fit together. It's a point of view from the foundation. But it's very visual and representative. So I'll use it here. And layers are coming in the infrastructure management, control, and the services. So, starting from the bottom, there is this segregated model of hard-wired number of projects, like open compute projects, and telecom infrastructure projects. So defining that. Then we have, for I/O obstruction and data pods, projects like data plan development kits is packet processing library, in software used for running networking workloads on common servers. And there we have, for example, if the I/O is another one of related projects, OVS is, Open Virtual Speech is one of those, virtual speeches being used. So operating systems, open one is, obviously, open source is Linux. And then we have a few for the networking. And then we have network control, very popular ones. For example, Tungsten Fabric, which was called Open Contrail until recently. It's the limitation of Contrail. And then we have Open Daylight and Onos there. So, cloud and virtualization management, the major one is OpenStack. And for orchestration, we have projects like ONAP and Open Source Mono. For the network data analytics, it's Panda, inside Linux foundation. And then we have, on top, we have this application layer. So there is also, for continuous integration and delivery, there is this OPNFV, focusing on various aspects of this integration and Automation And then we have, from Cloud Native World, we have Cloud Native Computing Foundation. So, they are also focusing on a number of those layers in normally containerized, decomposed Cloud Native So, this is an overview. A number of vendors and comm service providers will have heavier investment in some of them, lighter in the other ones. But it's also good picture to also think about, which of these are mature enough, at a given point in time, to be used as either fundamentals of the platform, or somewhere higher in the layers. And also, it's good to think about, from where do we derive more value, if we invest in mustering one or the other one of those open-source projects in the implementation from the vendors, and which one would be the wrong one NOTICES AND DISCLAIMERS", "label": [[541, 573, "Communications Service Providers"]]}
{"id": 55, "data": " So now let's take a quick look at two of the common data model languages, firstly YANG. YANG was originally conceived as a language that was used in the network configuration protocol NETCONF. And here, I'm just looking at a small extract from a VNF descriptor data model from the open source MANO project. In this example, I really just want to draw your attention to the way that one of the attributes that a VNF may require is expressed in the YANG language. Here we can see the definition of memory page size. Now, many DPDK applications require large memory page sizes for the performance. Here, in this extract, from the same VNF descriptor data model, we can see the way the YANG language describes the requirements of a VNF to have a CPU pinning policy applied. Again, many high performance data plane heavy VNFs NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[247, 250, "VNFs"], [412, 415, "VNFs"], [633, 636, "VNFs"], [729, 732, "VNFs"], [816, 821, "VNFs"], [294, 298, "MANO"]]}
{"id": 56, "data": " So, in this section, we'll come on and talk about what information and what data models are in the context of NFV management and orchestration. And these really form a part of the onboarding challenge. So firstly, what is model-driven orchestration? Well, it's a powerful approach to onboarding services, instantiating them, and managing their life cycle. A service model or a service template can be created, using a high level of human readable language.[O2] The service model will describe a service's components, the relationship between those components, the dependencies that those components have on each other, the requirements that they have on the infrastructure, and their capabilities. And this model needs to use reusable types or attributes and uses them to construct the service. The Service Orchestrator uses a service model and an infrastructure model to manage a service through its lifecycle. So, I've talked a little bit about or I've mentioned information models and data models. So, let's just briefly review why they are different and what we mean by information and data models. So an information model operates at a higher level. It's a conceptual and abstract model that's predominantly used during the creation of services. And it's really used by service designers and operators. Whereas a data model really operates at a much lower level. It includes a lot more detail and works at the level of describing the attributes, the characteristics, the requirements of a component, and how that component can be instantiated onto some hardware infrastructure. A data model can be considered to be made up of three main components, a set of attributes, i.e., a way of expressing the hardware requirements, an encoding scheme, i.e., the language in which the model is written, and a packaging format. There are at least two common encoding schemes, YANG and tusco, which we'll come on and talk about in a little bit more detail. Now, one of Intel's goals over the past few years and continues to be one of Intel's goals is to try to drive some industry consistency across the industry on these areas, particularly in the area of attributes. And the reason that we have taken this approach and feel this is an important goal is it can significantly lower the amount of effort required on board a VNF into multiple environments if there is some industry consistency in the way NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge subtitle 12 with 11. [O3]Subtitle 37 should include this timecode and stop at 00:02:20, 720 [O4]Subtitle 37 should include this timecode and stop at 00:02:20, 720", "label": [[110, 114, "NFV"], [2317, 2320, "VNFs"]]}
{"id": 57, "data": " So now let's take a quick look at two open source management and orchestration projects that Intel has been very involved in and are very active open source projects in the industry today. These two projects are known as Open Source MANO or OSM and ONAP. Let's start with OSM. OSM or the Open Source MANO project were started back in May 2016 and is hosted under the ETSI organization. Telefonica has been one of the primary leaders of the OSM project, but there are a large group of service providers and vendors who are active members of this project. And version 4 of the OSM codebase was released earlier this year in July 2018. OSM is very well aligned with the ETSI MANO specifications and it follows the data models from ETSI. And OSM really enables a lot of flexibility when it comes to choice of VIMs SDN controllers. Another very active project in this management and orchestration space is the ONAP project. ONAP was formed in February 2017, and it was formed out of the merging of two pre-existing projects, the ECOMP project from AT&T, and a project called Open Orchestration or Open O. ONAP's architecture also maps very well to the ETSI reference architecture. It does have quite a bit wider scope than OSM, and it includes areas, such as inventory and analytics and certain OSS functions, such as service provisioning. So, in summary, we've talked about the role of management and orchestration in NFV and the importance of information and data models, which can enable VNFs to be efficiently and effectively onboarded into VNF infrastructure environments and to achieve the best performance possible. And lastly, we've introduced two of the key open source projects that are active in this area. So, I hope you found this useful and interesting. NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please delete.", "label": [[234, 238, "MANO"], [301, 306, "MANO"], [673, 677, "MANO"], [811, 814, "SDN"], [1415, 1418, "NFV"], [1487, 1491, "VNFs"], [1541, 1545, "NFV"]]}
{"id": 58, "data": " So, in this section, we're going to talk about meeting the service assurance challenge to NFV. And specifically, we're going to talk about the platform service assurance capabilities that Intel architecture management technologies provide, and how they can be used to provide platform telemetry to serve as assurance layers in the platform. So, let's remind ourselves of why the network is being virtualized in the first place. So, the massive influx of devices and use cases coming with IoT and 5G is putting tremendous pressure on next generation infrastructure. The network really must scale to support billions of devices and a wide variety of use cases. The amount of global annual data continues to rise, almost doubling annually. And by 2019, a Cisco report estimates that two zettabytes of traffic will be traversing the global networks annually. All of these factors together mean that service providers really have to transform the way they build and operate their networks. They will be using techniques of virtualization and software-defined infrastructure and cloud techniques to build networks which are more cost-effective to build and run, which are more flexible and allow greater innovation. So today, the network is largely composed of purpose-built infrastructure, with each device containing its own management software. So, you'll see separate pieces of infrastructure or separate appliances for functions like routing and Virtual Private Networking and firewalls. This makes it very difficult for operators to deploy new services at speed, as each device needs to be individually provisioned and tested and put into the network, which is a very time-consuming and complex process.[O2] The network of tomorrow will be deployed using NFV and SDN. Instead of a separate router, VPN, and firewall on three different pieces of hardware, you can run all three on the same Intel architecture-based infrastructure. This will have obvious capex benefits of consolidating free hardware purchases into one. And then when you add software-defined networking, you start to add a degree of intelligence and flexibility to your network provision that can greatly reduce operating costs as well. The result is that networks will be scalable enough to cope with next generation demands on the number of devices and the amount of traffic, and they will provide a level of performance and complexity that can support a variety of use cases with very different require requirements on the infrastructure. Now, this won't be a binary transition. So traditional infrastructure and new NFV-based infrastructure will NOTICES AND DISCLAIMERS [O1]Please delete. [O2]Please merge these two subtitles. [O3]Please delete.", "label": [[91, 95, "NFV"], [497, 499, "5G"], [1756, 1759, "NFV"], [1764, 1767, "SDN"], [2587, 2590, "NFV"]]}
{"id": 59, "data": " So in this section, let's take a look at a reference stack for a universal CPE. This picture is another way of looking at the software stack that might be deployed as a uCPE platform. At the bottom, we have Intel processors and networking and storage solutions, i.e. the hardware layer, and the tightly coupled software layer of BIOS and firmware and drivers. And then above that, there are many options. So we have a host OS environment. We have a virtualization environment, including elements such as a DPDK-accelerated Open vSwitch, the ability in that layer to do chaining of VNFs-- so, the ability to define connectivity paths from one VNF to another. We can also, in the hardware layer, expect to see ingredients such as Intel QuickAssist Technology, which is a hardware offload engine for security and storage algorithms. And then hosted on the top of this software stack are vendors' VNFs, so virtual-network functions for capabilities such as software-defined WAN, routing, firewall, which could come from multiple different vendors. And really one of the strengths of an open reference stack such as this is the ability to host solutions from multiple vendors. One of the components I just mentioned was QuickAssist Technology. We're very pleased to see a growing base of collaboration partners on QuickAssist Technology. Just as a reminder, QuickAssist Technology is a hardware accelerator for security and some storage algorithms. And one of the reasons I want to point this out and to highlight the value of QuickAssist Technology is that in a universal CPE environment at the edge of the network, we are typically working with a type of NFE infrastructure in a very constrained environment. So we may have the ability to host one device on a customer premise. And a technology such as QuickAssist Technology which can really offload and accelerate some very compute-intensive algorithms such as some security algorithms can really be a very powerful ingredient at the edge of the network. So, for our ecosystem-enabling program, Intel Network Builders, we've spent several years working with partners to accelerate technology readiness from core NFE infrastructure to network-edge applications. Earlier this year we launched a new program as a part of this effort called the Intel Select Solutions for Universal CPE. And we launched this, really, to bring the benefits of this experience to the customer premise edge of the network. Intel Select Solution for uCPE consists of a reference design or a recipe for verified universal CPE solutions that Comm service providers can trust to deliver at least a minimum specified level of performance and that our partners can use as a foundation for innovation. So we have produced a reference design and validated the performance of that reference design such that our partners don't have to redo all of that work and can trust the performance they will achieve from the platform and can really focus on bringing their innovation to their VNF on top of the platform. We expect to make adoption and integration of universal CPE applications easier and faster by integrating and verifying these optimal settings most commonly used by universal CPE equipment providers. And I'm very pleased to say that many of our partners, including but not limited to Advantech, Lana, Premier, Silicomm, and Supermicro, have committed to deliver universal CPE platforms based NOTICES AND DISCLAIMERS", "label": [[76, 79, "CPE"], [170, 174, "CPE"], [582, 586, "VNFs"], [643, 646, "NFV"], [735, 757, "Intel QuickAssist Technology"], [894, 898, "VNFs"], [1216, 1238, "Intel QuickAssist Technology"], [1310, 1332, "Intel QuickAssist Technology"], [1354, 1376, "Intel QuickAssist Technology"], [1523, 1545, "Intel QuickAssist Technology"], [1569, 1572, "CPE"], [1801, 1823, "Intel QuickAssist Technology"], [2328, 2331, "CPE"], [2475, 2479, "CPE"], [2546, 2549, "CPE"], [2565, 2587, "Communications Service Providers"], [2999, 3002, "VNFs"], [3083, 3086, "CPE"], [3201, 3205, "CPE"], [3399, 3402, "CPE"]]}
{"id": 60, "data": " The second of the common languages used in beta models today is TOSCA, or the topology and Orchestration Specifications for Cloud Applications. This is the language that originated from the OASIS standards body. And its original aim was to enable the portability of cloud applications and services from one environment to another. Again, let's take a look at a simple, short extract from a data model written in the TOSCA language. In this case, the data model comes from a data model for a virtualized router. And here, much like in the last example, we can see the way that the requirement for that VNF, that virtual router, to have a CPU pinning. A policy applied is expressed in the TOSCA language. We can see that the format looks quite different to YANG. But really, what it's achieving is the same. In this second example, we can see how the TOSCA language, in this data model, expresses the requirements for a large memory page size, and for a CPU allocation policy that can express the requirements for the VNF to be run on a CPU core, in the right number domain. Again, this is a lot of detail. But really, it's very important as you describe the hardware requirements for high performance VNFs NOTICES AND DISCLAIMERS [O1]Please delete.", "label": [[602, 606, "NFV"], [1017, 1020, "VNFs"], [1201, 1205, "VNFs"]]}
{"id": 61, "data": " So in summary, SD-WAN and Universal CP really offer the ability to extend the cloud to the enterprise edge. They enable enterprises to get assured application performance independence of the transport layer, and they enable enterprises to really leverage economic bandwidth for their most demanding applications. SD-WAN and Universal CP offer the ability to simplify the management of the wide area network, and provide enterprises a more seamless way to access cloud hosted services and applications. So in summary, I believe there is really a large strategic benefit to the application of Universal CP and SD-WAN techniques to the network. And that's really in transforming the telco network into a cloud centric platform. By moving from closed appliances to a software centric platform, service providers can realize the vision of NFE and bring the power of cloud to their network. However, this only really works if they deploy a truly open and standard platform to host the cloud, whether it is in their data center, their central offices, or all the way out at the customer site with a truly universal CPE. So I invite you to visit Intel's network builders to learn more about the exciting opportunities that Universal CP and SD-WAN offers. Thank you. NOTICES AND DISCLAIMERS", "label": [[16, 22, "SD-WAN"], [314, 320, "SD-WAN"], [609, 615, "SD-WAN"], [1109, 1112, "CPE"], [1233, 1239, "SD-WAN"]]}
{"id": 62, "data": " In this section, we will introduce what is network function virtualization and the infrastructure, other related concepts, like software define networking and virtualized network functions. So let's start with the software define networking. So this is approached networking that allows managing network services through obstructions that are then decoupling it from the lower level functionality. The network protocols that we use for that for control plane, meaning controlling these types of network elements are then decoupled from the implementation of the forwarding functions or what is implementing the data plane. So, another complementary approach that we have is network function virtualization or NFV, which is the network architecture concepts of using IT principles, starting with volume servers that are then virtualized and running the software applications, implementing virtualized network functions. And if we need more complicated functionality, to build some kind of service out of it, then we can change these type of applications running on top of it. The applications themselves, we are calling virtualized network functions or VNFs, and they can be implemented as one or more virtual machines, initially or now containers in future likely functions, instead of having a single purpose hardware appliance running all the software on top of it. Examples are simpler elements like load balancer, firewalls, or more complicated, intrusion detection, and up to full enhanced packet core being virtualized and running in production that way. So this is running on the platform, which is NFV infrastructure, includes virtualization layer, and hardware for implementing computing, storage, NOTICES AND DISCLAIMERS", "label": [[710, 713, "NFV"], [1152, 1157, "VNFs"], [1607, 1611, "NFV"]]}
{"id": 63, "data": " Here we will cover how NFV allows transforming networks. So, on the left side of this representation, we have a physical model that is implemented in single purpose boxes that can be changed in a physical way. And this is for building services which are not very flexible. They are harder defined. And it is very hard to reconfigure these types of networking typologies. In NFV model, we have both volume servers consisting of volume ingredients, the hypervisor decouple from the virtualized network functions. As needed, we can dynamically provision those functions where they are needed in the distributed network, as long as this orchestration layer for workload placement is recognizing what kind of functionality and requirements those VNFs have and whether they need to be instantiated at which point in time. So, the transformation itself is from physical elements that were built on various types of proprietary silicon. They were proprietary operating systems to control them. In many cases, this was coming also with particular management suites that are only going to handle that single function. It was all very vertically integrated and inflexible and proprietary. And cost-wise, it was not using the advancements that are coming from IT world or cloud world. So, the idea is that this gets turned into the server built out of ingredients that are built and produced in volume for cloud and IT world, so processors, ethernet adapters, FPG adapters, and solid state drives, and open NIT software that is all being applied and consumed in volume in other infrastructure domains. So this was the original premise that we had with NFV. And then we start to virtualize functions that were easier at the beginning and move to the medium complexity ones. And now we are tackling those that are really technically or commercially rather challenging. So this allows networking within those virtual machines implementing the functionalities that are required in building those services. And using software defined networking principles, it allows, so, both virtualization and then allows for more agility. And eventually these networks can be automated and this way become consumable. So those approaches and technology-led architectures and transformations are being applied on different locations and platforms within this distributed network. This is a slide from a number of years ago from Telefonica public presentation. It explains just the way how today the control plane is centralized and the data plane is distributed. It will stay distributed. It's just that in both cases there will be virtualization applied. And this way in both local points of presence and regional data centers, we can apply this type of NFV transformation. Looking a little bit broader, we come to the cloud transformation vision for communication service providers, where public cloud running in those centralized data centers is already virtualized and automated. And now we are applying the same principles now for many years in the service provider private cloud environments. And the NFV is extending this into the distributed network, where first we did it for the core locations, ongoing now, and we will have separate session with a lot of examples how it gets applied on the Edge locations. And then the same applies, for example, for universal customer premise equipment for on-premise services. So this allows us to have consistent technology platform for all these types of different workloads, as long as this orchestration layer knows how to place and instantiate the functionality's software defined in the right locations. It allows for automating these types of environments. And it allows for software-based innovation. 88", "label": [[24, 27, "NFV"], [375, 378, "NFV"], [742, 746, "VNFs"], [1641, 1644, "NFV"], [2725, 2728, "NFV"], [2822, 2853, "Communications Service Providers"], [3077, 3080, "NFV"]]}
{"id": 64, "data": " In this section, we'll cover what is the benefit of NFV to communication service providers, and what were those initial use cases being virtualized. So NFV value to communication service providers comes in different categories. I already briefly touched on some of those. So let's cover them in a little bit more detail. We talk about service agility and faster time to revenue, which is because now when we have software defined, we can be much faster in provisioning those services in those new locations. We can also have the software innovation. So, once we have the platforms virtualized, instantiate the software in the right places, it's way faster than what it used to be, shipping and wiring those physical appliances. For CapEX, once we consolidate the hardware configurations onto a very small subset of those, it becomes-- especially because we talk about IT volume servers-- this helps with the CapEX equation compared to what it was to build all those different individual appliances. And for the operational costs, automation obviously helps. And the next slide will cover a little on how. Using uniform physical equipment is obviously easier for maintenance of this type of hardware environments. And it is also possible if the cloud platform is done with the right separations in place that we can use this NFV infrastructure to run production testing and service upgrades on the same platform. And another reason why this thing is better for operational costs is that finding IT skill sets-- so usual administrators of IT environments is much easier than finding on the market very specific experts on the proprietary elements that it's harder to find. So let's focus, in particular, on how does-- or the major optimizations and cost savings are always being applied in operations. How does NFV help transform it? So, in the way how the care networks are managed today, we are looking at a root cause analysis. And the idea here is to move from finding the faults and having enough manpower that knows how to troubleshoot the environments to move towards full automation through different steps. So first is automated watch. And watch here means machine watching machine, so without having human eyeballs involved. This can be done on massive scales using the right environments. Then detect some of the anomalies in the system, decide if something needs to be done, follow on this decision so that the action gets enforced, and this way we can automatically mitigate a number of those problems. And the last step here is to apply artificial intelligence to those systems and to learn what are the combinations of events that need to be avoided, and simply not allow this thing to happen if the administrators of the network are asking for it. So this will be based on having different layers of the stake automatically detect on the critical alerts and also, the last step based on the analytics. So, all this is proven so far in a number of leading communications service providers, both big ones and the smaller ones in size. And here is the list of initial use case tractions up to 2017 in different categories. So to provide consumer services driven by a lot of VoLTE buildout and other types of services being provided, because it's technically easy virtualize control plane, a lot of IMS, TAS, and, as we see, it got virtualized that way. In some operator, this is running in production and networked for three plus years already. And on business category of services, there is a lot of focus on SD1 and providing enterprise customer premises equipment and provider edge and different other similar functionality being virtualized so that, for example, on the edge of the network or on customer premises, we don't layer on top of each other physical appliances. They all run consolidated in some small virtualized platform, which is then energy and cost optimized if its on premise. And then, for wholesale services, there was quite a lot of focus on IoT and MVNO use cases, where the new build out of EPC was done in virtualized way. We also have now, in the meantime, some of the comm service providers virtualizing full core networking, including EPC, for the existing voice of normal customers. And then those three categories were customer facing services. A lot of internal functions in the carrier networks are being virtualized. And here is just an example list of those. Simply decisions for all those categories are being made. If there's new services built more or less from scratch, of course integrated into a lot previous systems, especially customer facing ones, into all these consoles and OSS, and so on, and for billing. A number of service providers are making these decisions when there is an end of life and end of service. And then they have to face, are they going to refresh this in physical or virtualize? So mostly making decisions to go virtualized because that's the right one. Going on the path to virtualized, automated, and eventually having the networks consumable. So those were the initial use cases up to 2017. And then we have the current ones. And its what is coming next is, for content services, a lot of caching, virtual headend, DVR, CloudDVR is being virtualized, already running in production in some networks. In Broadband services, BNG, BRAS, and CCAP is being virtualized also. We do have examples of that in production already. And then, for the mobile services, a lot of different approaches how to transform the radio access network. And what is a huge focus these days, applied across all those distributed locations is how to get edge computing platform and vertical consumer CommsSPs[O1] integrated with government services also delivered NOTICES AND DISCLAIMERS [O1]", "label": [[53, 56, "NFV"], [153, 156, "NFV"], [1810, 1813, "NFV"], [2549, 2573, "Artificial Intelligence"], [3977, 3980, "IoT"], [4290, 4292, "IoT"], [5162, 5166, "IoT"], [5438, 5442, "IoT"], [1325, 1328, "NFV"], [4663, 4664, "Core Network"], [3309, 3313, "IMS"], [3302, 3306, "IoT"], [3506, 3509, "IoT"], [3961, 3964, "IoT"], [4027, 4032, "EPC"], [4176, 4179, "EPC"], [5561, 5607, "EPC"]]}
{"id": 65, "data": " So, let's talk about service assurance, what it means, how it's achieved today, and how things will change as NFV comes into the network. So, service assurance is the application of policies and processes to ensure that services offered over networks can meet or exceed a predefined service quality level to ensure that user experience remains high. It involves and enables the identification of faults on the network and the repairing of issues in a timely manner, such as to minimize service downtime. And it involves the ability to proactively locate, diagnose, and repair service degradations or malfunctions before-- ideally before users or subscribers are affected at all. Now, providing robust service assurance capabilities is going to be critical. It's going to remain critical as the network transforms to a software-defined and virtualized environment. It's vital to have robust tools that can monitor systems for utilization and can detect faults that could lead to service disruption. Today, monitoring and management activities throughout the network are supported by discrete systems in fixed service chains with tightly integrated hardware and software products and very well-established management frameworks and assurance tools. Now, there are many advantages in cost, speed, and flexibility that come from an environment based on NFV and SDN. However, some of the traditional methods of service assurance may be made more challenging as a result of the separation of the software from the hardware and the ability to deploy services dynamically. So, issues like how to choose and direct workload placement, which KPIs you need to monitor all of the time, which ones do you only need to capture when you are troubleshooting an issue? The industry has a lot of work to do to standardize these things. And NFV deployments will almost certainly or rarely be in greenfield environments. i.e. NFV infrastructure will live alongside traditional appliance-based infrastructure for probably for many years to come. And so, the investment that service providers have made in their existing service assurance tools will continue, and NFV infrastructure will need to find a way to provide telemetry into those service assurance tools in a flexible and open way. So, if we think about service assurance today, one of the ways to think about how it's achieved is through a model that's known in industry as FCAPS, standing for: faults, configuration, accounting, performance, and security. These tools establish baselines, and they use fresh holds to monitor the performance of functions, and to decide if a function is outside of its agreed performance metrics, and if so, what actions are required. So, as I've mentioned, as in the highly likely environment scenario where NFV deployments will live alongside legacy physical appliances, it really makes it imperative that the work that Intel does to enable its platform telemetry and the industry does to enable NFV management tools, that they can integrate well into existing tools. NOTICES AND DISCLAIMERS [THEME MUSIC[O5]][O6] [O1]Please delete. [O2]Please delete.", "label": [[110, 114, "NFV"], [1350, 1353, "NFV"], [1823, 1826, "NFV"], [1907, 1910, "NFV"], [2143, 2146, "NFV"], [2781, 2784, "NFV"], [2970, 2973, "NFV"], [1771, 1775, "IoT"], [795, 813, "Network Transformation"], [1358, 1362, "SDN"]]}
{"id": 66, "data": " So, let's talk about the work that we at Intel and our partners are doing to enable \"Open\" service assurance. The open APIs and tools included in the NFV bBased Platform specification really provide the glue for parsing the rich telemetry provided by Intel platforms-- i.e. the hardware-- and the performance and event management information that's available from software-- now, that could be virtual machine managers, virtual VIMs, et cetera-- into a common framework such that systems such as network management, and analytics functions, MANO stacks, SDN controllers, other analytics tools can access that telemetery, understand it, and take action based on it. At Intel, we chose Collectd as the primary project for the collection of telemetry data from our platform. And we chose Collectd because it's very well-established in the industry, it's in use by many traditional network management tools, and it provides a very flexible and extensible plug-in model that enables you to easily extend it, and provide the ability to gather new data sources, and to publish northbound to a wide range of software components. So, Intel server platforms include many valuable features which are important for enabling high-performing VMFs, things like Intel network interface cards, hardware accelerators for security and compression algorithms, called Intel Quick Assist Technology. And there are many CPU features, such as Intel Resource Director Technology, which gives the ability to closely monitor and allocate memory and cache resources to applications. Many of these components can provide a rich set of telemetry that could be used for monitoring purposes and detect faults, along with other platform metrics such as power and thermals. But the challenge is that many of these components have their own native collection interfaces, and so, it's very complex to gather information from all of these sources. So, with the work that Intel has done, and under the Intel infrastructure management technologies umbrella, we have provided a single interface, a unified way to gain access to the telemetry information that is available from all of these sources for a single interface. So, in summary, if you can't measure, control the underlying platform resources, it will be very hard to measure, and control, and guarantee performance for services running on that infrastructure. So, in this slide, we take a look at an example of how a properly enabled platform can work with both traditional network management tools and work with newer MANO tools. So, in this example, we'll talk about a system which is running two virtual machines. And let's imagine a scenario where one of those virtual machines is running a video streaming service.[O4] And we'll imagine that that video streaming service is running in virtual machine one. If the application running in virtual machine two takes a disproportionate amount of platform resources, it could potentially affect the performance of the video streaming application in VM1, which could result in poor experience, stuttering video for the end user. So, what I want to demonstrate here is two paths to capture telemetry that indicates that that condition is under way. So, we have the ability through Intel Resource Director Technology to capture telemetry related to cache utilization, rates for workloads running on the platform. We can pass that information into the Collectd project. And that data can then be passed up to a network management system perhaps using SNMP protocol. And then that system can alert an admin who could potentially manually take action or some automated action could be triggered within that traditional network management system. At the same time, the same telemetry is available and can be passed up into Collectd. That event can be passed up into a MANO, and NFV management and orchestration, NFV MANO stack via, for example, the Ceilometer project in OpenStack. And then that MANO stack can initiate a policy that, for example, may require the noisy neighbor, the application running in virtual machine two that was causing the problem is moved to another platform, or is the resources available to that virtual machine are decreased to a point where the application with higher service level agreement, the video streaming application, returns to its expected performance level. So here, we see the same capability of taking resource director technology counters into Collectd and then through another plug-in can be applied both to traditional network management systems and to the emerging sets of MANO systems. And these two systems can coexist, and indeed can potentially cooperate and benefit from each other. So, a broad ecosystem is emerging to address the service assurance challenge in NFV. And Intel is working in many of these projects to ensure that the telemetry available on our platform is exposed. Of course, Intel is heavily involved in the NFV infrastructure platform, as that's primarily where our hardware ingredients are used. But we also operate in many of the software projects to try to enable the industry in this area. We are working heavily within the OPNFV project on a project called Barometer. We're heavily engaged, of course, in projects such as DPDK, and Open vSwitch, and FDIO, as well as taking an active role in some of the open source management and orchestration projects, such as Open Source MANO and ONAP. A broad ecosystem consisting of traditional network management vendors, OSS and BSS vendors, and new MANO projects and vendors really need to come together to tackle this end-to-end short service assurance problem. And finally, of course, one of the great promises of this technology is the ability to move beyond the simple detection of a fault followed by some manually initiated fix to a situation where an issue can be understood and potentially fixed automatically. So, we are seeing the emergence of artificial intelligence and machine learning techniques being applied to the service assurance problem. With machine learning, we should start to be able to detect patterns, and correlate data from multiple different subsystems, and really start to drive the ability to learn, and to predict, and to automate fixes potentially before they even really become an issue. So, this really promises to be an exciting area of development over the coming years, and one that Intel will be seeking to participate in and accelerate through our work in open source projects and with our work with our partner ecosystem. [O2]Please delete. [O3]Please delete. [O4]Please merge these two subtitles. [O5]Please delete. [O6]Please delete. [O7]Please delete. [O8]Please delete.", "label": [[151, 154, "NFV"], [542, 546, "MANO"], [555, 558, "SDN"], [2540, 2544, "MANO"], [1354, 1377, "Intel QuickAssist Technology"], [3841, 3844, "NFV"], [3831, 3836, "MANO"], [3875, 3878, "NFV"], [3879, 3883, "MANO"], [3958, 3963, "MANO"], [4584, 4588, "MANO"], [4779, 4782, "NFV"], [4942, 4945, "NFV"], [5414, 5419, "MANO"], [5530, 5535, "MANO"], [5936, 5959, "Artificial Intelligence"]]}
{"id": 67, "data": " Hi. I'm Dan Rodriguez. I'm a VP and General Manager of the Communication Infrastructure division within Intel. And the charter of our team is to work with the overall industry in delivering strong silicon solutions that you can utilize as you transform your overall networks. Why are you here today? Well, Intel is introducing a new Network Academy. And it is really a formal training program. And the series of classes that you're going to intake on are really geared towards helping you have the right recipes so you can successfully implement network transformation. Earlier this decade, the overall industry rallied or started rallying behind a concept called network transformation. This included Intel partnering with the likes of comm service providers, ISVs, OEMs, and even SIs. And the idea behind network transformation is to move the market or the network from a series of fixed function or purpose-built appliances to more general purpose servers utilizing cloud technologies such as virtualization. And the idea behind this would enable the comm service providers to have an infinitely more scalable, flexible, and intelligent network. This would set them up well to not only be able to support many different virtual network functions in a very efficient fashion, but also, to be able to scale out those compute resources to support all sorts of new use case in the future, including all sorts of media, Including: immersive media, analytics, internet of things, and even be set up for success in the 5G era. Can you tell us more about why people are transforming their networks? So, the reason why comm service providers are looking at transforming their networks is really a kind of a high level, maybe twofold. The first is, we continue to see increases in the amount of traffic across the overall network. And that trend will continue as we're seeing all sorts of new use cases, all sorts of new imaginative devices that will connect to the network. The second thing is that as we're going ahead and we're seeing what's going to happen at the later stages of LT and as we prepare for 5G, we're imagining all sorts of new and diverse use cases. And all these use cases will have many different requirements. Some use cases will require high bandwidth. Some will require low latency. Some will require both. Some even maybe require greater levels of privacy or security. So, because of this, the network needs to become much more flexible in scale to be able to not only handle this increase in traffic, but also, to be able to scale and to be able to support this diversity of use cases. Where has network transformation been deployed, and where is the future going to grow? We think back when we first started the journey on network transformation, we really focused really as an industry, in transforming the mobile core network to start. And since that time, we've seen a lot of virtualized EPC deployments as well as virtualized IMS deployments, just to name a couple. And now as we look ahead towards the future and we look ahead towards 5G and all those diverse use cases that I mentioned earlier, the market is now focused on, how do we transform the edge of the network to be able to deliver those services in a way that delivers the right quality of experience for customers as well as helping them manage the overall traffic. And some of the areas that are actually ripe for transformation include modernizing that central office in a concept called next generation CO as well as modernizing the access network and utilizing concepts such as virtual RAM. What is Intel's role in network transformation, and how are we providing value to the industry? Of course, our obvious role is we're going to deliver and continue to deliver leading silicon products that really support network transformation. But Intel does so much more than just delivering silicon products. To really ensure network transformation is done in a really open and standard way, we are participating in a broad array of industry standard groups as well as open source projects. In addition to this, we've also hosted and created something called an Intel Network Builders Program, which has the aims of pulling together the right members from the ecosystem, from ISPs comm service providers, to TEMS, to OEMs, all in the name of pulling together the right solutions to enable this sort of transformation. And finally, Intel is viewed as a trusted technical adviser. So, because of this, we've kicked off this formal training program to provide the overall industry the right tools and education to drive network transformation. We hope you'll take this opportunity to join us in this training program and partner with Intel and the rest of the ecosystem to drive network transformation. Thank you. NOTICES AND DISCLAIMERS", "label": [[665, 687, "Network Transformation"], [546, 570, "Network Transformation"], [1516, 1518, "5G"], [2103, 2105, "5G"], [2951, 2954, "IMS"], [3061, 3063, "5G"], [3607, 3629, "Network Transformation"], [3802, 3824, "Network Transformation"], [3910, 3933, "Network Transformation"], [4600, 4624, "Network Transformation"], [4760, 4784, "Network Transformation"], [2912, 2915, "EPC"], [2744, 2766, "Network Transformation"], [2616, 2638, "Network Transformation"], [808, 830, "Network Transformation"]]}
{"id": 68, "data": " So, we're talking about the transformation and the motivation of it inside the core of the network. And one of the things that we're going to do is we're going to leave the legacy for the most part untouched. And as we begin to introduce new technologies into that network, we're going to use those as our method of introducing this transformation that takes place in that network. And one of the first places that we're going to start, then, is with 5G. So 5G, the fifth generation radio access network, is a wonderful place, because it's coming out now. It's just starting to come in. And while we've done a lot of network function virtualization in the core of the network, this is the massive transformation that's going to drive additional resources into that network. And it's a prime candidate. Some of the authors really talk about 5G as going to be the use case that really drives that network function virtualization and the SDN deep into the core network. So, this is a good place for us to continue our conversation about that transformation that's going into the core of the network. So, let's talk about what 5G is. So 5G, it really is the transformation that's going to affect not only the people, but also those things that we spoke about, that 20 or 50 billion devices are going to be in that network. And in order to accommodate those resources, we really do need to drive that functionality into the network that's transformative. Because of the connectivity aspect of these devices, because the compute resources are going to be quite varied, depending on those types of devices that are in there, and because of the access into that cloud of the network. So, this is an element that really brings that all together. And when we talk about 5G, sometimes we think about it really as the aspects in the radio access portion of the network. And while it's certainly important and critical, that really isn't all of it. It's that we drive this completely deeper into the core of the network, those other layers of the regional office, and then deep into the core of the network into the cloud. So, while we want to think about it in both of those aspects, we're going to focus on different pieces of it as we move on through here. So, some of those functionalities that we talk about, then, inside 5G are going to allow us to talk about the critical aspects of those elements into this digital economy that's going to take place in there. The scale of the capability of the network are going to be 10 times increase in the density compared to what we have today. And in some of the cases, those IoT devices we're going to talk about later, some of those are going to have a very long battery life. When you think about the use cases that are going to be there, we're asking for devices that can have five to 10 years of useful battery life. And that is without being actively recharged. Now, there are some models that will allow us to talk about the possibility of having a recharge ability via solar or some other type of passive device that's in there, or capturing wind. In addition, those devices that are going to be on those networks really come into a variety of places. If we look at the connected car, for example, the typical autonomous driving car by some estimates it's going to generate about a gigabyte of traffic per second into the network. If we multiply that out by 60 times 60 in order to get the 86,400 gigabytes of data that that device would generate into the network-- of course, that's assuming that would be used on a 7 by 24 type basis. Now, we know that obviously the autonomous cars of today are not going to be used at that level. There are some estimates that say that they're only used somewhere in the range of 10% of that duty cycle. So, it's very bumpy in that area. But it's not just the cars that we talk about. When we think about autonomous driving, we think about fleet vehicles. Long haul trucks certainly can come into play here. Or even regional trucks that are doing small distribution type functionality can take advantage of this for a variety of areas. The smart hospital is another great example that's going to drive traffic into the 5G network. It's estimated that smart hospitals generate about 4k gigabytes of traffic per day. And in those devices, that traffic is probably more evenly distributed than we would find in the autonomous driving case. And then a massive source of traffic into the network is going to be the connected factory. And those are going to generate anywhere between 1 billion and more gigabytes of traffic a day into that network. And again, when we think about that network that we spoke about, all that traffic is going to get aggregated in. Lots of other use cases. And we're going to talk about those when we get into the IoT types of areas. But those will allow us to talk then about how this network is going to be different in the 5G use case-- in the core, not just the radio side-- based on those particular use cases. So, when we start talking about the transformation of the infrastructure today, we really have to look at those infrastructure attributes that come into play there. And when we think about the total cost of ownership, some of those total costs of ownership have to do with the thermals and the power that those devices are consuming. Again, if we think about that non-linearity of the increase of traffic that I spoke of, we certainly don't want to see a linear increase in power or thermals into those types of areas. Similarly, we've got new security thresholds that are constantly popping up, where the network needs to be more and more secure. In ancient times, the network itself was very closed off, and security was less of a significant concern. Because as long as you had physical security, your applications were not going to be able to cause disruption to the network. But with IoT devices coming on board, those devices-- and we have a history of use cases where those devices, in fact, can cause security problems inside the network themselves. And then similarly, we've got utilization concerns. One of the important things that come into play also is the location. Depending on the geographical location of these devices or the users-- whether those are human users or devices users-- you may have regional requirements for data sovereignty. And the location not only of the equipment itself is providing those resources, but the source of that information may in fact have sovereignty requirement. So, while you may have a communication service provider who operates in a spoke and hub and they've got distribution across geographical sovereign boundaries, the sovereignty itself may require localization of that information. So, it's not only important that you know where the equipment is geographically, but you also know where the data is, in many of these cases. And that data may be subjective to some decision factor that comes into play, where some data has to be required to be maintained locally from that sovereign standpoint. And then other data, because either of abstraction or the nature of that information may not be. So, you're going to have those types of things that come into play. And then similarly, we look at the transformation of the network itself. Those elements that I spoke about that are in there. Sometimes we talk about them in the legacy world as having been purpose built. That's not necessarily a bad word, but what we really meant was-- the network itself has long been a collection of computers. But these computers are not standard high-volume servers by any large stretch of the imagination. They were purpose built. They may have had adjuncts that were added into it, either for some of these issues that we spoke about, or for traffic, or for routing, or some other things. But the transformation itself in NFV is going to rely on a resource pool that consists of compute network, and storage elements that are more like standard high volume servers. And then those applications can come on into it as part of that. So, we move from that world of purpose-built platforms into those where we've got a platform itself-- networking, compute, and storage-- interconnected into a transporter or interconnect network. And then we can put those applications on it that both have better performance, better security, better lifecycle management, and such that meet the needs not only of the consumer and user with cell device, if we think about it in that way, but the home user, or the enterprise user, and then in the large sense, those IoT users that are coming into play there. All right, so I mentioned the over the top or the through the network. And we just talked about purpose-built platforms. Again, one of the struggles that has been long recognized inside the communication service provider network is that there's a very long lead time for them to introduce new functionality into their network. And one of the additional motivations behind NFV and SDN transformation that takes place is to shorten that, is to go to a software-based model similar to what you would think of as a Facebook or a Google or a Alibaba, Baidu type of an application, where we can introduce new functionality into the network more quickly. And by having platforms that are standard high-volume servers and applications that can go on top of those platforms, we should be able to test out the viability of those platforms and roll them out in a trial basis into the network faster than in the days when we had to go to a vendor and acquire a purpose-built platform, because it would take them time to build not only the physical infrastructure, but the software enablement that's on top of that, and deliver that physical platform into that network, where we could go and do our trials and our tests before leading into the roll out. So again, transforming because of the traffic rate, and then shortening the time span that it takes to introduce those platforms in here. We're going to get back to 5G very quickly. But for the moment, let's think about all those elements as how they come into play. So, one of the other words that you're going to hear is fail fast. I don't think failure's a good word when we talk in the communication service provider network scope, because failure is not one of the options that we really like to talk about. But what we really mean is experiment quickly. And we're going to experiment quickly with new functions that may lead to revenue generating services. And if those functions are proved not to be viable, then we didn't spend a lot of money having purchased purpose-built platforms and putting them in the network in order to do that. So, we could experiment quickly. And then obviously, the big motivation is to reduce that time to revenue, because of the pressures that the comm service providers are under based on the through the network functionality. So 5G. Back into the end. We talked before about how 5G is more than just that radio access. Certainly, that's a critical aspect of it, where we see that 5G is going to offer millimeter waves. This is things that operate in the 25 to 30 gigahertz type of range. Very high frequency. And then maybe in what we'd call the sub-6 gigahertz range, or that mid frequency range on the RF side. And those radios are going to have to change, and all that functionality. But 5G is more than just that radio aspect and the functionality, the heavy math that goes into creating those devices that operate in that space. But it really is that transformation of this end-to-end strategy inside the network. And here we've switched left to right and right to left. So, if we start at the right with the smart devices themselves-- so whether those are the handsets that we carry, or whether those are sort of a fixed wireless access point, where we can use the very high capacity of the 5G network to replace that short tail that exists in the comm service provider network to get us greater bandwidth for that last mile, that last couple of kilometers that are in there, whether to connect at home or connect at the factory, or some other type of device. Right on into the IoT access points, that maybe even some of them will have that very long life of five to 10 years from a battery standpoint. The wireless technology itself, as we've said, we've got a couple of options inside that 5G use case. It is going to introduce new spectrum into that functional use inside to the core of the network. And this is a licensing opportunity the comm service providers are going to have to go out and go to auction in many cases in order to acquire those licenses. Nationally, the government's going to license that out to those comm service providers on the macro network itself. We've also got some things that are starting to be called 5G technology in the Wi-Fi, in the unlicensed spectrum space and some of the other unlicensed spectrum spaces. Those are typically very short distances compared to the macro network. They are a really good access point. But as soon as we get on the backside of those, we're going to find that either we need to connect those into something like a 5G access point to move that traffic into the macro network, as we get it deeper into the core of the network, or through some wired connectivity. Whether that's a fiber or whether that's a cable type of connectivity, we still need to deliver that traffic into the core, into that access network where we've just seen that aggregation. I just spoke about the highways. With 5G, we're going to see-- I mentioned EPC. We're going to change the way that functions, because again, we've got some things that the devices are going to bring, both from a traffic and from a utilization standpoint, as we transform that into that core of the network. And then we finally get to the point where we can provide services into that core of the network, or transport that information much deeper into the network. So, the 5G strategy that we talk about really is an end-to-end strategy that includes not only our architectures of our silicon, but the enhancements for accelerated packet processing capabilities, like FPGAs. And then moves right on into the areas of security, both from the platform level, from an application level, and from a system level, and then the software enablement that takes place that allows us to do all of those elements that we had, like experimentation for that quick development of new applications. So obviously, we're here from Intel. We're talking about this. Why is the transformation of this network so interesting to us? Well, we transform businesses. This is a massive business transformation that's going on. And transforming these types of elements with technology and enabling the ecosystems is really is in our DNA. So, whether we look at the capabilities that we implement inside our silicon itself, whether it's a virtual technology, the packet acceleration, things like the DPDK, whether we've got the quality of service, the amount of resources outside of the silicon through APIs that tell us about the performance of that. Vector matrix instructions like our AVX technology, where security applications can be implemented more effectively using things like our QAT technology. And then into the software world of things like our mech library, the wireless stacks, some of our turbo capabilities, etc. So, it's an entire ecosystem enablement that needs to be built from the ground up, just like a strong building has to have a massively strong foundation underneath it. And in order to build that structure from the top, we look at those technologies, and then build it up from that point and deliver that into the community. NOTICES AND DISCLAIMERS", "label": [[80, 99, "Core Network"], [452, 454, "5G"], [459, 461, "5G"], [841, 843, "5G"], [936, 939, "SDN"], [1076, 1097, "Core Network"], [1123, 1126, "5G"], [1134, 1136, "5G"], [1761, 1763, "5G"], [2315, 2317, "5G"], [4200, 4202, "5G"], [4931, 4933, "5G"], [5910, 5914, "IoT"], [7885, 7889, "NFV"], [8610, 8613, "IoT"], [9025, 9028, "NFV"], [9033, 9036, "SDN"], [10059, 10061, "5G"], [10963, 10966, "5G"], [11014, 11016, "5G"], [11115, 11118, "5G"], [11426, 11428, "5G"], [11932, 11934, "5G"], [12219, 12223, "IoT"], [12347, 12356, "Wireless"], [12432, 12435, "5G"], [12877, 12879, "5G"], [13598, 13601, "5G"], [14033, 14035, "5G"]]}
{"id": 69, "data": " Hi. My name is Larry Horner. I'm a solution architect with Intel's professional services organization. And we're going to talk about the communication service providers network. And the transformation that are underway in that network. The impetus for this transformation actually began over 10 years ago with the introduction of what is now the ubiquitous iPhone in 2007. And with the introduction of that device, there was a massive increase in the network traffic that was placed onto the network. And in addition, a number of applications began emerging that had not originally been perceived or anticipated. As we move forward and find out where we are today, that initial transformation has driven us to a point where we are projecting that there are going to be a massive number of devices onto the network in the next five years. By some estimates, that's between 20 and 50 billion, depending on whose reports you might read. In addition to that, this introduces some operational complexity into the network that places constraints on to the operational effectiveness in that network. In addition, there's also the exponential increase of traffic into that network due to the video on demand. Or sometimes what we would consider the over-the-top or the through the network applications that are coming onto this. We go further and we see that the use cases really require a ubiquitous mobility. The users really don't care where they are or what network they're really connected to, in many cases. They want that device to operate effectively in any of those locations. And then as I mentioned earlier, the OTT or the through the network are driving cloud services initiatives into that network. All of this combines together to place significant challenges into the communications service provider network. And in 2012, a gang of them got together and developed what they called the network function virtualization effort under the ETSI. And we're going to talk about how that's impacting some of the work that's going on in the network and how that transformation is driving. So, if we step back for a second and actually look at what we're calling the communications service provider network. And some people may use the term Telco. It's kind of a legacy description of it. But we're going to transform away from that. And what we're really talking about here then are both the wired and the wire-line services that communication service provider network provides. In this diagram, we see the dark blue areas are really the functionality that is operated by that communication service provider, collection of companies. And globally, around 360 or so of them exist. And they're the ones who monetize the services that are inside those dark blue elements of the cloud. The lighter blue elements in this diagram would be the end users, whether those end users are human beings. Individual users or consumers, as we sometimes call them inside our space. Or whether those are enterprises. They could be business. They could be government. They could be other types of institutions. And then in addition, you might find that their consolidation points that take place, municipalities, may provide Wi-Fi access that provide consolidation. Let's say, from a library into the communication service provider network. Now when we look at that network, the communications service provider network, the dark blue sections in here, we've got ingress points or access points, as we like to call them inside that network. Those can be wired. And in that case, that connectivity comes in through some type of customer premises equipment or CPE. And we're going to talk a little bit later on in some of the sections about virtualizing the CPE and the functionalities that take place there. And how that fills into this whole picture of the transformation that we spoke of. And the other axis comes in through the wireless interface. We're all familiar with a mobile network. We're all familiar with Wi-Fi at this point. Those are all ingress points as well. Into this network, whether it's from a macro cell or a micro cell like a Wi-Fi that come in then. And as soon as possible really, in many cases, we want to take that wireless signal and we want to put it onto some physical infrastructure. Like a fiber or some type of ethernet interface to drive that back into the central locations where we can aggregate that information into data centers. Sometimes those would be called the next generation central offices. Sometimes you'll hear that description come out in this place. Then similarly, those elements are interconnected deeper into the core network. And in that core network we can have very large collections of compute capabilities that exists inside that network and the transformation that takes place in there. In those two elements, that I talked about, that next generation central office or the regional office center and then in a core network, we're seeing a massive transformation going on. That we talk about under network function virtualization or we turn that into our acronym or a TLA for the three-letter acronym of NFV and SDN or Software Defined Networking. So, if we want to think about those elements very quickly. And we'll talk more in many of the sections about it and double click down onto those things. Network function virtualization is about transforming the way the applications are built and deployed into that network. And software defined networking the control aspect of it. And it tells us how we stitch those things together. How we allow that interfaces to be plumbed, if you will. If you think about it as how that data has to be flowing through those network elements. So, we've got the workloads themselves in NFE and we've got the interconnect aspects of it in SDN. And sometimes we overload those. And you'll hear some authors talk about network function and software defined networking simultaneously. And that's fair game. But typically, we're going to want to discriminate those out because one is the actual workload itself. And the other is the control of that workload. That's at the high level. And then we have to realize, it has to come into place. And it has to be operationalized. Operationalizing it is the activity of doing the management and the orchestration or the MANO layer. And then you'll see that the particular functionalities in the core of the network, one of the first ones that we're going to bump into is going to be the EPC or the evolved packet core. And we've put a little V in front of it because if it leads with a little V, it's the cool thing. And that's the virtualized EPC. And we can show how that can be distributed. And then applying these concepts of NFE and SDN into that environment. And then finally, we get into the deeper portion of the network where the data or the cloud may be. And what I like to say when I show this slide is that this is only half of the picture. Because if you really think about how this works, if you're sitting there using your handset or something, and you're streaming a video. This shows that information going out into that data center and how it's going to touch all of those elements. But then there's a return path. And that return path may hit another data center, and another core network, and another access network. So, if you're doing something like streaming a conference call between two users, you're using one of the currently popular streaming devices for that. There's actually a similarity of it where you don't care if we're on network A and network B. The two users are on there. You're Skyping your granddaughter or whatever it happens to be. And similarly, they've got a device at their end that replicates or folds this model over here. So just keep in mind that this really just shows half of the picture in many of those cases. Of those elements that exist inside that network. So that's setting the foundation for what we're going to be talking about here. Again, if we take another transformative look at what we mean by that network. Again, we're going to start at the left hand side of this slide. And at the top, we've got a little icon that shows you that these are the wireless connected devices. And they're going to come in through some aggregation switch. And then possibly into ethernet nodes or other access nodes. If you're at a home or a small business, you may be coming in through some digital subscriber line. And again, into an aggregation point. And then into the core of the network. And similarly work your way down. Maybe you've got a passive optical network if you're advanced enough that you've got fiber optics that comes actually into the premises itself. And not just being terminated at the curb. And what we want to understand here is that these are all the ingress points, if you will. If you think about it from working again, from that left to right as we flow through. And then inside that network, the next thing we're going to bump into our larger aggregation nodes where all of this traffic is going to have to be consolidated together. So, I like to use the model of highways and local streets. So, you've got lots of local streets and maybe some cul-de-sacs that are out there. That's at the far left hand side of things. And then that traffic begins aggregating and deeper into that network into those highways. So, we've got on ramps on that highway. And that traffic may have to go some distance or may even interconnect with other networks, other interstate points, as it goes further. So, if you're thinking about a user who may be sitting in the central of a very large geographical nation, let's say, you're sitting in the United States and you're in St. Louis. If you've got traffic that needs to go to your Skyping to someone in New York. You want that traffic to head in the direction of the east, not the direction of the west. So, in that portion of that geography, you're going to have highways, intersection points that head in both the east or the west direction. Similarly, if you want to talk to someone in San Francisco, you want that traffic to be heading that other direction. So, while we show this transport network in the middle of the network as sort of a loop here in the cloud, it's actually you can think of it as a set of interconnected highways, if you will. That have very specific traffic controls and capabilities in it. That allow the direction of that. So simply, when you think about it, it's not a simple connection from end point to end point. But rather aggregation of those pieces of information, just like cars or trucks on the highway that have to go long distances and go through a common infrastructure. And it's this common infrastructure that's under that stress for that projected 20 billion or 50 billion devices that we're talking about being connected. And similarly, it's under stress because of the through the network type of applications that are in there. If we drop down a little bit lower into this picture and understand what's happening, then there's also a physical transport aspect of these things. I mentioned fiber already. It's very reasonable that we think about the interconnects between these long distance points as having inter-connectivity. There's going to be the physical infrastructure. We're not talking about transforming that. What we're going to be talking about of transporting the layers above it. But it's important to understand that that layer does exist and it plays a critical role. And then similarly, as we go deeper into the core of the network, there is functionality that takes place. Not just in the communication service provider network for things like the evolved packet core that we talked about and the functionality that's in there. Is again, as you think about how that device is going to work, is those  communications service providers renumerate. They get money out of that service being used. And they may be doing things like counting packets or utilization. If you're an enterprise, you may have subscribed to a particular quality of service or a particular utilization level. And you might have service level agreements that determine how much you're going to pay or how much renumeration you're going to get back if the service is not met. So, there are elements inside the network that keep track of all of that and then generate those types of reports inside that communication service provider network. In addition to providing useful content for things like the routing of the traffic and analytics that could be in there. If you're the communications service provider network, you also are very much interested in the operational aspects of your network. This collection of computers that are generating events. Information all the time about the traffic that is going through the health of their system. And then you've got things where you've got to concern yourself with the service life management of those platforms. Is how do I continue to ensure that these platforms are secure, upgrading the applications or the operating systems to meet the security requirements that are obviously, in that network. And the reason that we're looking at this transformation. Let's step back for a second. And think about what I was just talking-- The network function virtualization, it's a means to an end, as we describe it. And that means to the end is changing the way we build and operate these networks. The reason we're doing that is so we linearly try to grow our networks at the point that our traffic is increasing. We can't do that. It's unsustainable. Because the traffic in the network is actually growing exponentially. And the communications service providers simply can't exponentially invest in more infrastructure into the network. Nor can they exponentially invest in the operations, the staff that's necessary in order to transform that network at that rate. So, they've got to do something to keep up with that traffic growth rate. And still not have an exponential growth in what they charge their customers. Because that's not a business model that's going to be sustainable for them. So, what network function virtualization allows us to do is transform the way we build and operate those networks. So that's what we're going to drill down into. And hopefully, this is starting to make a little bit of sense as we go through it. So, let's back out and say, I'm going to transform the network. But I really don't want to make the investment to transform what's existing in my network. We've got new technologies that are coming along. As I deploy this new technology, these are the prime targets for transforming the way that we build this network. Because again, we're talking about a network that's been operational for decades. The first phone call happened over 140 years ago and we've been building this global network for over 140 years. Now, some of the equipment in it is not 140 years old. I don't think any of it is. But certainly, some of the equipment, some of the functionality that's in there has legacy purpose and legacy utility. And the ROI on a piece of equipment that's still operational and a network that's still generating revenue is really a good thing to continue to maintain. But when we introduce new technology or introduce new functionality into the network, those are the prime targets. Not exclusively the only target. But those are the prime targets for this transformation. And one of them that's really current today in our discussion is the technology that we call 5G. NOTICES AND DISCLAIMERS", "label": [[3636, 3639, "CPE"], [3734, 3738, "CPE"], [5140, 5143, "NFV"], [5148, 5151, "SDN"], [5809, 5812, "SDN"], [6330, 6334, "MANO"], [6497, 6500, "EPC"], [6654, 6657, "EPC"], [6748, 6751, "SDN"], [15607, 15609, "5G"]]}
{"id": 70, "data": " So, we talked about some of the motivation behind 5G. And one of the large drivers is going to be what we call the Internet of Things, or IoT. IoT really is that 20 to 50 billion connected devices that are going to come onto the network that give us those different bits of functionality that we're going to talk about, how that drives the motivation for some of these transformations inside that network. So, the first question that we really probably want to answer is, what is a thing? What do we mean by that? So, let's think about the set of devices that are going to be connected. Today we know what those devices are. For the most part, the handset itself, the mobile phone, that's not going to be considered a thing when we talk about it. We would call that a UE, or user endpoint. Typically, we wouldn't consider that into this special case of those connected devices that are IoT. Similarly, we might not think of it as the home gateway that may be connected into those types of devices. Typically, what we're talking about when we get that collection of devices or the things are those elements that would not normally have a natural human interface, like a keyboard, either directly accessible to them, like we have on our mobile device, or into a data center computer, where there may not be a keyboard connected all the time. But we could connect a keyboard or access to it, and control that device through a keyboard. So, there's this whole collection of other devices. Sensors would be a great type of a concept. Let's take the connected elements inside your home, whether that's going to be an appliance like a refrigerator or a dishwasher or a washing machine, or what's going to be a center inside a factory that's controlling some type of position on a robotic arm or some other type of a device. Those are the elements that we're talking about that would be things. Typically, you wouldn't find a keyboard connected to those, the traditional QWERTY type style keyboard. Obviously, a dishwasher is going to have some type of a keyboard. But it could fall into that key classification of a thing because of its functionality. But the other important point, these are not the isolated devices. So again, if you look back at some type of a legacy factory, where you've got a CNT type machine that is completely self-contained. And even though it has some automation capability or programmability to it, if it's lacking that interface connectivity to the network, and sometimes I'll break it down and say if it doesn't have an IP address, it doesn't have an IPv4 and IPv6 address that allows us to access that device and control it, it's probably not going to be in that classification of a thing itself. So, what we're talking about, then, is that there's going to be 20 to 50 billion of these things that come in that are going to allow for those applications, whether that's a health care type of functionality, whether it's a smart city where we've got detection for parking or metering or some type of crowd control type of thing or autonomous driving. Those are all the elements that we're going to find when we talk about those types of devices that go into that classification of a thing. So, what are some of those requirements, then? Well, there are going to be massive IoT devices. So again, we talked about smart cities, whether it's transportation or parking, or really when we get into energy monitoring or environmental types of functionality. So, you go into a very large city, where you've got tens of millions of people, whether that's someplace like Mexico City or New York or Tokyo, there are certain functionalities that the infrastructure needs to support that could improve the lifestyle for people. And parking and transportation is one of those great models. If you think about someone in a residential area in some of those high-density communities, spending 20 minutes driving around their block trying to find where's their next parking space that I can park my car in so that I can leave it overnight before I use it again. Those are significant problems, because it adds to congestion into that municipality. Where if they said, hey, if the city was able to enable the detection of those facilities, and tell them if you go here, you're going to likely find some place to store your car for overnight-- or congestion inside that city itself. We've got construction going on, we're going to reroute traffic and communicate effectively for that reroute. Intelligent buildings, energy savings that go into a place. This is a massive type of an opportunity, where if a power industry is integrated well into that, and they have some maintenance activity that was unanticipated and they need to shed load in order to do that, they may have some type of ability to communicate that information in devices. And say, hey look, you need to raise the temperature in your building a couple of degrees. So, it takes a load off of this infrastructure, so we can apply functionality to perform that operational maintenance that's necessary without causing impacts to the greater power grid. Those are all types of things that, in very large cities, we can do. But in order to accomplish that, we've got to get that information from those devices back into a large data center, operate online, and possibly provide feedback into those types of areas. So, those are the types of massive IoT devices that are in there. Again, industrial, agricultural. There's seven billion or so people in the world today. And we all need to be fed a couple of times a day. All that food has to come from somewhere. Agriculture is a massive industry. There's a story about a company that built tractors, and they would sell tractors on a regular basis about every 10 years to the farm community. And they realized that they only see these customers once every 10 years, and that was life expectancy of those devices. But the agriculture industry itself has transformed in the last four or five years to the point this tractor company now is no longer a tractor company. They're an information data company. It just happens to be that the tractor is one of the devices that used to get information out to the field and to performs operations in the field, so we can do things like monitoring, control access. And the devices themselves are controlled through geo satellite positioning and autonomous driving and a variety of things. But they can detect when is the optimal time either to apply irrigation, to apply nutrients to the devices, or even to plant or harvest their crops. All of that goes into the efficiency of that industry. Pulling that information back in, then, helps the commodity industry predict the availability of those resources, and manage that industry a whole lot more effectively. Massive transformation that we see is going to take place in there. Similarly, manufacturing. If you look at it the challenges of manufacturing, automation of manufacturing is a good thing. It improves the quality of the product at the end of the day. It also may, in fact, drive the cost down, which is an efficiency that helps the end consumer of those products along the way. So, anything that we can do there improves that performance. Even tying that back into this supply line management, whether we're ordering parts or making sure that we've got enough of the components that are necessary in order to meet those orders and providing that information back into some corporate center. Because large industries are no longer consolidated into a single location, but manufacturing may be distributed. But you're still going to have a headquarters. That headquarters needs that information in order to control and manage that infrastructure that they're responsible for generating product and profits on. And then we've got things like the commercial IoT areas, HVAC sensors. So again, I gave an example of a control system of an HVAC system where you may, in fact, ask to shed load from a power grid for some type of a device. And we're seeing smart meters, power industries are driving those in very, very interesting applications there. And then finally, in the commercial area in inventory management, there's an expression in some industries that freight at rest is freight at risk. So, if you've got the ability to have your freight more effectively controlled and monitored, you may, in fact, have less leakage of that inventory management from a transportation standpoint, right on down into looking elements of commodities inside a storefront. I saw recently in one of the big box stores, they've got a really cool app now that you can actually tell them what you're going to find. They'll take you right down the aisle and put you in front of the bin using your mobile device. So, all of that traffic flows back to the network. And it's nascent at this point. But if you think about that industry itself, it's going to place a significant load on the operational elements inside that network. So, with all of those things that could be possible for an IoT standpoint, we're going to look at what are some of the requirements there. So, some of the critical IoT functionalities are things like remote health care. Obviously, no one wants a health care service that is life-dependent not to have the highest standards of quality and availability. Similarly, an autonomous driving type of an application, both for traffic safety and for human safety, are very necessary to factory automation. Again, if you've got humans interacting with robotic devices that are moving around in the factory, you want to make sure that those robots are getting quality information from those sensors that are detecting the movement of those humans, so that we don't have an industrial problem that would cause us to fill out lots of additional paperwork. Especially in the public safety area, a lot of interest that goes on into the public safety area whether that's crowd control like at a concert, or whether that's elements of-- again we're talking about redirecting traffic in certain scenarios, where you may have a road closure or an emergency situation that's taking place, and providing that information back. And also improving the emergency response elements. One of the applications that we may look at, when we talk about things, is allowing slices of the network-- and we use that term later and describe that, so that we can actually allocate resources to emergency responders in the times of some type of natural disaster, so that their load gets prioritization. We're thinking about a variety of examples, whether that's a natural disaster due to a wildfire or a hurricane or a storm. You may, in fact, want to allocate resources to the public safety sector that reserve those capacities inside the network for their utilization and shed load from other consumers. It may be putting traffic onto that network, so that those critical services get access to the resources and the communication infrastructure they need without building a separate dedicated network. You don't want to build a separate dedicated network in this case for a variety of reasons. One is locality, and the other one is going to be cost. You don't know exactly where you're going to need it all the time. And geographically, it may vary from time to time. So, you would have a lot of infrastructure that would be invested that's not necessarily used at a very high duty cycle. But when it does need to be used, you certainly would have to be able to access it. And then we talked about the energy drivers in there as well. The energy sector, obviously a very important one. Sometimes, it's one of the ones that we forget. When we look at the total cost of ownership of some of these platforms in the network, a significant portion of that over the life cycle of a system is actually associated with the energy itself of those platforms. And if we can manage that, whether that is spinning down functionality when it's not being used to shed load off of those platforms so they consume less power, again to an example where we've got operational considerations for maintenance activities to control that energy or that power consumption, NOTICES AND DISCLAIMERS", "label": [[51, 53, "5G"], [139, 142, "IoT"], [887, 890, "IoT"], [3297, 3300, "IoT"], [3363, 3377, "Transportation"], [7874, 7877, "IoT"], [9085, 9088, "IoT"], [10847, 10858, "Utilities"], [9190, 9193, "IoT"], [3756, 3770, "Transportation"]]}
{"id": 71, "data": " So, we're going to talk about some of the verticals inside the IoT. And the first one that we're going to really want to talk about at this point is transportation logistics. There are a lot of interesting requirements that come about from this. So, the ecosystem and the carrier opportunities, so the comp service provider carrier opportunities, from a use case standpoint, revolve around both inventory and supply chain management. The possibility for smart travel, we've talked a little bit about that. Smart airports. And then also the possibility of delivery services, either through drone or robot delivery services. And the opportunities come into place, then, really are broken down into two significant areas. The first is the massive connectivity with non-time critical sensing. And by non-time critical, we mean both the duration, the interval in which that takes place. And the information itself is not going to be critical. So, for example, the ability to predict maintenance associated with some of these platforms, or to perform asset tracking and monitoring. And when I say it's not time critical or significant from a quality service perspective-- for example, if you've got a predictive good maintenance interval that schedules up and says, look at the next 100 miles, in the next 50 miles, in the next 10 miles, in the next 5, if you were to skip that information for whatever reason about the 50th mile, it's not going to be critical because there are several bits of communication that flow along the way. Similarly, with asset tracking type of devices, if it has an interval that passes if one of those is dropped somewhere along the way for whatever reason, it's not a catastrophe that's in there. The other use case is the massive connectivity for the time critical and the sensing. And these types of elements, you really don't want to have any of the information lost in order to have it delayed. It needs to flow very rapidly through the networks. So, we've got two different bits of opportunity that take place there in this logistical area. And for that type of device, you want to think about, like, for example, the sensors on a car that is providing information about road congestion or some type of critical activity that is instantaneous that needs to be communicated in order to prevent a further complication along the way. Similarly, you may have context-aware type services providing information in these types of areas that may require geofencing. Again, that geographical requirement to contain inside some regulatory element. So, as we break this down into this table that's on the slide-- and we're certainly not going to go through each of these elements. You can double-click on it yourself as well. You can see that we've broken down the functionalities themselves into the intermediaries, the platforms, network connectivity itself, the devices and equipment that are going to be used in order to provide that end. The vehicles where we expect to find those types of things. And then the infrastructure of the transportation. So, you'd expect that the road would have a different requirement than a rail line or an airport or a harbor type of an element from that transportation infrastructure level. And it may in fact be controlled by different elements, from the local municipality to a government to some private type of an enterprise as the key players. So, if you think about things like a harbor or a port, you may have a regional operator that builds their own infrastructure to control the flow of that data traffic that's in that area, whether those are Wi-Fi or macro-type networks that pull that information to track those platforms, those boxes, those shipping containers, and allocate them to the trucks correctly. As we look back up through those types of functionalities that we've got here, we're going to develop different requirements, different functional requirements, into the network itself, into that 5G network, from these iOS type devices that are out there. So that platform, for example, has a localized positioning, there's going to be requirements that the vendor themselves, the OEM, the ODMs, need to put into place on that vehicle, right on into the telco operators themselves and how they're going to deploy the manageability of those tech platforms. If it's a digital map provider, someone is controlling that information deep inside the core of the network. And again, you can think about that model that we looked at before, where information needs to flow into that cloud type of an environment, through the network, to be turned back around and maybe in this case distributed to a wide audience of consumers, if we're looking at smart travels, for example, on those types of applications. So, there's a reference here on this slide if you want to go down a little bit deeper into it. So, in our last slide, we actually talked about the connectivity, whether it's non-time critical, and still massively connected over the time critical. And you could find those two cases show at the top of this slide. And again, we're not going to go through each one of these data points here. We'd been talking about this slide for a very long time. But to show the functionality that drives requirements into that operational network, into that 5G network, based on those elements. And then finally, the vehicle to another vehicle to act. Sometimes we see that. In this example, a remote drone operation. So, let's take a look at the non-time critical for the moment, just to pick a couple of these things out So, from a latency standpoint, the non-time critical, obviously, is not going to be significant. Transition that over to the time critical or the sensitive type of an area, we need less than a 30 millisecond for some of these activities for a round-trip turnaround. So end-to-end really means round-trip turnaround. So, from the device itself into the network where we perform some type of an operation. And then a compute load result turns back, and then to a control aspect either in the device itself that the sensor is a part of, or to other devices. Similarly, taking that same latency across to a vehicle to x or drone type the requirements may even be less. We may find it that we need to operate in the sub-10 millisecond type range in that area. Reliability, we can make an argument as to whether the reliability is critical or noncritical, probably in the real-time of the critical sensing type of an area. But it certainly wouldn't be critical in the area of a robotic delivery type system or drone type delivery system. You don't want it just falling out of the sky because of a lack of connectivity into the network, because of congestion into the network, for example. Similarly, when you look at the devices themselves, we have to ask, how dense are they going to be deployed geographically? Because again, when we think about these devices coming into the network, there's going to be some type of aggregation point, and most likely a wireless aggregation point, 5G aggregation point, that's collecting all of that information. If you think about the cell tower today, if you've ever gone to a large event, a stadium-sized event, and found that you've got problems using something as simple as SMS in there, that may be because there are just too many devices inside that very confined space. Well, that's a density issue. And the network itself is only designed to accommodate a certain amount of density. But if we look at the IoT types of use cases, we may find that we've got a very dense type of concentration. And that requires us to put more infrastructure out there on the network to handle all those devices, so, they can meet those types of need. So, when you look at device density across this area, 10 of the 4 per kilometer is a very dense type of a device. So, if you think about the macro network of yesterday, it may not be able to accommodate that number of devices. It just can't address them in that area. So, the 5G network gives us a greater density of possibility, so that we can meet the needs of some of these types of use cases when you think about the smart city or the smart parking or the hospital types of things. Security is always very critical. The data that we look at in here is going to require encryption end-to-end that comes into play. And that's just the beginning of it. It's not just security of the information in flight. It may also be security of the information while it's at rest. And the positioning of the devices-- and these are all, again critical elements that we go through and we look at the collection of these types of technologies to place requirements on the network. And we simply don't have the resources today in the legacy 4G network in order to meet all of these functional requirements as we look at the opportunities for the business simply in transportation and logistics NOTICES AND DISCLAIMERS", "label": [[64, 67, "IoT"], [150, 164, "Transportation"], [3058, 3072, "Transportation"], [3212, 3226, "Transportation"], [3973, 3976, "5G"], [5319, 5321, "5G"], [7063, 7065, "5G"], [7528, 7532, "IoT"], [8032, 8035, "5G"], [8908, 8922, "Transportation"]]}
{"id": 72, "data": " Another interesting IoT vertical we mentioned earlier is health care, or health and wellness. And in this vertical, again, we're going to see some tabular information that we're not going to dig down into in every one of these boxes. We're going to touch across them. So, the use cases, again, for health care and wellness, monitoring remote health care, possibly with assisted surgery, I'm not signing up to be the first one for that. But then information mediation as well is certainly useful. What are the opportunities to the comm service provider or the carrier that exists out there? Again, real-time video. You think about TelePresence or an augmented reality. For remote health care is that today, it's not unusual that, if you have a health care plan of some type, you can actually interact via video conference with a nurse or a nurse practitioner, or maybe with a physician, or even a physician's assistant in some of these cases. So that you don't have to go you know to an office visit if it's-- I've got a really funny rash. I was out working in the yard, and I'd like to talk to somebody about that. And they could call in a prescription and say, look, this is most likely what you picked up. So those types of things are certainly opportunities for the network. The massive monitoring sensing that may come into place, both for health and wellness, this is going to be a more critical aspect than describing, hey, I've got this funny-looking abrasion on my arm that I want somebody to look at. But rather, if we've got a cardiac patient who has had some sort of health care monitoring system in their home, that this critical information comes into play. And then we've got the possibility for real-time command and control of devices themselves. And again, I think that's a little bit further out. But certainly, if we open the aperture and think about the things that are possible, it's in the realm of those things that are possible. So again, we could look down the functionality itself. So, we're going to see various platforms and network connectivity requirements, the device and equipment. And then some of the health facilities and some of those examples that come into play in this table, and in the devices as they drive requirements into the network and who those players are in these bits of functionality. So just like when we talked about transportation and logistics, and we've got particularly use case attributes laid out here for the real time or the TelePresence type of activity, the massive sensor. And then obviously the real-time command and control. And then what those elements are from the description standpoint, so we understand what they are, where they come into play. What's the potential technology that's in there? So, it's certainly possible that we could do things from a monitoring standpoint of a particular device through the 4G network, not necessarily requiring the functionality of the 5G. But when we look at it the actual Telepresence type functionality, or robotic control of some type of surgical device, those are going to require the bandwidth and capacity that we see inside the 5G network. Obviously, we've got reliability issues for some of those that are more critical than others and end-to-end latency that are driving those considerations. So again, we're not going to read through each and every one of these elements on the slide. I encourage you to take a look at it. Positioning standpoint-- probably not as dense as we saw from a transportation standpoint in this area. And if we're talking about geolocation, probably not as critical that we know exactly what the location of those devices are. NOTICES AND DISCLAIMERS", "label": [[20, 24, "IoT"], [2371, 2385, "Transportation"], [2945, 2948, "5G"], [3145, 3147, "5G"], [3507, 3521, "Transportation"]]}
{"id": 73, "data": " So one of the other IoT verticals that very interesting in the comm service provider space involves smart cities and utilities. So in this area, again, we're going to look at the network performance requirements. The key performance indicators, KPIs, for some of the subcategories that we see down that first column for smart lighting, metering, parking, connect and transport, environmental monitoring, and in addition digital security. The bandwidth requirement, BW. The bandwidth requirement, for example, from digital security is significantly higher at 10 to 100 megabits per second than we would find, for example, a smart metering And the smart metering example is significantly, less with a 1 to 100 or to 1,000 kilobits per second. Smart parking falls in that 10 to 100 megabits per second. So much greater than the lighting or the metering requirement but significantly less than our digital security. The density, again, smart metering, possibly very dense. Digital surveillance, not quite as dense in that area. The availability, obviously, if we're going to do this, we went relatively high availability and similarly the reliability to reduce the maintenance activities that are in there. So those use cases, the comms service provider you might be interested in into these you know large metropolitan type smart city enabled connected lighting. So again we talked about energy a little earlier in our conversations. And one of the things that you might be interested in, is if you put in smart lighting, if the traffic volume decreases significantly at 2:00 in the morning, and you can detect traffic increasing, suddenly a freight company releases a number of its trucks you might want to start turning your lighting on, then, for that 15 minutes as those trucks are rolling out between 2:30 and 3:00 in the morning, and then 3:30 to 4:00, or whatever it is, you may be able to reduce that. But if you have that information, and it's available under your network, that's a use case that could actually save the city money from an energy cost standpoint by detecting the flow of the transportation infrastructure and disabling the lighting when it's not there and then re-enabling the lighting in the real time when it's there. Emergency service management is a fine example. Again, we've talked about some of those types of examples where you've got a need for network access for some contingency type of an operation from an emergency management situation. And then smart grids, we've talked about the performance requirements inside the electrical grid, and if the municipality has the ability to provide feedback into that. So what are the opportunities, then, in that comm service provider? We're going to break these out again into both real-time video monitoring and guidance, and the functionality that are there. Massive connectivity that has maybe non-real time sensing. When you think about pollution controls, you're monitoring environmentals, for example, in that area. If you have sensors that are doing that, or you are detecting some type of weather, massive connectivity possibilities there in a non-real time area. And then we also have massive connectivity for time-critical content. Again, you think about things like the digital surveillance or detection of a natural disaster and control type areas. So we're going to take those opportunities, and transform them into double-clicks, if you will, on this slide across the top. So real-time video monitoring and guidance. And the massive connectivity for non-time-critical sensing and then massive connectivity for time-critical sensing. The description below tells you some of the examples you might find that come into play there. What's the potential tech? So 5G falls into that scope for that real-time video monitoring, because of the capacity and the low latency that's required there. Maybe you can do some of the sensing with lesser technology, whether that's 4G or Wi-Fi technology. May be overkill, may not be in the 5G space. But nevertheless the technology exists that we can provide that forward or backward capability into those devices if the 5G is there. And we can get a sense whether the town connects to it effectively, you may just want to be able to do that. And obviously, on the massive connectivity for the time-critical, you're going to see the functionality of 4G and 5G that comes into play here. Our expected data rates, again, are significantly double-clicked on, if you will, here. The video is going to-- if it's a 4K or an 8K type of video, you're going to see 30 to 40 megabits or 80 to 100 megabits per second functionality that's in there. Very high-definition video is going require about 10 megabits per second throughput from the network in order to sustain that type of a video. And again, that's from a single camera. If you think about enabling municipality, you may need a large number of these sensors that are deployed. And then you would have the multiplicity of those devices in the network, which places more infrastructure requirements onto that device. So you may think about the smart lighting to having a video camera associated with it as well. And rather than running a hard wire from that camera all the way back maybe you've got a small 5G transmitter cell that's on that device powered by the same infrastructure that the light is powered providing connectivity back into some monitoring site. Again, latency, depending on what these devices are going to have different latency requirements. Maybe our real-time video, even though it says real time, has a lot larger latency capability than, for example, what we might see on some type of critical center activity. Again, reliability, we've taken a look at some of the studies that tell us how reliable these devices need to be. Not the devices themselves, but the device and the end to end capability inside that network. Smart grid management, obviously, if you're going to put that into play and count on it to do the things you want, you're going to have to have very high reliability for it. Whereas if we're looking at metering control, or weather-type control, maybe that's not monitoring Not control the weather. Wouldn't that be nice? We're going to see that that's probably false to that non-critical area. So the way we design and operate the network may have different requirements on it. Again, the density is going to drive a lot of our need here. The camera density may be different than we're going to find from a monitoring or a center requirement that's in there. And then also, we've got some coverage considerations that come into play inside those functions. 155", "label": [[20, 24, "IoT"], [117, 127, "Utilities"], [2099, 2113, "Transportation"], [3749, 3751, "5G"], [4013, 4015, "5G"], [4143, 4146, "5G"], [4380, 4383, "5G"], [5278, 5280, "5G"], [6432, 6435, "IoT"]]}
{"id": 74, "data": " Another interesting vertical that's going to drive some requirements into the network-- now let's remember what the scope of what we're trying to do here. We're trying to show the breadth and depth of these requirements that are going to be placed onto that network. And there's going to be an \"aha\" moment for us as we get into one of the further sections about that. But let's take a look at the industrial use case. Again, we think about that connected factory when we talk about that, and the massive data burden that that's going to place onto that network. Again from a carrier perspective, why would we want a macro network, a licensed spectrum network inside, let's say, a very large factory, where we could deploy, for example, similar technologies like the unlicensed space from a Wi-Fi One of the critical points there is the manageability aspect of it. If we've got any experience with Wi-Fi, we know if we're deploying lots of Wi-Fi, and those signals are overlapping, that there's a high level of management that has to take place in configuring those devices. That's just one of the aspect of it. In the macro network when the comm service provider is deploying, for example, 4G or 5G microcell or picocell into those types of environments, that manageability comes as a result of the technologies that are being deployed. And the responsibility for that is not, then, with the enterprise, but rather with the comm service provider who are experts at this. So again, if you think about the factory type of an environment, or even a large stadium environment, in some cases it's advantageous to that enterprise to have a comm service provider come in and employ deploy a macro network capability, because there are fundamental differences in the manageability of those elements into that. So when we look at the potential tech across here, you want to think large-scale enterprises, it may be to their advantage to have a macro operator come in and provide that wireless connectivity. Not just because of the data, which they're there. But also because of the manageability, the control, the configuration, the actual operational aspects of those platforms that come into play. So again, back to the use cases we may find in there-- so automated or programmable production lines, process automation, supply chain management. Again, think about inventory control flowing, just-in-time or nearly just-in-time arrival of those developments, of those elements that are going to be consumed by that industry and how all of that back pressure is created all the way back into the order management system and the logistics and the freight that's in there. So if you know that you've got a six-week lead time to get the collection of clamps that you need, in order to hold all these cabling or hoses together inside an airframe that you're manufacturing, from that standpoint, you want that to flow as seamlessly as possible so that you're not starving a production line, nor are you holding far more inventory locally than you need to in order to meet your expected flow of demand of that information. Also, human resource assistance, information mediation are elements there. Another use case that we see in this area is the industrial use of drones. And again, this is for very large plant management. So again, you can think about chemical plants are a really fine example of this, where there's a lot of infrastructure that's in play, a lot of critical infrastructure. It's not very dense in humans for a variety of reasons. Some of it may just be the environment that you're operating. Some of it may be the scope of it. Some of it may be elevation. Drones are a wonderful device to allow inspection of these devices, of these plants, and enabling those devices, again, with video camera capabilities to provide feedback to an operator so they can do inspection of these types of devices. Is corrosion developing? Do we have a sensor that's failing? Did a wire get knocked loose in the last hailstorm? For example, it may have come through. Whatever it happens to be, these are types of applications that certainly come into play. Even the comm service providers themselves are using drones for inspecting their cell towers, again, for the same type of thing, rather than having someone come out with a boom, or a human being climbing up the tower itself to do visual inspections. But nevertheless, we want to get that information back effectively so we can make quality decisions based on that information. The media itself providing 3D panoramic view of the video. Here, we think about some of the other opportunities. Think about an open air type of stadium environment, whether that's a sailing race, for example. This is not a course where you can simply hang our cameras in a variety of locations. But you may want to improve the user experience in wondering that race as it takes place in real-time. Well, drone capability is a wonderful thing that to do, where we've got infrastructure that we can put up in the sky and we can watch what's happening from a sport's experience. And we would put that into this type of a classification. There are a number of trials have gone on in the 5G radio network. And some of these areas are actually pretty interesting, both from a viewer standpoint and NOTICES AND DISCLAIMERS[O1] [O1]", "label": [[1198, 1200, "5G"], [2508, 2517, "Industrial"], [5204, 5207, "5G"], [3234, 3245, "Industrial"]]}
{"id": 75, "data": " So I've got one more of the IoT verticals that we want to spend a little bit of time with. And that's the connected car, or it also includes autonomous driving. We sometimes overload these a little bit in our minds. There are actually going to be some different functionalities in them. But again, if we think about the transportation issues associated with a connected vehicle, we'll see that we've got some specific functionalities that drive us to look at a 5G type of a network. So for the connected car, the implications, especially with the service delivery in there, looking at this cylindrical icon here, is in-car infotainment systems. So you think about streaming video for the passengers in the back. The massive sensors for the car monitoring itself, this ties in both not only to the modern car itself that has an amazing amount of compute resources and sensors in it that are used for monitoring and controlling that device, but also tying that back into the functionality associated with autonomous driving. Even think about simple things of what you can do with backup control now and tying that information into the backup camera. And being able to change the velocity of a vehicle as you're backing up as those devices may in fact be detecting that you've got an obstacle that you might not be paying quite enough attention to that to avoid or actually apply the brakes as you're approaching that device, and the connectivity that's associated with that. But also the sensors being able to relay information about the vehicle's health and safety features back out to a service monitoring point if that's necessary. The end user you, tethered into that car system itself. So not only infotainment in the car itself, but you may also have Wi-Fi hotspot type of connectivity to user endpoints that are in that car, and then using the macro connectivity of the device itself to consolidate those elements. And then the big vector, obviously, here is the autonomous driving aspects of the vehicle itself. And while we wouldn't be looking at necessarily the 5G network for every aspect of it, there are significant aspects of it that will come into play there. And that's going to possibly vary, depending on the vendor itself that's going to be building and integrating those platforms. So earlier on, we mentioned very briefly the data flow information requirements that the autonomous car, or the autonomous vehicle, would bring to bear. And we mentioned the fact that the dual cycle of the use of that platform is less than 24 by 7. By one report, we can look at the graph on the screen here, and see the bumpiness, if you will, obviously between the hours of 12:00 AM and 5:30 AM. In most regions, the utilization of those vehicles are typically low. And consequently, any of the data that flows from that vehicle into the network is going to be low. And then we've got what we would expect to see the bumpiness associated with a morning commute, the drive time, it gets a little bit of abatement during the mid-morning hours. It goes back up again as people were running errands around lunchtime. Again, some abatement during the middle of the day itself. We've got an evening drive time and most of those geographies. And then a continual drop-off in there Unlike the transportation network, the physical network which has to accommodate that flow, we may, in fact, look at a communication's network where we can turn services on and off, or ramp them up and down, if you will, to consume our resources or allocate them to other bits of logical functionality during those quiet times. Between 1:00 AM, let's say, and 4:30 AM, or whatever it happens to be. Maybe we can crunch some big data operations with our platforms on these aggregated systems in that type of functionality. Back to that bit rate, a variety of sources, depends on who you ask as to how much information is actually going to be sourced to the network, if you look at it through GPP, standards body responsible for 5G, they're giving one of the lower estimates. If you look at Hitachi, for example, an industrial firm that has a lot of experience with enabling sensors on very large devices, a couple of those are a magnitude larger from their estimates. So there's still some debate going on as to how much information the connected car is going to derive onto the network. But it's certainly a lot more than we have today. When we look at the functionality of those use points, now in that connected car, the services they are actually different characteristics. They're going to have different quality of service, QoS, in there. And then also, some of those elements are going to be heavy in the control plane. That is, the signaling aspects, and others going to be heavy in what we call the user plane or the data plane. If you think about streaming a video to a connected car, or streaming video from a connected car, that's going to be heavy user plane type functionality. If you think about information from sensors, that's a control plane, an example of a control plane bit of functionality. And also, as we talked about, one of the things that that's going to happen inside of that network, the comm service provider network, it's going to allocate those resources based on that demand, so that we don't overbuild and allocate user plane functionality where necessary during those busy times, and allocate the control functionality similarly in that area. On the control plane area, this is a difficult table to go through. So you're going to have to study it a little bit. But highlighted here is an example of the 4G versus 5G. So average LTE capabilities versus what we would estimate would be used by the connected car, is that we see that there's significant differences. There's in some cases between 400% and 700% difference in that IoT control plane usability from that car standpoint. So a connected car using 5G, for example, for a control plane functionality using the Hitachi example, for example, would place a burden on it that the LTE, the 4G network, is not capable of meeting. So if you were to do that in this type of environment, you'd clearly need a 5G capability in order to provide that. So one of the discussion points that takes place in the vehicle-to-vehicle, our connected device, is vehicle-to-vehicle communications. You've got multiple devices on cars, if you will, that are interacting with each other. Or you've got the device, the vehicle, communicating back to some infrastructure, or the vehicle communicating into some type of a data center, into that type of cloud. And if we look at the flow of information into that network as it moves along, that device obviously moving at a relatively fast speed. If you think about the velocity of a human using a cell phone, walking around in a metropolitan area, its order of magnitude's less than a car that's traveling down some sort of interstate at highway speeds. There are going to be significant handoffs of that device as it moves along. Similarly, if you've got platooning going on in an connected device type of an area, where these devices themselves-- you think about transportation and a fleet of trucks, where platooning may come into play or be interesting, those devices are cooperatively communicating with each other about their route, about the conditions, about their velocity, about the traffic patterns that they are experiencing as they're moving along. And they may be doing this actually through a macro network, and communicating out into the larger macro network, and then back in order for that control aspect. You're going to want some of that information to be closely managed, rather than driving it deep into the core, into an area that we're going to talk about as edge computing. You may, in fact, take some of that compute resources in the platooning example, and drive some of that functionality closer to those roadside edge units to minimize that delay and to improve the performance, actually, of the network. So that if you've got multiple devices in here, you're not congesting the network carrying that information deep into it. When you know that you've got a cooperative set of platforms that are in close proximity to each other, that they're still using that macro network for that communication through to each other in that environment. So we've got a single management point, if you will, and consolidation point to those devices as we flow along. Another use case for the connected car-to-cloud architecture. And here, we're interested in the functionality of virtualization reconfigurability software defined management of it, and automation of those functional controls. And this is a bi-directional flow. So for everything that flows from the left, we've got equal amount of information flowing to the right. That's a little bit different than what we think about from the control plane center aspect, where typically that's a heavily unidirectional type flow of information. But here, we may be looking at not only sensor collection and sensor fusion, but the storage and processing capability taking place on the platform itself, transferring that information then in a consolidated way through some macro network. In some cases, that may be a 4G LTE style network, we'll talk about that in a moment, in an environment that we've not upgraded to the 5G space. Because when we roll 5G out, it's not going to be a flash cut. It's not going to be ubiquitously deployed simultaneously. There may be some short period of time in which it's enabled. And if we've got devices that are out there that are 5G-enabled, those devices will need to interoperate with a 4G network, in addition to a 5G network. And that consolidation needs to take place onto that platform. Again, information flowing into those roadside units where we may be able to provide some type of edge compute capability in those devices, deeper into that network, we can perform analytics or location services on there. Interesting areas of functionality that we're still talking about. And there's been some debate going on inside the industry itself as to how much augmented reality services for passenger awareness we might be able to introduce into this network and into these devices, as well as virtual reality type of services for, again, if you think about entertaining the passengers, the non-driver type usage. Ultimately, these functionalities are going to be driven by the use cases, that the devices themselves and the users of those devices are going to drive in the industry. So we're going to build this 5G network and we're going to enable these cars. And we're going to discover that what we thought was going to be the greatest use case is likely different than the actual one that develops. Highly likely that smart city services are going to come into play here again. So when we think about very dense metropolitan areas, the ability to control traffic flow and to communicate effectively to those drivers, we think examples like that smart parking example that we've given you are going to come into play here. And all at the same time, a lot of this information is going to flow deeper into the network. So for example, if you've got an autonomous vehicle that's able to provide a lot of sensor information, the manufacturer in fact may be interested in how that platform is performing so they can collect analytics. Not just for maintenance activities, but also to improve the quality of their platform as it moves forward. So if they can collect that information, they can actually act on, and they can perform, analytics NOTICES AND DISCLAIMERS[O1] [O1]", "label": [[28, 32, "IoT"], [107, 121, "Connected Cars"], [321, 335, "Transportation"], [462, 464, "5G"], [495, 508, "Connected Cars"], [2071, 2074, "5G"], [2720, 2731, "Utilities"], [4004, 4006, "5G"], [4090, 4102, "Industrial"], [4313, 4326, "Connected Cars"], [4481, 4494, "Connected Cars"], [4856, 4869, "Connected Cars"], [4897, 4910, "Connected Cars"], [5624, 5627, "5G"], [5707, 5721, "Connected Cars"], [6167, 6171, "5G"], [7156, 7170, "Transportation"], [9381, 9383, "5G"], [9411, 9414, "5G"], [9628, 9630, "5G"], [9716, 9718, "5G"], [10574, 10582, "Industrial"], [10613, 10615, "5G"]]}